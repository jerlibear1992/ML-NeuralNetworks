{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>DEEP LEARNING with RNN, LSTM, GRU, and with BayesianCV Optimizer from SciKit-Optimize.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9049536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6504498e",
   "metadata": {},
   "source": [
    "##### We will first load our numpy array data we created from our DataWrangle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7702, 2, 919)\n",
      "(7702, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# load numpy array from npy file\n",
    "from numpy import load\n",
    "# load array\n",
    "X_data = load(r'C:\\Your_Path\\X_data.npy')\n",
    "Y_data = load(r'C:\\Your_Path\\Y_data.npy')\n",
    "# print the array to see that the shapes are correctly specified\n",
    "print(X_data.shape)\n",
    "print(Y_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's now create our typical Train-test split along with our validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=4)\n",
    "\n",
    "# We further split our training datasets (X_train and y_train) into a train2 (X and y) set and a validation set (X_val and y_val)\n",
    "X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6161, 2, 919) (4928, 2, 919) (1233, 2, 919) (1541, 2, 919) (6161, 1, 1) (4928, 1, 1) (1233, 1, 1) (1541, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Let's check the shapes. Dimensionality should be the same for all sets = 3D.\n",
    "print(X_train.shape, X_train2.shape, X_val.shape, X_test.shape, y_train.shape, y_train2.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "daccbe02",
   "metadata": {},
   "source": [
    "##### We will here need to import A LOT of packages because we are doing everything! RNN... LSTM... GRU... different layers etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import InputLayer\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l1_l2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN\n",
    "##### This simple RNN is a recurrent neural network which we can simply build. It contains one input layer, one simpleRNN layer, and one output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 10)                9300      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,311\n",
      "Trainable params: 9,311\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_simpleRNN = keras.Sequential()\n",
    "model_simpleRNN.add(InputLayer(input_shape=(2, 919)))\n",
    "model_simpleRNN.add(SimpleRNN(10, activation='tanh', return_sequences=False))\n",
    "model_simpleRNN.add(Dense(1, activation='sigmoid'))\n",
    "print(model_simpleRNN.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next to compile our model we need to specify a few things, such as the Adam optimizer, the learning rate, and loss function, and AUC\n",
    "\n",
    "model_simpleRNN.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 18.\n",
      "Epoch 28: early stopping\n",
      "Train: 0.917, Test: 0.728\n"
     ]
    }
   ],
   "source": [
    "# Next, we will need to define our batch size, which is how many samples to run at once\n",
    "# and the epoch, which specifies when a dataset has been fully parsed through by our model which will calculate our loss and AUC fit.\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 1000\n",
    "\n",
    "# We instance early stopping here, which allows us to specify validation auc\n",
    "# as our performance measure to monitor, and the mode allows us to specify\n",
    "# that we are trying to objectively increase our validation AUC.\n",
    "\n",
    "# If instead of AUC or accuracy, we monitor val_loss (validation loss), we\n",
    "# would then want to select 'min' as our mode because we would want to minimize\n",
    "# our loss. Verbose set to one allows us to discover our training epoch that was stopped.\n",
    "# We add patience=10 to delay the number of epochs run which we don't see improvements.\n",
    "\n",
    "es = EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=10, restore_best_weights=True)\n",
    "\n",
    "history_simpleRNN = model_simpleRNN.fit(X_train2, y_train2,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[es] # note callback can be passed thru fit, evaluate, predict to look into various stages of model training and specifics\n",
    "                                   # In this case, we are use callback to pull out what epochs and the model parameters.\n",
    "                    )\n",
    "\n",
    "# evaluate the model\n",
    "_, simpleRNN_train_auc = model_simpleRNN.evaluate(X_train, y_train, verbose=0)\n",
    "_, simpleRNN_test_auc = model_simpleRNN.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (simpleRNN_train_auc, simpleRNN_test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 29)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training history\n",
    "epoch_count = range(1, len(history_simpleRNN.history['auc']) + 1)\n",
    "print(epoch_count)\n",
    "plt.plot(epoch_count, history_simpleRNN.history['auc'], label='train')\n",
    "plt.plot(epoch_count, history_simpleRNN.history['val_auc'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# We see that our best epoch was determiend to be weights, biases, and params\n",
    "# from epoch 18, but of course patience=10 extended the training into 28 which finally triggered our earlystop.\n",
    "# In this graph, we see that we are starting to overfit our training data, and that our \n",
    "# testing data caps at between 0.7-0.8 AUC."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 10)                37200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,211\n",
      "Trainable params: 37,211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Now let's begin with a very simple LSTM model with one input, one LSTM layer, and an output dense layer.\n",
    "model_lstm = keras.Sequential()\n",
    "model_lstm.add(InputLayer(input_shape=(2, 919)))\n",
    "model_lstm.add(LSTM(10, activation='tanh', return_sequences=False))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "print(model_lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 18.\n",
      "Epoch 28: early stopping\n",
      "Train: 0.911, Test: 0.774\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 1000\n",
    "\n",
    "# patient early stopping\n",
    "es = EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=10, restore_best_weights=True)\n",
    "\n",
    "history_lstm = model_lstm.fit(X_train2, y_train2,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[es]\n",
    "                    )\n",
    "\n",
    "# evaluate the model\n",
    "_, lstm_train_auc = model_lstm.evaluate(X_train, y_train, verbose=0)\n",
    "_, lstm_test_auc = model_lstm.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (lstm_train_auc, lstm_test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 29)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABXFElEQVR4nO3dd3hUZd7G8e/MpIcUUkiBQOi9dxArCrJiV1QURMVV0ZXN66qsXVfZVZfFXXFZXbGjWLCs2BFQEQVBOoROaAkJIb1MMnPeP04KkSQkIcnJJPfnuuaamTPnzPxmHJk7z3mKzTAMAxERERGL2K0uQERERFo2hRERERGxlMKIiIiIWEphRERERCylMCIiIiKWUhgRERERSymMiIiIiKUURkRERMRSXlYXUBNut5vDhw8TFBSEzWazuhwRERGpAcMwyM7OJjY2Fru96vYPjwgjhw8fJi4uzuoyREREpA4OHDhAu3btqnzcI8JIUFAQYL6Z4OBgi6sRERGRmsjKyiIuLq7sd7wqHhFGSk/NBAcHK4yIiIh4mFN1sVAHVhEREbGUwoiIiIhYSmFERERELOURfUZqwuVyUVRUZHUZHsnhcODl5aVh0yIiYolmEUZycnI4ePAghmFYXYrHCggIICYmBh8fH6tLERGRFsbjw4jL5eLgwYMEBAQQGRmpv+5ryTAMnE4nqamp7N27l65du1Y7MY2IiEh98/gwUlRUhGEYREZG4u/vb3U5Hsnf3x9vb2/279+P0+nEz8/P6pJERKQFaTZ/AqtF5PSoNURERKyiXyARERGxlMKIiIiIWEphpBmIj49n7ty5VpchIiJSJx7fgdVTnX322QwYMKBeQsSaNWsIDAw8/aJEREQsoDDSRBmGgcvlwsvr1P+JIiMjG6EiERGxSr7TRXqek+O5To7nOTmeV1R2O7ugGG+HHT9vO37eDny9zGs/bzt+Xg5zm7cdXy9H2T5+3g78vMpvO+zWDgJpdmHEMAzyi1yWvLa/t6NGo3puvPFGVqxYwYoVK3juuecAeOWVV5g2bRqfffYZDz74IJs2beKrr74iLi6OhIQEfvrpJ3Jzc+nZsyezZ89m7NixZc8XHx/PzJkzmTlzJmCOLHrppZdYsmQJX375JW3btuXvf/87F198cYO8bxERqR3DMEjNKWRXSg6pOYWk55YHjPQ8Jxl5TtJzi0qunRQWuxu0Hi+7jUW/H8HgDmEN+jpVvr4lr9qA8otc9Hr4S0tee+vj4wjwOfVH+txzz7Fjxw769OnD448/DsCWLVsAuP/++3n22Wfp1KkTrVu35sCBA0yYMIEnn3wSX19fXn/9dSZOnEhiYiLt27ev8jUee+wxnn76aZ555hn+9a9/MXnyZPbv309YmDVfNBGRliojz0licjY7juawIzmbHSnm5Xhe7ZYw8XbYaB3gY14CvUuufQjy86LYZVBQ5KKgyE1h8YnX5u2CIhcFpduLXBQUu3GeEHCK3QbeDuu6kTa7MOIJQkJC8PHxISAggOjoaAC2b98OwOOPP875559ftm9YWBj9+/cvu//EE0/w4Ycf8sknn3DnnXdW+Ro33ngj1157LQBPPfUU//znP1m9ejXjx49viLckItLi5RQWsyMlm50p2SQm55SFjqPZhZXub7NBh7AAYkP9ywJGWIAPoQE+hAX6EBrgTVigT1noCPSpWet7TbndBoXF5UElLNC65UCaXRjx93aw9fFxlr326RoyZEiF+zk5OTz66KMsWbKEI0eOUFxcTH5+PklJSdU+T79+/cpuBwYGEhwczNGjR0+7PhGR5s4wDAqK3OQ5i8lzusgvcpHndJHnLCbfad7OL7l/JKugpLUjh0MZ+VU+Z9tQf7pHB9E1qhXdo4LoFhVElzat8KuH3426sttt+Ps48PexroZSzS6M2Gy2Gp0qaap+Oyrmnnvu4euvv+bZZ5+lS5cu+Pv7c+WVV+J0Oqt9Hm9v7wr3bTYbbnfDnnMUEWnqilxu1u0/zvIdqWw8mEFOoYv80tBRGjROo99hVLAv3UrCRvcoM3x0jQqila/n/i41Bn06FvHx8cHlOvUXfuXKldx4441cdtllgNlSsm/fvgauTkSk+UjJKmBFYirLdxzl+51pZBcU1/hYP287AT5e+Hs7CPAxL35lt70Ib+VTFj66RbUiNEArn9eFwohF4uPj+fnnn9m3bx+tWrWqstWia9euLF68mIkTJ2Kz2XjooYfUwiEiUo1il5t1SRksSzzK8sRUth3JqvB4WKAPZ3aNYFTnCFoH+hBQcqoiwMdBgLdX2W1/bwd2i4e8thQKIxa55557mDp1Kr169SI/P59XXnml0v3mzJnDTTfdxKhRo4iIiOC+++4jKyur0n1FRFqq6lo/bDbo1y6Us7tFcnb3SPq1C7V8Xg2pyGYYhmF1EaeSlZVFSEgImZmZBAcHV3isoKCAvXv30rFjR/z8/Cyq0PPpcxQRT1La+rG8pPVj629aP1oHeHNmt0jO6d6GMV0jCG/la1GlLVt1v98nUsuIiIh4jNTsQl5ZuZeFq5PIOGGeDrV+eDaFERERafL2H8vlxe/28N7ag2WTdZW2fpzdPZIzu0aq9cODKYyIiEiTtflQJv9esZvPNx3BXdKpYGD7UG47qzNje0ap9aOZUBgREZEmxTAMVu46xvwVu/lhV1rZ9nO6R3LbWZ0Z1jGsXmciFespjIiISJPgcht8vvkI/1mxh02HMgFw2G1M7BfD78/qTM+YqjtAimdTGBEREUsVFLn4YN1BXvpuD/uO5QHmZGPXDG3PzWd0JC4swOIKpaEpjIiIiCUy84t486f9vLJyH2k55mJyoQHeTB0Zz9RR8ZYu3CaNS2FEREQa1aGMfF77cR8Lf04ip9CcnCw2xI9bxnTimmFxHr2+mNSN/ot7sPj4eGbOnMnMmTOtLkVEpEqZeUWs2nOMH3ensXJXGrtTc8se6x4VxO/P6sTE/rF4O+wWVilWUhgREZF6le908cv+dFbuMgPI5kOZZcNywZygbFh8GL8/qxPndG+jkTGiMCIi0hIcysjnvV8OsCwxlSBfL9qG+hMb6k/b1v60DfWnXWt/okP86tQ6UeRys/FgBit3HWPlrjR+TcrA6aq4oGfnyEBGdzEXpxvZKZyQAO/6emvSDCiMWOTFF1/k0Ucf5eDBg9jt5f/zX3LJJYSHh/PAAw+QkJDATz/9RG5uLj179mT27NmMHTvWwqpFxJMUFrv4ZutR3lmTxA+70jjVSmR2G0QF+50UVNq29qddyXWAjxdut0FiSjYrd6Xx4+5jrN6bXtb3o1RMiB+jOkcwuks4ozpHEB2iNa+kas0vjBgGFOVZ89reAWb7Yw1cddVV3HXXXSxbtozzzjsPgPT0dL744gs+++wzcnJymDBhAk8++SS+vr68/vrrTJw4kcTERNq3b9+Q70JEPFxicjaL1hzgw18PcvyE9VtGdQ7nsoFtsdtsHMrI59DxfPO65OIsdnMks4AjmQWw/3ilzx0a4I0NKjxv6faRncIZ1SWC0Z3D6RgRqNMvUmPNL4wU5cFTsda89p8Pg09gjXZt3bo1F154IQsXLiwLI++//z4RERGcc8452O12+vfvX7b/E088wYcffsgnn3zCnXfe2SDli4jnyi4o4n8bjrDolwNsOJBRtj062I8rB7fj6iFxtA+ver4Ot9sgLbewPKAcz+dwSUg5WLItu6C4bHE6f28HQzuGMbpzOKO7RNArJhi7pmaXOmp+YcSDTJ48menTp/PCCy/g6+vLW2+9xTXXXIPdbicnJ4dHH32UJUuWcOTIEYqLi8nPzycpKcnqskWkiTAMgzX7jrNozQE+23SE/CIXAF52G2N7RjFpaBxndous0fotdruNNkF+tAnyY2D71pXuk1VQxKHj+RQWu+kVE4yPl0a/SP1ofmHEO8BsobDqtWth4sSJGIbBkiVLGDp0KN9//z3/+Mc/ALjnnnv4+uuvefbZZ+nSpQv+/v5ceeWVOJ3OhqhcRDzI0ewCFq87xLtrDrAnrXyYbOfIQCYNjeOyge2IDKr/FWyD/bwJjlHHU6l/zS+M2Gw1PlViNT8/Py6//HLeeustdu3aRffu3Rk0aBAAK1eu5MYbb+Syyy4DICcnh3379llYrYhYqaDIxXc7Unlv7UG+3X4UV8lY2QAfBxf1i2HS0DgGtW+tfhrikZpfGPEwkydP5qKLLmLLli1cf/31Zdu7du3K4sWLmThxIjabjYceegi3213NM4lIc5OR5+Tb7Uf5aksKK3aklp2GARjYPpRrhsbxu36xtPLVP+Xi2fQNtti5555LWFgYiYmJXHfddWXb58yZw0033cSoUaOIiIjgvvvuIysry8JKRaQxHMrI5+styXy1NYWf96aXtYCAOWX6hL5mK0jXqCALqxSpXwojFrPb7Rw+fHIfl/j4eL799tsK22bMmFHhvk7biHg+wzDn7PhqSwpfbU1m86GKf3T0iA7igl5RXNA7mt6xwToNI82SwoiISCNzuQ3W7j/OVyUtIEnp5XMj2W0wpEMYF/SO4vxeUXQI94w+cCKnQ2FERKQRFBS5+GFnGl9tTeabbUdJzy0fGefrZWdM10gu6BXFuT3bENGq/kfCiDRlCiMiIg2ksNjFdzvS+HTjYb7ZmkKus7wDaoi/N+f1bMMFvaI5s1sEAT7651haLn37RUTqkbPYzQ+7Uvl04xG+3pJC9glrtsSG+HFB72gu6B3F0PiwOi1KJ9IcKYyIiJymIpebH3cfY8nGw3y5JYXM/PJ1W6KDzREwF/WPYWBcqDqgilSi2YQR41TLUUq19PmJ1E6xy83Pe9P5dOMRvth8pMLCcZFBvkzoE81F/WMZ3L611mwROQWPDyMOhwMAp9OJv7+/xdV4rrw8sze/t7emehapisttsGZfOks2HuHzzUdIyynvhBoe6MP4PtFc1C+WYR3DarQejIiY6hRG5s2bxzPPPENycjL9+/fnX//6F8OGDat036KiImbPns1rr73GoUOH6N69O3/7298YP378aRVeysvLi4CAAFJTU/H29sZu1znY2jAMg7y8PI4ePUpoaGhZuBNpSQzDIM/pIrugmJzCIrIKis3bBcVkFxSRXVDMgeN5fLE5maPZhWXHhQZ4M763GUBGdArDS31AROqk1mFk0aJFJCQkMH/+fIYPH87cuXMZN24ciYmJtGnT5qT9H3zwQd58801eeuklevTowZdffslll13Gjz/+yMCBA0/7DdhsNmJiYti7dy/79+8/7edrqUJDQ4mOjra6DJF65XIbrD9wnB92HuNodgHZJeEip7C45Hb5fXcNz1QG+3kxrnc0v+sXw+guEeqEKlIPbEYtOwsMHz6coUOH8vzzzwPgdruJi4vjrrvu4v777z9p/9jYWB544IEKs4deccUV+Pv78+abb9boNbOysggJCSEzM5Pg4OBK93G73VrRto68vb3VIiLNRlpOId/tSGVZYirf70wl44S+HKfisNsI8vOila8XQX7eBPl5EeTrRZCfF6EBPozpGsGYrpH4eCmAiNRETX6/oZYtI06nk7Vr1zJr1qyybXa7nbFjx7Jq1apKjyksLMTPz6/CNn9/f3744YcqX6ewsJDCwvKm0JqsyWK32096HRFp/lxug40HM1iWmMqKxKNsPJTJiX9iBft5MaZbJF0iWxHk50Wwnzet/MyAEeTnTStfL4JLbvt52zXaRcQCtQojaWlpuFwuoqKiKmyPiopi+/btlR4zbtw45syZw5lnnknnzp1ZunQpixcvxuVyVbo/wOzZs3nsscdqU5qItCDpuU6+25HK8sSjrNiRWmEkC0Dv2GDO7h7JOd3bMCAuVH05RJq4Bh9N89xzzzF9+nR69OiBzWajc+fOTJs2jQULFlR5zKxZs0hISCi7n5WVRVxcXEOXKiJNlNttsOlQJssTU1mWeJQNBzMqtH4E+XkxpmsEZ3dvw9ndImkTrFZSEU9SqzASERGBw+EgJSWlwvaUlJQqOz9GRkby0UcfUVBQwLFjx4iNjeX++++nU6dOVb6Or68vvr5am0GkpXMWu/n714m8/8tBjuVW7BPWIzqIc3qY4WNQh9bqSCriwWoVRnx8fBg8eDBLly7l0ksvBcyOo0uXLuXOO++s9lg/Pz/atm1LUVERH3zwAVdffXWdixaR5i8tp5A73lzH6n3pALTy9eKMLhGc3T2Ss7pHEhOieYVEmotan6ZJSEhg6tSpDBkyhGHDhjF37lxyc3OZNm0aAFOmTKFt27bMnj0bgJ9//plDhw4xYMAADh06xKOPPorb7ebee++t33ciIs3G5kOZ3Pr6LxzOLCDI14vZV/Tlgl7RGsUi0kzVOoxMmjSJ1NRUHn74YZKTkxkwYABffPFFWafWpKSkChOPFRQU8OCDD7Jnzx5atWrFhAkTeOONNwgNDa23NyEizccnGw5z7/sbKChy0ykikBenDKFLm1ZWlyUiDajW84xYoabjlEXEc7ncBs9+lci/l+8G4OzukTx3zUBC/LVEgYinapB5RkREGkJmfhF3v/MryxNTAbjtrM78aVx3re8i0kIojIiIpXYdzeHW139hT1ouft52/nZFPy4Z0NbqskSkESmMiIhlvt2ewt1vrye7sJjYED9enDKEPm1DrC5LRBqZwoiINDrDMPj3it0882UihgFD41vz7+sHE9FK8wuJtEQKIyLSqPKdLv70/gY+3XgEgMnD2/PIxN4ativSgimMiEijOXg8j1tfX8vWI1l42W08enFvrh/RweqyRMRiCiMi0ih+3nOM299aR3quk/BAH/59/WCGdQyzuiwRaQIURkSkQRmGwZs/J/HYJ1sodhv0jg3mxSlDaBuq6dxFxKQwIiINxlns5pFPtvD26iQAJvaP5ekr+uHv47C4MhFpShRGRKRBFBa7uPX1tazYkYrNBveO68FtZ3XCZtNEZiJSkcKIiNS7Ipebuxb+yoodqfh7O3hh8iDO6dHG6rJEpIlSGBGReuVyGyS8u4Gvtqbg42XnpSlDOKNrhNVliUgTpoH9IlJv3G6D+z/YyP82HMbLbmP+9YMURETklBRGRKReGIbBo//bwntrD2K3wT+vHci5PaKsLktEPIBO04g0Q4cy8nl91T5a+Xhx61md8PVq2NErhmEw+/PtvL5qPzYb/P3q/kzoG9OgrykizYfCiEgzsutoDvNX7OajXw9R7DYA+GJLMs9dM4AubYIa7HXnfrOTF7/bA8CTl/blsoHtGuy1RKT5URgRaQY2HszghWW7+XJrMoaZQRjeMYwdKdlsOZzFRf/6gQd/14vJw9vX+9Dafy/fzXNLdwLw8EW9uG54+3p9fhFp/hRGRDyUYRis2n2MF5bv5oddaWXbL+gVxR3ndGFAXCgpWQXc894Gvt+ZxoMfbWZ54lH+dkU/wutpddxXV+7lb19sB+De8d256YyO9fK8ItKy2Ayj9O+opisrK4uQkBAyMzMJDg62uhwRS7ndBl9vS+GF5bvZcCADAIfdxiUDYrn9rM50jQo6af8FK/fy9BeJOF1uIlr58uxV/Ti7++nN+7FoTRL3fbAJgD+c24WEC7qf1vOJSPNT099vhRERD1HkcvPJ+sPMX7GbnUdzAPD1sjNpaBzTx3QiLiyg2uO3Hs7i7nd+LTt22uh47hvfAz/v2ndu/Xj9IWYuWo9hwPQxHfnzhJ6aWVVETqIwItJMFBS5ePeXA/xnxR4OZeQDEOTrxQ0jOzBtdEcig2p+yqWgyMXsz7bx2qr9AHSPCuKf1w6ke3TNO7d+sfkIMxb+isttcP2I9jxxSR8FERGplMKIiIfLKijijVX7eWXlXtJynABEtPLhpjM6cv2IDgT7edf5ub/dnsK9728kLceJj5edP1/Yg6mj4k8ZKpZtP8qtb/xCkcvgysHtePqKftjtCiIiUjmFEREP9u6aAzzx6VayC4sBaBvqz21ndeKqIXF1Oq1SmdTsQv70/gaWJ6YCcFa3SJ65qh9tgvwq3X/lrjSmvboGZ7Gbi/rF8Nw1A3EoiIhINRRGRDzUt9tTuPm1XzAM6NqmFbef3ZmJ/WPxdtT/hMmGYfD6qv08+dk2nMVuwgN9ePrKfpzXs+LMqb/sS+eGl1eTX+Ti/F5RvDB5UIPUIyLNi8KIiAdKTM7min//SE5hMdcOi+PJS/s2ymmQHSnZ/OHtX9menA3ADSM68OcJPfH3cbDxYAaTX/qZ7MJixnSN4L9ThzT4jK4i0jzU9Pdbf9qINBHHcgq5+bU15BQWM6JTGI9f0qfR+mN0iwrioxmjublknpA3ftrPxOd/4JMNh5myYDXZhcUM7xjGizcoiNSJ2wXbPoV1b4DbbXU1Ik2OJj0TaQIKi13c9uZaDh7Pp0N4AP+ePLjRT4P4eTt46KJenNUtkv97bwO7jubwh7d/BWBg+1BevnEo/j4KIrVSlA/rF8Kq5yHdnC6f3FQYk2BtXSJNjFpGRCxmGAYPfriZNfuOE+TrxctTh9A60Meyes7sFsmXM8/k/F5mv5HescG8Om0YrXz1t0uN5aXDimfgH31gSYIZRHxamY99+wTs/d7a+kSaGP3rImKx/36/l/fWHsRug+cnD2rQBe1qKizQhxdvGMzWI1l0jmxVbyN4mr2MJFj1Aqx7HYpyzW0h7WHkDBh4PXx+L6x/C96/CW77HoKira1XpIlQGBGx0NJtKTz1+TaAslMkTYXNZqN3bIjVZXiG5E2w8p+w+QMwXOa26L4weib0uhQcJf/UTngWDq+Ho1vMQDLlk/LHRFow/V8gYpHEZHMEi2HAdcPbc+OoeKtLktowDNi7AlY+B7u/Ld/e6WwYfTd0Ogd+O4mcTwBc/Tq8eDbsX2mesjn/scasWqRJUhgRsUDpyJlcp4uRncJ57OLemlLdU7iKYdvHZgg5ssHcZrND78tg1B8gdkD1x0d0gUueh/emwsq5EDccekxo6KpFmjSFEZFGduLImfjwAE0g5imceWZ/jx//BRnm2j54+cOgKTDyDmgdX/Pn6n0pJN0OP/8bProNbl0BYR0bomoRj6AwItKIDMPggdKRM35e/HfqUEtHzkgNrV8IXz0IecfM+wHhMOz3MPQWCAyv23Oe/zgc+gUOrjFbSW76Crwrn4q/WXEVw/LZkJMMMQMgdiBE9WkZ712qpDAi0ohe/G4P75eMnJl33SC6tGlldUlSHWcefHaP2SICZuvHyDthwGSz/8fp8PKBq16F+WPM0z1f3A8T555mwU1ccaHZcXf7p+b9X980r+1e0KZneTiJHWAGFK+ar0gtnk3TwYs0km+2pjD9DXPNmUcn9uLG0WqWb9JSd5gtFke3mn1Czp4FZyTU/+iXXd/Am1cCBlz2IvSfVL/P31Q4c+GdybBnGTh8Ycg0OLYLDv9a3uJ0Irs3RPU6IaAMhDa9zBAnHkNr04g0IduTs7jihR/Jdbq4bnh7nry0jzqsNmUb34X/zTTnCglsA1e+DB3PbLjXWzYbVvwVvANg+rdmK0FzUpAJCydB0irzPV77tjnqCMxRSZkHzVByZL15ffhXyD9+8vM4fCCqtxlMYvqbE8kZbnO6fcNVfm24zWn3T9xWdu0u38cvxOzz46ch7A1FYUSkiUjLKeSS51dyKCOfUZ3Dee2mYeqw2lQV5ZunS9a+at6PHwNXvAxBUdUedtrcLnjzCrPVILwr3LoMfK2f/K5e5KXDG5eZQcM3BCa/B+2HV3+MYZgTyP02oBRk1n99kT3NmkLj6v+5RWFEpCkoLHYx+aWf+WX/ceLDA/hoxmhCA9TM3CQd2w3vToWUTYANzroXzroP7I00+2xumtl/JPsw9LnCDEGe3nqWnQyvXwqp28xOv9cvPvXQ56oYBhzfVx5MUjaDq8g8hWZ3gM1xwrXdvD7psd/c3/qJ2ZG2VRRc927da5MqKYyIWMwwDP7vvQ0sXneIID8vPrxjtDqsNlWbF8MnfwBnNgREwBUvQedzG7+OpJ/h1QngLjZnax02vfFrqC8ZSfDaxXB8LwTFwA0fQZseVldVUcYBWHi12S/IOxCuXADdx1tdVe3s+MrsEBzTD3peAq2azizOoDAiYrn5K3bz18+347DbeOXGoZzZhKZ6lxLFhfDlA7DmJfN++1HmD1JwjHU1rXoBvpxlduC86UtoN9i6WuoqbRe8fjFkHYLQ9ua09011HpWCTHh3CuxZbracXPi0Z4TAY7vhi1mw88vybTa7eWqx92XQcyIERlhXXwmFERELfb01hVtLRs48dnFvpmqq96Ynfa85WqZ0FtUzEuCcB6xfK8YwzB/HbZ9ASBz8/jsICLO2ptpI3mz2Eck9ChHdYMrHEBxrdVXVcxXBpzPLhxqPugvGPm6e7mlqCnPg+7/DqufB5TSHRfebZLbuHP61fD+bw+x0XRpMLPoOKYyINJLsgiL2H8tj/7E89h3LZf+xXD7deIQ8p4vJw9vzF42caXq2/Q8+mgGFmeDf2hxS2+0Cq6sqV5Bprl+Tvge6nG/2Z2iKP4y/dXAtvHk5FGSYCwVe/2GTO21QJcOA75+Fb/9i3u95MVz+Inj7W1tXKcMwF2L86iGzXxFA5/Ng/F8hspt5P30vbP0ItnxYHrLBDCydzjaDSY/fmd/5RqIwIlJPDMMgI6+I/el57D+Wy760kutjuew/lsexXGelx2nkTBNU7ISvHzanYQdoNwyuegVC2llbV2WSN8F/x0JxAZz7IJz5J6srqt6+H8zhu84caDcUJr8P/qFWV1V7G9+Fj2eYrQ7thsK171h/uiN5E3x+n7m4IkBoBxg/G7pPqLqT87HdZijZ8lFJp+wSdm+zP1Tvy8w1kRp4WLPCiEgdrdp9jB93p7HvWGn4yCWroLjaY8IDfegQHkB8eCAdwgPpFBnIBb2j8PVqpJEYcmoZSfDejXBorXl/1F1w3iPg8La0rGr9+qb5w2izmx1AO51ldUWV2/k1LLreDE4dz4Rr3gZfD+6sve8Hc4K2ggxz1t3JH5gLHDa2vHRY9iT8ssCcF8XLH8b8n/ndrc30+Wk7S4LJh+bpnFIOH7N1pfdl0P1C8Kv/31eFEZE6+HJLMr9/Y22lj0UF+9IhPJD48ICS60A6hAfQITyAIL8m+IPmKjLnzWiAf2A8TuLn8OFt5o+LXyhcNt/8x9cTfDzDDCWBkfD7763tXFuZrR/D+zeDuwi6jYerXmse68yk7oC3rjQXRfRvbQasDiMb57XdLlj3Gix9AvLTzW29L4Pznzj9+VCObjdP5WxeDGmJ5dsdvubyBPW8grTCiEgtpWYXMm7ud6TnOjm7eySjOofToSRwtA8LIMDHQ5ZycrvNf8i+edT88Q1sA+FdzL/swk+4tO7Y/KfWzk6Brx+CjYvM+20Hw5WvQOsO1tZVG0X55umalM3QfiRM/V/lrTluNxTnm9Oul16K8szTJs68kvu55gii4LZm59KwTqf3HVi/0AxLhht6X272sWjKLU21lZMKb08yW9McPmaI7XNFw75m0k/w2Z8geaN5v00vuPBvDTMD8NFtZijZsticmj9hW713NlYYEakFwzC45bVfWLr9KD2ig/j4ztGeeYolZYs5jfnB1afe12Y3zz2XhpMTw0pQrGd0mKyK22U2bS99wuykig1G3AFjH/XMAHZst9mhtTDL7Bjq5VcSMHJKAkdJ8Kgtm8M8DRHRteTSrfxyqtEXq18yFxEEGHg9TPxn400Q15icebB4evnifmMfhdEz639Cuqwj8M0j5cHZL8Qc3TXk5oYf4WUYZmfp8M71/tQKIyK18PbqJGYt3oSPw84nd42mR/Rpfs/S95jDMhvrr0RnLiz/K6yaZ6674dPK/IdswLVmD/tju82/fI7tgmM7zfvOnKqfzzsAwjqb/zj1uAh6X+o5f/EeXAtL/lg+miBmAFw0x2wV8WRbP4F3b6jZvt6B5qrCPoElt0vvtzJHVmTsN/sRVPcd8A8rCSYnhpSuZoBd9S+z5Q1g+O0w7inPDq+n4naZo1h+mmfeH3wjTPh7/YSEYif89AJ890zJfw8bDLrB7M9kdcfZeqAwIlJD+9JymfDP78lzunhgQk+mn9np9J7w2yfhu6fNpvDhv4dBUxt2VEHiF2azbmaSeb/nRBj/NwhpW/UxhgE5KWY4SdtZElJKAsvxveYMoCdqrPdyOvKPw9LH4ZdXAMNcB+W8h2DITc3nL/Z9P0DWYTMs+gSWX7xLgoZPgNnJsSbBwDDM6drTdpRcdppBNW0nZB6o+ji7t9k/BMwRPuc84PnT1tfUz/8x1y4y3NBlrNnH4lRrCBmG+d3MOmxOApd1qOR2yf3URMg+Yu7bbqg56VrbQQ3+VhqLwohIDRS73Fz9n1WsS8pgRKcwFt4yArv9NP5h3fwBvH9TxW3egeZfOsNvq99ZKDMPwef3ljcfh8SZU4if7nTWriJz5MmxXea58l9eMSewAvMHb+ANMOI2s3m/KTAM2PC2+ZdrXpq5rd81cMET0KqNtbV5KmdueVBN21kxrBQXmPuMfRTO+KOlZVpi+2fwwc3mabGovubSAS5nebjIPHRC8CgJHcX51T9nYBs4/zHze9vMWpgURkRq4Plvd/LsVzsI8vXi85ljaNc6oO5PdvhXWDDe/Md6xAyI6mWeNikdSmezmxMOjbwT4obX/a9JVzGsftEc8ufMMZvdR84wF3XzCax7/VUpLoRN75szPp74XnpOLHkvw+r/NWsqZSss+T9I+tG8H9Edfvd36DjGupqaM7cbsg6arSNNbVRPYzq0zpxTpTSk10RAhNk5NLhtyfUJt9sNaZj/d5sAhRGRU9h8KJNL562k2G0w5+r+XD7oNCa+yk4xOxhmHy6ZMXOReWrAMMxl4VfNg13flO/fdogZIHpeXLvzzofWmh1US3vaxw2Hi/4BUb3rXntNGQbs/tZ8L7uXlm9vN6zkvUxsvNMhhTmw4m/muXZ3sXma4qz7zE6qnthBVTzP8f3mcgKHfzVbNoJjzcnzfhs0gmPNDuHNYbhzHSiMiFSjoMjFRf/6gV1Hc5jQN5p51w2q+5TtxYXw6u/g4Bqzk98t31Q+q+HRbeaP54ZF4Co0t4W0N095DLyh+vlACjLNkSFr/gsY5lwZ5z8GA6dY06ybstUMJZveNZuowezYOOIOGDj51OfR68owzKncv7jfbAYHs4Pt+L+e/vwLInXhKvKczt0WUBgRqcZj/9vCKyv30SbIly9nnknrwDr+NW0Y5jwL698yA8j0ZaceHpeTaoaKNf8t7+PgEwSDp5qdREPbV3z+LR+aq3PmJJvb+l0DF/ylaaz5kZ1irni75uXyyZl8Q2DIjTDs99V3oq2t9D3w2b2w62vzfmgHmPAMdBtXf68hIvVKYUSkCj/sTOP6l38G4NVpQzm7+2l0clw1D778s9mH4voPzDUfaqoo31wHY9W88pkQbQ7odTGMvMuc5+Gze8pP74R3gd/NaZpTgjvzzE6kP71gdnwEsy9L78th6M3m7KFgfk42m3mNreL9k7bZzPuG25zT4vu/my1KDh8Yfbe5yq7PafTxEZEGpzAiUonMvCLGzf2O5KwCbhjRgScu7VP3J9v1Dbx1lfljOf6vMOL2uj2P213SF+N5s39JKZvDnDPE4WuuR3HGTPDyrXu9jcHthp1fwo/Pw/4f6v/5O51tjhiK6Fr/zy0i9a6mv991Otk8b9484uPj8fPzY/jw4axeXf1sj3PnzqV79+74+/sTFxfHH//4RwoKCury0iKn5aGPN5OcVUCniEBmTehR9ydK2wnv3WQGkYElw3brym6HrmNhykdw20oYMNkcrWC4oONZcMcqOPu+ph9EwHwv3S+EaUvg1uXQ9ypz8izfEPNUlHfJnBhefmbIsnubLSilrSJVCW4LVy4wF4tTEBFpdmo9fdyiRYtISEhg/vz5DB8+nLlz5zJu3DgSExNp0+bk5u6FCxdy//33s2DBAkaNGsWOHTu48cYbsdlszJkzp17ehEhNfLz+EJ9sOIzDbmPOpAF1X2smPwPevsacZjxuhDmUtL4mfYruA5e+YM6+eHyfOWzWUyeUih0IV/y3dscYhnnBMIOeUXLt8Gl28y+ISLla/989Z84cpk+fzrRp0+jVqxfz588nICCABQsWVLr/jz/+yOjRo7nuuuuIj4/nggsu4Nprrz1la4pIfTqSmc9DH20G4K5zuzAgLrRuT+R2mZOaHdsFwe1g0hsN02IRFAXtT2MuEk9ls5mhw+4wRyh4+ZhDIhVERJq1Wv0f7nQ6Wbt2LWPHji1/ArudsWPHsmrVqkqPGTVqFGvXri0LH3v27OGzzz5jwoSqlykuLCwkKyurwkWkrtxug3ve20BWQTH924Uw45wudX+yrx8259jwDoBrF2qGTxGRelCrduq0tDRcLhdRUVEVtkdFRbF9+/ZKj7nuuutIS0vjjDPOwDAMiouLue222/jzn/9c5evMnj2bxx57rDaliVTptVX7WLnrGH7eduZMGoC3o45/Zf/6ltnJFMxTKTH9669IEZEWrMHbPpcvX85TTz3FCy+8wLp161i8eDFLlizhiSeeqPKYWbNmkZmZWXY5cKCaRZtEqrHraDZ//dwMyg9M6EnnyFZ1e6IDq+HTmebts+6D3pfVT4EiIlK7lpGIiAgcDgcpKSkVtqekpBAdHV3pMQ899BA33HADt9xyCwB9+/YlNzeXW2+9lQceeAB7JeeCfX198fX1gJED0qQ5i93MXLSewmI3Z3aL5PoRHer2RJmH4J3J5kyjPS6Cs+6v30JFRFq4WrWM+Pj4MHjwYJYuLV+Xwu12s3TpUkaOHFnpMXl5eScFDofDXL/CA6Y4EQ/2r293svlQFqEB3jxzZb+6TffuzIN3rjUXxIrqA5f9R50pRUTqWa3HNiYkJDB16lSGDBnCsGHDmDt3Lrm5uUybNg2AKVOm0LZtW2bPng3AxIkTmTNnDgMHDmT48OHs2rWLhx56iIkTJ5aFEpH6tnb/ceYtM2cCfeqyvkQF12GRqtKp3o9sgIBwuGYh+NbxNI+IiFSp1mFk0qRJpKam8vDDD5OcnMyAAQP44osvyjq1JiUlVWgJefDBB7HZbDz44IMcOnSIyMhIJk6cyJNPPll/70LkBLmFxSS8ux63AZcPbMuEvnVc6vz7v8OWxeakXFe/Aa3reJpHRESqpengpdmZtXgTb69Oom2oP5/PHEOwXx1W1Ny+BN65zrx90VwYMq1eaxQRaQlq+vtdxykoRZqmpVuTWbJ6K8E2G3Mu7UOwvQiK3SWLrzlq1t8jZQssvtW8PXS6goiISANTGJHmZfF0Nvp9Z95+p4p9bA5zhs8K1yVhxWYHZw4U5UHHM2H87EYrXUSkpVIYkWYjOSOP0UWrql1vDTAXoHO5qt8nohtc9Zo5JbmIiDQohRFpNjZt3cL5tiKK8ML7gUPmRsNlridjuMzl7U+8b7hLbrtP2KfkfpueCiIiIo1EYUSajcO71gOQ7teeKO86DOUVERFLaPYmaTbyD5vTvhvhp7EQnoiINDqFEWkWMvKcBOXsBSA4rrfF1YiISG0ojEiz8Mu+43S2HwYgIKanxdWIiEhtKIxIs7BmXzqdbEfMOxFdrS1GRERqRWFEmoXNu5NoY8sw7yiMiIh4FIUR8Xh5zmIKkhMBKA6MAd8giysSEZHaUBgRj7c+KYN4w5xXxNGmm8XViIhIbSmMiMdbvS+9rPOqTadoREQ8jsKIeLzVe9PpbDPDCBFqGRER8TQKI+LRilxufk3KOCGMqGVERMTTKIyIR9t8KJOiokI62FPMDWoZERHxOAoj4tHW7EsnzpaKNy7wDoSgWKtLEhGRWlIYEY+2eu/xE07RdAG7vtIiIp5G/3KLx3K7DdbsO6Hzarj6i4iIeCKFEfFYO4/mkJlfRDdH6TTw6i8iIuKJFEbEY63elw5AX7/SzqtqGRER8UQKI+Kx1uxNBwzau83ZV9UyIiLimRRGxCMZhsHqvemEkY1fcRZgg/DOVpclIiJ1oDAiHung8XySswrK+4uEtgdvf2uLEhGROlEYEY+0eq/ZX+TMsOPmBp2iERHxWAoj4pHWlHReHRyYam5Q51UREY+lMCIeqXQkjdakERHxfAoj4nFSswvZk5oLQOv8/eZGnaYREfFYCiPicX4pnV8kyhdHhsKIiIinUxgRj1N6iuaC6DzAAL8QCIy0tigREakzhRHxOKWdV0eEHDM3RHQDm83CikRE5HQojIhHyS4oYuvhLAB6eGlNGhGR5kBhRDzK2v3HcRvQPiyAoOy95sbwLtYWJSIip0VhRDxK6SmaofFhkLbD3KiWERERj6YwIh5lzV5zxtVh8aGQttPcqDAiIuLRFEbEYxQWu1h/MAOAEZGFUJQLdi8I62htYSIicloURsRjbDyYibPYTUQrH9q7S2Zebd0RHN7WFiYiIqdFYUQ8RunieMM6hmE7plM0IiLNhcKIeIzSMFKx86rWpBER8XQKI+IRXG6DdfvNzqsKIyIizYvCiHiEbUeyyC4sJsjXi54xwRpJIyLSjCiMiEcoPUUzqENrHEU5kHXIfEATnomIeDyFEfEIpZOdDesYBsd2mRsDIyEgzMKqRESkPiiMSJNnGEbFMJJWEkZ0ikZEpFlQGJEmb29aLmk5Tny87PRrF6LOqyIizYzCiDR5pa0iA9qF4uvlKA8j4QojIiLNgcKINHk/nzDZGaCRNCIizYzCiDR5ZSv1dgwDt6u8A6tO04iINAsKI9KkJWcWcCA9H7sNBrUPhYwkcBWCwxdC21tdnoiI1AOFEWnSVpe0ivSKDSbIz7v8FE14F7A7LKxMRETqi8KINGlrTlyPBqBsgTydohERaS4URqRJK515dXhZ59XSYb3qvCoi0lwojEiTlZHnJDElG4Ah8b8dSaOWERGR5kJhRJqsX/aZq/R2igwkopWvuVETnomINDsKI9JklU0BX9oqkpcOuanmbU14JiLSbCiMSJN10mRnpfOLBLcF31YWVSUiIvWtTmFk3rx5xMfH4+fnx/Dhw1m9enWV+5599tnYbLaTLr/73e/qXLQ0f3nOYjYfygROGEmjUzQiIs1SrcPIokWLSEhI4JFHHmHdunX079+fcePGcfTo0Ur3X7x4MUeOHCm7bN68GYfDwVVXXXXaxUvztT4pg2K3QUyIH+1a+5sbNQ28iEizVOswMmfOHKZPn860adPo1asX8+fPJyAggAULFlS6f1hYGNHR0WWXr7/+moCAAIURqVbpZGdD48Ow2WzmRoUREZFmqVZhxOl0snbtWsaOHVv+BHY7Y8eOZdWqVTV6jpdffplrrrmGwMDAKvcpLCwkKyurwkValgrr0ZQqW623iwUViYhIQ6lVGElLS8PlchEVFVVhe1RUFMnJyac8fvXq1WzevJlbbrml2v1mz55NSEhI2SUuLq42ZYqHK3K5Wbc/AzhhsjNXERzfa95Wy4iISLPSqKNpXn75Zfr27cuwYcOq3W/WrFlkZmaWXQ4cONBIFUpTsPlQJvlFLkIDvOkSWTJqJn0vuIvBOxCCY60tUERE6pVXbXaOiIjA4XCQkpJSYXtKSgrR0dHVHpubm8s777zD448/fsrX8fX1xdfXtzalSTNSeopmSIcw7PbS/iInjKQp7UMiIiLNQq1aRnx8fBg8eDBLly4t2+Z2u1m6dCkjR46s9tj33nuPwsJCrr/++rpVKi3G6r3mzKvDOrYu36g1aUREmq1atYwAJCQkMHXqVIYMGcKwYcOYO3cuubm5TJs2DYApU6bQtm1bZs+eXeG4l19+mUsvvZTw8PD6qVyaJbfb4Jf9v1mpF8onPFMYERFpdmodRiZNmkRqaioPP/wwycnJDBgwgC+++KKsU2tSUhJ2e8UGl8TERH744Qe++uqr+qlamq2dR3PIyCvC39tBn7Yh5Q+UtYxoJI2ISHNT6zACcOedd3LnnXdW+tjy5ctP2ta9e3cMw6jLS0kLUzq/yKAOoXg7SkKtYeg0jYhIM6a1aaRJWbO3klM0ualQkAnYIKyzNYWJiEiDURiRJsMwDFbv/c1KvVDeKtK6A3j7WVCZiIg0JIURaTIOHs8nOasAL7uNge01kkZEpKVQGJEmo7RVpG+7EPx9HOUPaE0aEZFmTWFEmoxf9pvzi1ToLwInhJGujVyRiIg0BoURaTI2HMgAYFD70IoPlC2QpzAiItIcKYxIk5DvdJGYkg1A/7jQ8geK8iEjybyt0zQiIs2Swog0CZsPZ+JyG0QF+xIT4l/+wLHdgAF+oRAYYVV5IiLSgBRGpEkoPUXTv11oxQdOHEmjBfJERJolhRFpEtaXhpETT9GARtKIiLQACiPSJJSGkQEnhZHSlhF1XhURaa4URsRyaTmFHDyej81mzjFSwTG1jIiINHcKI2K5jQczAOgc2YpgP+/yB9xuzTEiItICKIyI5dYfyAQq6byafRiK8sDuBa3jG70uERFpHAojYrny/iK/OUVT2l8krBM4vBERkeZJYUQsZRhG2bDeAXGtKz6okTQiIi2CwohYav+xPDLzi/DxstM9OqjigxpJIyLSIiiMiKU2lHRe7R0bjI/Xb76OJ054JiIizZbCiFjq16QMoJLOqwBpu8xrhRERkWZNYUQsVdoyMvC3K/UWZpujaQDCuzRqTSIi0rgURsQyzmI3Ww5nAZWtSVPSeTWwDfj/5jEREWlWFEbEMonJ2TiL3YT4e9MhPKDigxpJIyLSYiiMiGXWHzgOmIvj2X67Iq9G0oiItBgKI2KZ0plXT1ocDzSSRkSkBVEYEcuUdl49aeZV0GkaEZEWRGFELJFVUMTu1BwA+v2286rbBem7zdsRGkkjItLcKYyIJTYdzMQwoF1rfyJa+VZ8MGM/uJzg5QchcdYUKCIijUZhRCxRujhe/0r7i5ScognvAnZHo9UkIiLWUBgRS5Qujjew2s6rGkkjItISKIyIJUo7r1beMqKRNCIiLYnCiDS6I5n5pGQV4rDb6B0bfPIOGkkjItKiKIxIoys9RdMtKogAH6+Td9BpGhGRFkVhRBpdtZOd5aVD3jHzthbIExFpERRGpNGVtoxUO9lZcDvwCWy8okRExDIKI9KoXG6DjTXqvKpTNCIiLYXCiDSq3ak55DpdBPg46Nom6OQdNJJGRKTFURiRRlU62VnftiE47LaTdygbSaOWERGRlkJhRBpVeX+R0Mp3UMuIiEiLozAijaraaeCLnXB8n3lbYUREpMVQGJFGU1DkYntyNlBFGDm+FwwX+LSCoOjGLU5ERCyjMCKNZsvhTFxug8ggX2JD/E7e4cSRNLZK+pOIiEizpDAijaZ0srP+7UKxVRY21F9ERKRFUhiRRlPtZGcAu5eZ11F9GqcgERFpEhRGpNFU23n1+H7Y9z1gg96XNWZZIiJiMYURaRTpuU6S0vMA6Ncu9OQdNr5rXnccA6FxjVeYiIhYTmFEGsWGkingO0UGEuLvXfFBw4ANb5u3+1/XuIWJiIjlFEakUZT1F6msVeTAakjfDd6B0HNio9YlIiLWUxiRRlFtf5ENC83rXpeAb6tGq0lERJoGhRFpcIZhVD0NfFEBbP7QvN3/mkatS0REmgaFEWlwB9LzOZ5XhI/DTo+Y36zUm/gZFGZCSBzEj7GmQBERsZTCiDS49SWdV3vGBuPr5aj4YGnH1X6TwK6vo4hIS6R//aXBrU/KAGBAu99MdpadAruWmrf7X9u4RYmISJOhMCINrnRY74D2oRUf2PSeuTBeu6EQ0aXR6xIRkaZBYUQaVJHLzeZD5WvSVFA2t4haRUREWjKFEWlQicnZFBa7CfbzIj48sPyBIxshZTM4fKHP5dYVKCIillMYkQZ14vwidvsJK/WWtop0vxD8Wzd+YSIi0mQojEiDqnR+EVeR2V8EdIpGRETqFkbmzZtHfHw8fn5+DB8+nNWrV1e7f0ZGBjNmzCAmJgZfX1+6devGZ599VqeCxbOUdl6t0F9k11LITYXASOhyniV1iYhI0+FV2wMWLVpEQkIC8+fPZ/jw4cydO5dx48aRmJhImzZtTtrf6XRy/vnn06ZNG95//33atm3L/v37CQ0NrY/6pQnLKSxm59EcAPrFnTCst3T6975Xg8O7kiNFRKQlqXUYmTNnDtOnT2fatGkAzJ8/nyVLlrBgwQLuv//+k/ZfsGAB6enp/Pjjj3h7mz888fHxp1e1eISNBzMwDGgb6k+bID9zY146JH5u3tb07yIiQi1P0zidTtauXcvYsWPLn8BuZ+zYsaxatarSYz755BNGjhzJjBkziIqKok+fPjz11FO4XK4qX6ewsJCsrKwKF/E8Gw6YQ3or9BfZ8iG4nBDVB2L6WVOYiIg0KbUKI2lpabhcLqKioipsj4qKIjk5udJj9uzZw/vvv4/L5eKzzz7joYce4u9//zt/+ctfqnyd2bNnExISUnaJi4urTZnSRGwoG0lz4ikazS0iIiIVNfhoGrfbTZs2bXjxxRcZPHgwkyZN4oEHHmD+/PlVHjNr1iwyMzPLLgcOHGjoMqUBnNR5NW0nHFwDNgf0vcqyukREpGmpVZ+RiIgIHA4HKSkpFbanpKQQHR1d6TExMTF4e3vjcJQvkNazZ0+Sk5NxOp34+PicdIyvry++vr61KU2amJSsAo5kFmC3QZ+2JS0jpa0iXc6DoKiqDxYRkRalVi0jPj4+DB48mKVLl5Ztc7vdLF26lJEjR1Z6zOjRo9m1axdut7ts244dO4iJiak0iEjzUDrZWbeoIAJ9vcDthg2LzAd1ikZERE5Q69M0CQkJvPTSS7z22mts27aN22+/ndzc3LLRNVOmTGHWrFll+99+++2kp6dz9913s2PHDpYsWcJTTz3FjBkz6u9dSJNz0mRn+76HrIPgFwLdJ1hWl4iIND21Hto7adIkUlNTefjhh0lOTmbAgAF88cUXZZ1ak5KSsNvLM05cXBxffvklf/zjH+nXrx9t27bl7rvv5r777qu/dyFNTll/kdIwUnqKpvfl4O1nSU0iItI02QzDMKwu4lSysrIICQkhMzOT4OBgq8uRU3C7Dfo/9hXZhcV89ocx9Aq3w7PdoCgXbvoK2g+3ukQREWkENf391to0Uu/2pOWQXViMv7eDblGtYNv/zCAS1hnihlldnoiINDEKI1Lv1pdMdta3bQheDnv59O/9rwWbrZojRUSkJVIYkXpXYbKzjAOw93vzgf6TrCtKRESaLIURqXfry8JIKGx8BzAgfgyEtreyLBERaaIURqReFRS52HbEXEuof9sQ2PCO+YDmFhERkSoojEi92noki2K3QUQrH9rlbYFju8A7AHpdbHVpIiLSRCmMSL0q6y/SLhRb6dwiPSeCb5B1RYmISJOmMCL1qrS/yKC2/rD5A3OjTtGIiEg1FEakXpW2jJxtWwcFmRDcFjqeaW1RIiLSpCmMSL3JyHOy71geAN2O/M/c2G8S2B3VHCUiIi2dwojUmw0HzcnOBoUV4b2nZGVnnaIREZFTUBiRerM+KQOAG1qtBsMFbQdDZDdrixIRkSZPYUTqTelKvWfmf2NuUKuIiIjUgMKI1AvDMNhwIIMetiTCsxPB7g19rrC6LBER8QAKI1IvDqTncyzXyVVe35kbuo+HgDBrixIREY+gMCKnze02eOSTzThwcbn3KnNj/+usLUpERDyGwoictn9+u5Nliamc672Z1u7jEBABXc+3uiwREfEQCiNyWpZtP8pzS3cC8HDcBnNj36vA4W1hVSIi4kkURqTO9h/L5e53fsUw4JYhrYlLWWY+0P8aawsTERGPojAidZLvdPH7N9aSVVDMwPah3N9uM7gKoU0viOlvdXkiIuJBFEak1gzDYNbijWxPziailQ//vm4QXutfNx8ceAPYbNYWKCIiHkVhRGrttR/38dH6wzjsNp6/bhDRudsgeRM4fHSKRkREak1hRGplzb50/rJkGwCzLuzBiE7hsK6kVaTnxZpbREREak1hRGrsaFYBd7y1jmK3wUX9Yrj5jI5QmAOb3jd3GDzV2gJFRMQjKYxIjTiL3dzx1jpSswvpHhXE01f2w2azwZYPwZkNYZ0gfozVZYqIiAdSGJEaeeqzbfyy/zhBvl7Mv2EwAT5e5gPrXjOvB01Rx1UREakThRE5pQ9/PcirP+4D4B+TBtAxItB8IGUrHFwDdi8YMNm6AkVExKMpjEi1th7OYtbiTQD84dwujO0VVf5gaatI9wuhVRsLqhMRkeZAYUSqlJlXxG1vrqWgyM1Z3SK5e2y38geLCmDDO+btQTdaUp+IiDQPCiNSKbfbYOaiX0lKzyMuzJ/nrhmAw35Cn5Btn0BBBoTEQedzLKtTREQ8n8KIVOq5peZKvL5edv49eTChAT4VdyidW2Tg9WB3NH6BIiLSbCiMyEmWbkspW4l39uV96dM2pOIOx3bDvu/BZjfDiIiIyGlQGJEK9qXlMnPRegCmjOzA5YPanbxTacfVLmMhpJLHRUREakFhRMrkOYu57c21ZBcUM7hDax78Xa+Tdyp2wvqF5u1BmnFVREROn8KIAKUr8W4qWYnXlxcmD8LHq5Kvx47PITcVWkVBt3GNX6iIiDQ7CiMCwCsr9/Hx+sN42W28MHkQUcF+le+4tuQUzYDJ4PBuvAJFRKTZ8rK6ALHeyz/s5YlPtwLw5wk9GdaxipV3j++H3d+atwfd0EjViYhIc6cw0oIZhsHfvkhk/ordAEwbHc+00fFVH/Drm4ABHc8yF8YTERGpBwojLVSRy839H2zig3UHAbh3fHduP6uzuRJvZVzFJWEEGKyOqyIiUn8URlqgPGcxM95ax7LEVBx2G7Mv78vVQ+KqP2j3Usg+DP5h0OOixilURERaBHVgtdjGgxkMfuJrbnltDQeP5zX46x3PdXLdSz+zLDEVP287L94w+NRBBMo7rva/Frx8G7ZIERFpURRGLDbn6x0cy3XyzbajXPCP71jww15cbqNBXutQRj5Xzv+R9QcyCPH35q1bRnBez6hTH5idDDu+MG/rFI2IiNQzhRELbTuSxfLEVOw2GBAXSp7TxeOfbuXyf//I9uSsen2txORsLn9hJbtTc4kJ8eP920YyuEPrmh3865tguCBuBER2r9e6REREFEYs9OJ3ewC4sG8Mi28fxV8u7UOQrxcbDmRw0T9/4NkvEykocp3266zem85V838kJauQrm1asfiOUXSNCqrZwW53+aJ4ahUREZEGoDBikYPH8/hkw2EAbjuzM3a7jetHdODrhLMY1zuKYrfB88t2MeG57/lpz7E6v86XW5K54eWfySooZkiH1rx320hiQvxr/gR7V0DGfvANgV6X1rkOERGRqiiMWGTBD/twuQ1Gdwmnb7vyVXGjQ/z4zw1DmH/9INoE+bInLZdrXvyJWYs3kplfVKvXWPhzEre/uZbCYjdje7bhjZuHExrgU7tCSxfF63cV+ATU7lgREZEaUBixQEaek3fWJAHw+zM7V7rP+D4xfJ1wFtcNbw/A26sPMHbOCj7fdATDqL6Dq2EYPPfNTv784SbcBkwaEsf86wfj7+OoXaG5abDtU/O2FsUTEZEGojBigTd/2k+e00XPmGDGdI2ocr8Qf2+euqwvi24dQafIQFKzC7n9rXXc+sZakjMLKj3G5TZ46OPN/OObHQDcdW4X/npFX7wcdfhPveEdcBdB7ECI6Vf740VERGpAYaSRFRS5eGXlPgBuO6uTOeOp213tMcM7hfPZH8Zw17ld8LLb+HprCufPWcEbP+3HfcIw4IIiF3cuXMebPyVhs8Hjl/Tm/y7oXvWsqtUxjPJTNIOm1P54ERGRGlIYaWTvrz3IsVwnbUP9mdArHJb/DWa3hYWTIPNglcf5eTv4vwu68+kfzmBAXCjZhcU89NFmrv7PKnamZJNVUMTUBav5fHMyPg47z187iCkj4+teaNJPkLYDvAOgz5V1fx4REZFT0HTwjcjlNnjpe3M473398vD+77lwdIv54I4vYN9KOP8xGDwN7JXnxB7RwXxw+yjeWLWPZ75M5Jf9x5nwz++JCfEnKT2PVr5evDhlMKM6V336p0ZKW0X6XA5+waf3XCIiItVQy0gj+nJLMsnHMnjEbxET19xgBpGAcLjwGWg3DJzZsCQBXpsIx3ZX+TwOu40bR3fk64SzOK9HG4pcBknpeUS08uWdW0ecfhDJz4AtH5m3B914es8lIiJyCmoZaSSGYfD9Nx/zuc8/6EQyGECfK+DCpyEwAobeDKtfhKWPw/4f4N+j4dwHYMQdYK98FExsqD//nTqEzzYl8/3OVO44uwvtw+th+O2m96A4H9r0gnZDTv/5REREqmEzTjVOtAnIysoiJCSEzMxMgoM98JRBYTbJi2cRnfgGAO7AKOwT50KPCSfvm74X/vcH2Pudeb/tYLhkHrTp2Ti1GgbMHwMpm2D832DEbY3zuiIi0uzU9Pdbp2ka2q6l8MLIsiDyS9hF2O9cXXkQAQjrCFM+gYn/BN9gOLTWDAfL/wbFzoav9/A6M4g4fKHf1Q3/eiIi0uIpjDSU/OPw0R3w5uWQeYAD7khucM4icvKL4B9a/bE2m7kOzIyfoduF5lwfy5+Cl86BQ+satu61JR1Xe10CAWEN+1oiIiLUMYzMmzeP+Ph4/Pz8GD58OKtXr65y31dffRWbzVbh4ufnV+eCPcK2/8G84bD+LcDGd62vYJzzbwT3uYAO4YE1f57gWLj2bbjiZbOja8pm+O958PXDUJRf/3UX5sDmD8zbWhRPREQaSa3DyKJFi0hISOCRRx5h3bp19O/fn3HjxnH06NEqjwkODubIkSNll/37959W0U1WzlF4dyosuh5yUiC8K0ev+ohpKVeShx+3VTH1e7VsNuh7JcxYbXZ4Ndyw8jmYfwbsX1W/9W9ZDM4cCOsMHUbX73OLiIhUodZhZM6cOUyfPp1p06bRq1cv5s+fT0BAAAsWLKjyGJvNRnR0dNklKirqtIpucgwDNiyCecNg60dgc8AZCXDbD8zf06bSBfFqLTACrlwA17wNraLh2C545UL47E9QmF0/72PtCTOu1mXWVhERkTqoVRhxOp2sXbuWsWPHlj+B3c7YsWNZtarqv9JzcnLo0KEDcXFxXHLJJWzZsqXa1yksLCQrK6vCpcnKPAgLr4YPbzX7iUT1henfwthHyCiyn3JBvFrrMcHsSzLwBsAwhwO/MBISP4esI2YwOcX08pVK2QKHfgG7Fwy4rn5qFRERqYFazTOSlpaGy+U6qWUjKiqK7du3V3pM9+7dWbBgAf369SMzM5Nnn32WUaNGsWXLFtq1a1fpMbNnz+axxx6rTWnWSN9r9uHIOwYOHzjrXhg9ExzeALyxqmYL4tWafyhc8rx52uZ/f4CMJHj7mor7+LQquQSCbyvwCSq5blV+feLtXd+Yx3WfAK3a1F+tIiIip9Dgk56NHDmSkSNHlt0fNWoUPXv25D//+Q9PPPFEpcfMmjWLhISEsvtZWVnExcU1dKm1U5gD71xnBpGoPmYn0zY9yh4uKHLx6o/7gBMWxKtvnc+B21fBsidhw9tQkGn2KQGz74czp/bPqY6rIiLSyGoVRiIiInA4HKSkpFTYnpKSQnR0dI2ew9vbm4EDB7Jr164q9/H19cXX17c2pTUutxs+/D0c3QqtomDy+xAcU2GXExfE+13fmCqeqB74toLxs82LYZijbJw55ukaZ27J7RxzqvnCnBO2ZZ/wWK75eGQP6HRuw9UqIiJSiVqFER8fHwYPHszSpUu59NJLAXC73SxdupQ777yzRs/hcrnYtGkTEyZUMemXJ/juGdj+qXlqZtKbJwWRExfEmz6mI16ORprOxWYDnwDzolMtIiLiIWp9miYhIYGpU6cyZMgQhg0bxty5c8nNzWXatGkATJkyhbZt2zJ79mwAHn/8cUaMGEGXLl3IyMjgmWeeYf/+/dxyyy31+04ay/Yl5gRkAL+bA3HDTtrlyy3J7D+WR2iAN1cPbWKnl0RERJqYWoeRSZMmkZqaysMPP0xycjIDBgzgiy++KOvUmpSUhN1e3hJw/Phxpk+fTnJyMq1bt2bw4MH8+OOP9OrVq/7eRWM5ug0W32reHvZ7GHTDSbsYhsH8FeaKu1NGxhPgo7UIRUREqqOF8moqLx1eOheO74X4MXDDh2WjZk704+40rnvpZ3y97Px4/7mEt2rCfV9EREQakBbKq0+uYnj/JjOIhLSHq16rNIgA/GeF2Vfk6iFxCiIiIiI1oDBSE988AnuWgXcAXLsQAsMr3W3bkSxW7EjFboPpYzo1cpEiIiKeSWHkVDYsglXPm7cvfQGi+1a564vfma0iE/rG0D48oDGqExER8XgKI9U5tA4+ucu8PeYe6H1ZlbsePJ7HJxsOA/U49buIiEgLoDBSlewUc/VdVyF0Gw/nPFDt7i//sLd+FsQTERFpYRRGKlNcCO/eAFmHIKIbXP4i2Kv+qI7nOnln9QFArSIiIiK1pTDyW4YBn/0JDvwMviFwzdvgV31Lx5s/7Se/yEWv+l4QT0REpAVQGPmtX16Gda8BNrjyZYjoUu3uJy6I9/uGWhBPRESkGVMYOdG+lfD5febtsY9A1/NPeUijLYgnIiLSTCmMlMpIgnengLsY+lwBo2ee8pANBzL493Jz6vdGXRBPRESkGdHCKQDOPHjnOshLg+h+cPHz5gq4VTh4PI9nvkzk4/XmUN7oYD8tiCciIlJHCiOGAR/PgORNEBAB1ywEn8onLMsqKOKFZbtZsHIvzmI3NhtcNrAtfxrXXQviiYiI1JF+QVfOhS2Lwe4FV78OoSe3cBS53LyzOol/fLOT9FwnACM7hfPA73rSp63mFBERETkdLTuM7PgKvnnMvH3h3yB+dIWHDcPgm21Hmf35Nvak5gLQKTKQP1/Yk/N6ttHIGRERkXrQcsOIMxc+ug0wYPCNMOTmCg9vPpTJX5Zs5ac96QCEB/ow8/xuXDM0Dm91VBUREak3LTeM+ATCpLfgp3lw4TNlHVYPZ+Tz7JeJLP71kLmbl52bz+jI7Wd3JtjP28qKRUREmqWWG0YAOow0L0BOYTH/Xr6L/36/l8JiNwCXDojlnnHdaddaK/CKiIg0lJYdRoBil5t31hxg7jc7SMsxO6cO6xjGg7/rSb92odYWJyIi0gK02DBiGAbLEo/y1Gfb2XU0B4BOEYHcf2EPzu8Vpc6pIiIijaTFhhGny82DH27mcGYBrQO8ufu8rkwe0UGdU0VERBpZiw0jvl4O7p/Qky2HMrnjnC6E+KtzqoiIiBVabBgBuLh/LBf3j7W6DBERkRZN5yRERETEUgojIiIiYimFEREREbGUwoiIiIhYSmFERERELKUwIiIiIpZSGBERERFLKYyIiIiIpRRGRERExFIKIyIiImIphRERERGxlMKIiIiIWEphRERERCzlEav2GoYBQFZWlsWViIiISE2V/m6X/o5XxSPCSHZ2NgBxcXEWVyIiIiK1lZ2dTUhISJWP24xTxZUmwO12c/jwYYKCgrDZbGXbs7KyiIuL48CBAwQHB1tYoefTZ1m/9HnWH32W9UufZ/3RZ3lqhmGQnZ1NbGwsdnvVPUM8omXEbrfTrl27Kh8PDg7WF6Ge6LOsX/o8648+y/qlz7P+6LOsXnUtIqXUgVVEREQspTAiIiIilvLoMOLr68sjjzyCr6+v1aV4PH2W9UufZ/3RZ1m/9HnWH32W9ccjOrCKiIhI8+XRLSMiIiLi+RRGRERExFIKIyIiImIphRERERGxlMeGkXnz5hEfH4+fnx/Dhw9n9erVVpfkkR599FFsNluFS48ePawuyyN89913TJw4kdjYWGw2Gx999FGFxw3D4OGHHyYmJgZ/f3/Gjh3Lzp07rSnWA5zq87zxxhtP+q6OHz/emmKbuNmzZzN06FCCgoJo06YNl156KYmJiRX2KSgoYMaMGYSHh9OqVSuuuOIKUlJSLKq46arJZ3n22Wef9N287bbbLKrYM3lkGFm0aBEJCQk88sgjrFu3jv79+zNu3DiOHj1qdWkeqXfv3hw5cqTs8sMPP1hdkkfIzc2lf//+zJs3r9LHn376af75z38yf/58fv75ZwIDAxk3bhwFBQWNXKlnONXnCTB+/PgK39W33367ESv0HCtWrGDGjBn89NNPfP311xQVFXHBBReQm5tbts8f//hH/ve///Hee++xYsUKDh8+zOWXX25h1U1TTT5LgOnTp1f4bj799NMWVeyhDA80bNgwY8aMGWX3XS6XERsba8yePdvCqjzTI488YvTv39/qMjweYHz44Ydl991utxEdHW0888wzZdsyMjIMX19f4+2337agQs/y28/TMAxj6tSpxiWXXGJJPZ7u6NGjBmCsWLHCMAzzu+jt7W289957Zfts27bNAIxVq1ZZVaZH+O1naRiGcdZZZxl33323dUU1Ax7XMuJ0Olm7di1jx44t22a32xk7diyrVq2ysDLPtXPnTmJjY+nUqROTJ08mKSnJ6pI83t69e0lOTq7wPQ0JCWH48OH6np6G5cuX06ZNG7p3787tt9/OsWPHrC7JI2RmZgIQFhYGwNq1aykqKqrw/ezRowft27fX9/MUfvtZlnrrrbeIiIigT58+zJo1i7y8PCvK81gesVDeidLS0nC5XERFRVXYHhUVxfbt2y2qynMNHz6cV199le7du3PkyBEee+wxxowZw+bNmwkKCrK6PI+VnJwMUOn3tPQxqZ3x48dz+eWX07FjR3bv3s2f//xnLrzwQlatWoXD4bC6vCbL7XYzc+ZMRo8eTZ8+fQDz++nj40NoaGiFffX9rF5lnyXAddddR4cOHYiNjWXjxo3cd999JCYmsnjxYgur9SweF0akfl144YVlt/v168fw4cPp0KED7777LjfffLOFlYlUdM0115Td7tu3L/369aNz584sX76c8847z8LKmrYZM2awefNm9QWrB1V9lrfeemvZ7b59+xITE8N5553H7t276dy5c2OX6ZE87jRNREQEDofjpF7fKSkpREdHW1RV8xEaGkq3bt3YtWuX1aV4tNLvor6nDadTp05ERETou1qNO++8k08//ZRly5bRrl27su3R0dE4nU4yMjIq7K/vZ9Wq+iwrM3z4cAB9N2vB48KIj48PgwcPZunSpWXb3G43S5cuZeTIkRZW1jzk5OSwe/duYmJirC7Fo3Xs2JHo6OgK39OsrCx+/vlnfU/rycGDBzl27Ji+q5UwDIM777yTDz/8kG+//ZaOHTtWeHzw4MF4e3tX+H4mJiaSlJSk7+dvnOqzrMz69esB9N2sBY88TZOQkMDUqVMZMmQIw4YNY+7cueTm5jJt2jSrS/M499xzDxMnTqRDhw4cPnyYRx55BIfDwbXXXmt1aU1eTk5Ohb989u7dy/r16wkLC6N9+/bMnDmTv/zlL3Tt2pWOHTvy0EMPERsby6WXXmpd0U1YdZ9nWFgYjz32GFdccQXR0dHs3r2be++9ly5dujBu3DgLq26aZsyYwcKFC/n4448JCgoq6wcSEhKCv78/ISEh3HzzzSQkJBAWFkZwcDB33XUXI0eOZMSIERZX37Sc6rPcvXs3CxcuZMKECYSHh7Nx40b++Mc/cuaZZ9KvXz+Lq/cgVg/nqat//etfRvv27Q0fHx9j2LBhxk8//WR1SR5p0qRJRkxMjOHj42O0bdvWmDRpkrFr1y6ry/IIy5YtM4CTLlOnTjUMwxze+9BDDxlRUVGGr6+vcd555xmJiYnWFt2EVfd55uXlGRdccIERGRlpeHt7Gx06dDCmT59uJCcnW112k1TZ5wgYr7zyStk++fn5xh133GG0bt3aCAgIMC677DLjyJEj1hXdRJ3qs0xKSjLOPPNMIywszPD19TW6dOli/OlPfzIyMzOtLdzD2AzDMBoz/IiIiIicyOP6jIiIiEjzojAiIiIillIYEREREUspjIiIiIilFEZERETEUgojIiIiYimFEREREbGUwoiIiIhYSmFERERELKUwIiIiIpZSGBERERFLKYyIiIiIpf4fhh6bmGPNmPYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training history\n",
    "epoch_count = range(1, len(history_lstm.history['auc']) + 1)\n",
    "print(epoch_count)\n",
    "plt.plot(epoch_count, history_lstm.history['auc'], label='train')\n",
    "plt.plot(epoch_count, history_lstm.history['val_auc'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Once again, our LSTM performed relatively similar to our simple RNN.\n",
    "# Reasons for this likely is because LSTM does much better with sequential data\n",
    "# that has multiple periods. In our data, we ONLY have 2 time periods (years 0 and 3)\n",
    "# that were used to make predictions on time point 3 (year 6)...\n",
    "# And so, the results for our very simple LSTM were much more like the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 10)                27930     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,941\n",
      "Trainable params: 27,941\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let's keep trying with an additional model, but this time a GRU instanced in the same format as our simple RNN and LSTM.\n",
    "model_GRU = keras.Sequential()\n",
    "model_GRU.add(InputLayer(input_shape=(2, 919)))\n",
    "model_GRU.add(GRU(10, activation='tanh', return_sequences=False))\n",
    "model_GRU.add(Dense(1, activation='sigmoid'))\n",
    "print(model_GRU.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 18.\n",
      "Epoch 28: early stopping\n",
      "Train: 0.924, Test: 0.757\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 1000\n",
    "\n",
    "# patient early stopping\n",
    "es = EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=10, restore_best_weights=True)\n",
    "\n",
    "history_GRU = model_GRU.fit(X_train2, y_train2,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[es]\n",
    "                    )\n",
    "\n",
    "# evaluate the model\n",
    "_, GRU_train_auc = model_GRU.evaluate(X_train, y_train, verbose=0)\n",
    "_, GRU_test_auc = model_GRU.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (GRU_train_auc, GRU_test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 29)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training history\n",
    "epoch_count = range(1, len(history_GRU.history['auc']) + 1)\n",
    "print(epoch_count)\n",
    "plt.plot(epoch_count, history_GRU.history['auc'], label='train')\n",
    "plt.plot(epoch_count, history_GRU.history['val_auc'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Once again, just like our LSTM model, the GRU performed relatively similar to our simple RNN.\n",
    "# Objectively, it appears there are no significant differences between the RNN, LSTM, and GRU\n",
    "# when the sequence of data we feed are not long. However, if we had multiple years of follow-up data\n",
    "# Then perhaps the simple RNN will begin to fade out with either vanishing or exploding gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 2, 10)             37200     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 10)                660       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,871\n",
      "Trainable params: 37,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Will there be a difference if we add one LSTM layer, and one GRU layer?? Let's find out.\n",
    "\n",
    "model_lsRU = keras.Sequential()\n",
    "model_lsRU.add(InputLayer(input_shape=(2, 919)))\n",
    "model_lsRU.add(LSTM(10, activation='tanh', return_sequences=True))\n",
    "model_lsRU.add(GRU(10, activation='tanh', return_sequences=False))\n",
    "model_lsRU.add(Dense(1, activation='sigmoid'))\n",
    "print(model_lsRU.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lsRU.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 20.\n",
      "Epoch 30: early stopping\n",
      "Train: 0.926, Test: 0.746\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 1000\n",
    "\n",
    "# patient early stopping\n",
    "es = EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=10, restore_best_weights=True)\n",
    "\n",
    "history_lsRU = model_lsRU.fit(X_train2, y_train2,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[es]\n",
    "                    )\n",
    "\n",
    "# evaluate the model\n",
    "_, lsRU_train_auc = model_lsRU.evaluate(X_train, y_train, verbose=0)\n",
    "_, lsRU_test_auc = model_lsRU.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (lsRU_train_auc, lsRU_test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 31)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training history\n",
    "epoch_count = range(1, len(history_lsRU.history['auc']) + 1)\n",
    "print(epoch_count)\n",
    "plt.plot(epoch_count, history_lsRU.history['auc'], label='train')\n",
    "plt.plot(epoch_count, history_lsRU.history['val_auc'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# It does not significantly improve upon our testing AUC as it seems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleRNN plus an additional dense/hidden layer with 10 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_1 (SimpleRNN)    (None, 10)                9300      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,421\n",
      "Trainable params: 9,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Going back to our most basic RNN model, let's add one additional hidden layer with 10 nodes\n",
    "# And see how this improves/decreases model performance.\n",
    "\n",
    "model_simpleRNN2 = keras.Sequential()\n",
    "model_simpleRNN2.add(InputLayer(input_shape=(2, 919)))\n",
    "model_simpleRNN2.add(SimpleRNN(10, activation='tanh', return_sequences=False, dropout=0.2))\n",
    "model_simpleRNN2.add(Dense(10, activation='relu'))\n",
    "model_simpleRNN2.add(Dense(1, activation='sigmoid'))\n",
    "print(model_simpleRNN2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simpleRNN2.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 18.\n",
      "Epoch 28: early stopping\n",
      "Train: 0.899, Test: 0.759\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 1000\n",
    "\n",
    "# patient early stopping\n",
    "es = EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=10, restore_best_weights=True)\n",
    "\n",
    "history_simpleRNN2 = model_simpleRNN2.fit(X_train2, y_train2,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[es]\n",
    "                    )\n",
    "\n",
    "# evaluate the model\n",
    "_, simpleRNN2_train_auc = model_simpleRNN2.evaluate(X_train, y_train, verbose=0)\n",
    "_, simpleRNN2_test_auc = model_simpleRNN2.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (simpleRNN2_train_auc, simpleRNN2_test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 29)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training history\n",
    "epoch_count = range(1, len(history_simpleRNN2.history['auc']) + 1)\n",
    "print(epoch_count)\n",
    "plt.plot(epoch_count, history_simpleRNN2.history['auc'], label='train')\n",
    "plt.plot(epoch_count, history_simpleRNN2.history['val_auc'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Still not a significant change as above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1b273ec",
   "metadata": {},
   "source": [
    "##### Given that now we have tried multiple different combos (RNN, LSTM, GRU, and combos of them all), we still see that our AUC is pretty much capped at the 0.7-0.8 region for our testing sets. This is where the concept of hyperparameter tuning comes into play. We will now begin our BayesianCV Optimization from Sci-Kit Optimize in order to truly find a model which we believe gives us the best AUC aka fit. To simplify things, we will only use this on an LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75c500c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need to import a ton of packages from sci-kit Optimize (skopt) in order for this\n",
    "# to work.\n",
    "from skopt import gp_minimize, BayesSearchCV, dump, load\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.utils import use_named_args\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gp_minimize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8634d60b",
   "metadata": {},
   "source": [
    "##### gp_minimize is skopt's Bayesian optimiziation function which uses the Gaussian Process. This is used because if the function evaluation process is expensive, a standard optimization using gridsearch or randomsearch would take a very, very long time.\n",
    "\n",
    "##### The idea here is to approximate a search function using the Gaussian process, which takes the function values that follows a multivariate gaussian with the covariance given by a GP kernel between the parameters. The next parameter chosen to be evaluated then can be made by an acquisition function over the Gaussian prior, and this will make the tuning much quicker to execute.\n",
    "\n",
    "##### For more info on this, check out this skopt link: https://scikit-optimize.github.io/stable/modules/generated/skopt.gp_minimize.html\n",
    "\n",
    "##### Also a shout out to Ryan Mardani's blog post here which was mainly borrowed to construct the optimization process in our instance: https://towardsdatascience.com/bayesian-hyper-parameter-optimization-neural-networks-tensorflow-facies-prediction-example-f9c48d21f795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b34653d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first set some fixed values.\n",
    "num_timestamp = 2 # we have 2 because of 2 follow up years\n",
    "num_features = 919\n",
    "num_output = 1 # 1 output classifying either 0=no heart fail; 1=heart fail.\n",
    "epochs = 1000\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af2d570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's begin to define our search space with all the potential hyperparameters that can be tried.\n",
    "dim_num_lstm_layers = Integer(low=0, high=3, name='num_lstm_layers')\n",
    "dim_num_lstm_nodes = Categorical([5, 10, 20, 30], name='num_lstm_nodes')\n",
    "dim_act_lstm = Categorical(['tanh', 'sigmoid'], name='act_lstm')\n",
    "\n",
    "dim_num_dense_layers = Integer(low=0, high=5, name='num_dense_layers')\n",
    "dim_num_dense_nodes = Integer(low=1, high=30, name='num_dense_nodes')\n",
    "dim_act_dense = Categorical(['relu', 'tanh', 'sigmoid', 'softmax'], name='act_dense')\n",
    "\n",
    "dim_act_last = Categorical(['sigmoid', 'softmax'], name='act_last')\n",
    "\n",
    "dim_l1_lambda = Categorical([0.0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6], name='l1_lambda')\n",
    "dim_l2_lambda = Categorical([0.0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6], name='l2_lambda')\n",
    "dim_dropout_rate = Categorical([0.0, 0.1, 0.2, 0.3, 0.4, 0.5], name='dropout_rate') # drop out is a regularization format to avoid overfitting. \n",
    "                                                                                    # Default interpretation her is probability of training a given node (1.0=no dropout; 0.0=no output)\n",
    "\n",
    "dim_learning_rate = Real(low=1e-5, high=1e-1, prior=\"log-uniform\", name='learning_rate')\n",
    "\n",
    "dimensions = [dim_num_lstm_layers, dim_act_lstm, dim_num_lstm_nodes, dim_dropout_rate, dim_num_dense_layers, dim_num_dense_nodes, \n",
    "              dim_act_dense, dim_learning_rate, dim_l1_lambda, dim_l2_lambda, dim_act_last] # This creates one searchable list for all our hyperparameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's now define our model function here, which we have to specify how we are going to loop through the dimensions we created earlier.\n",
    "\n",
    "# 1. We will create our model function\n",
    "def create_model(num_lstm_layers, act_lstm, num_lstm_nodes, dropout_rate, num_dense_layers, num_dense_nodes, act_dense, \n",
    "                 learning_rate, l1_lambda, l2_lambda, act_last):\n",
    "    # Just like before, let's start with a sequential model.\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Initialize an input layer first.\n",
    "    model.add(InputLayer(input_shape=(num_timestamp, num_features)))\n",
    "\n",
    "    # Initialize the first LSTM layers by looping through each value of 'num_lstm_layers'\n",
    "    for i in range(num_lstm_layers):\n",
    "        # We can name each layer though Keras will give them by default too.\n",
    "        name = 'layer_lstm_{0}'.format(i+1)\n",
    "\n",
    "        # For each LSTM layers specify the nodes, act function, name, drop out rate, and regularizers.\n",
    "        model.add(LSTM(np.int32(num_lstm_nodes),\n",
    "                       activation=str(act_lstm),\n",
    "                       name=name,\n",
    "                       return_sequences=True,\n",
    "                       dropout=dropout_rate,\n",
    "                       recurrent_regularizer=l1_l2(l1=l1_lambda, l2=l2_lambda)))\n",
    "    \n",
    "    # Initialize one more LSTM layer prior to adding dense layers\n",
    "    model.add(LSTM(np.int32(num_lstm_nodes),\n",
    "                       activation=str(act_lstm),\n",
    "                       return_sequences=False))\n",
    "    \n",
    "    # Initialize a series of dense layers specified by 'num_dense_layers'\n",
    "    for i in range(num_dense_layers):\n",
    "        name = 'layer_dense_{0}'.format(i+1)\n",
    "\n",
    "        # For each dense/hidden layer, specify nodes, activation function, and the name.\n",
    "        model.add(Dense(np.int32(num_dense_nodes),\n",
    "                        activation=str(act_dense),\n",
    "                        name=name))\n",
    "    \n",
    "    # Initialize the last dense layer, and specify how many output nodes.\n",
    "    model.add(Dense(num_output, activation=str(act_last)))\n",
    "\n",
    "    # Finally, compile our model with a binary cross entropy loss function and the ADAM optimizer.\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(learning_rate),\n",
    "              metrics=['AUC'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbe15682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a log file to keep track all the models and the hyperparameters we are tuning.\n",
    "def log_dir_name(num_lstm_layers, act_lstm, num_lstm_nodes, dropout_rate, num_dense_layers, num_dense_nodes, act_dense, \n",
    "                 learning_rate, l1_lambda, l2_lambda, act_last):\n",
    "    # The dir-name for the TensorBoard log-dir.\n",
    "    s = os.path.join(log_path, \"lr_{0:.0e}_kernel_{1}_dense_{2}/\")\n",
    "    \n",
    "    log_dir = s.format(num_lstm_layers, act_lstm, num_lstm_nodes, dropout_rate, num_dense_layers, num_dense_nodes, act_dense, \n",
    "                       learning_rate, l1_lambda, l2_lambda, act_last)\n",
    "    return log_dir\n",
    "\n",
    "log_path = 'C:/Your_Path/WHIchf_skopt_log'\n",
    "if not os.path.exists(log_path):\n",
    "    os.makedirs(log_path)\n",
    "result_path = 'C:/Your_Path/WHIchf_skopt_result'\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)\n",
    "path_best_model = os.path.join(result_path+'best_model.keras')\n",
    "best_auc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "095f5f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. We then create a fitness function, which fits the model and returns a fitness value and the AUC score.\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(num_lstm_layers, act_lstm, num_lstm_nodes, dropout_rate, num_dense_layers, num_dense_nodes, act_dense, \n",
    "            learning_rate, l1_lambda, l2_lambda, act_last):\n",
    "    # Print the hyper-parameters.\n",
    "    print('num_lstm_layers:', num_lstm_layers)\n",
    "    print('activation_lstm:', act_lstm)\n",
    "    print('num_lstm_nodes:', num_lstm_nodes)\n",
    "    print('dropout_rate:', dropout_rate)\n",
    "    print('num_dense_layers:', num_dense_layers)\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('activation_dense:', act_dense)\n",
    "    print('learning_rate: {0:.1e}'.format(learning_rate))\n",
    "    print('l1_lambda: {0:.1e}'.format(l1_lambda))\n",
    "    print('l2_lambda: {0:.1e}'.format(l2_lambda))\n",
    "    print('act_last:', act_last)\n",
    "    print()\n",
    "    # Create the neural network model now with the create_model function earlier with these hyper-parameters.\n",
    "    model = create_model(num_lstm_layers, act_lstm, num_lstm_nodes, dropout_rate, num_dense_layers, num_dense_nodes, act_dense, \n",
    "            learning_rate, l1_lambda, l2_lambda, act_last)\n",
    "     \n",
    "    # Creates log files with all hyperparameters from earlier.\n",
    "    log_dir = log_dir_name(num_lstm_layers, act_lstm, num_lstm_nodes, dropout_rate, num_dense_layers, num_dense_nodes, act_dense, \n",
    "            learning_rate, l1_lambda, l2_lambda, act_last)\n",
    "     \n",
    "    # Create a callback-function for Keras which will be run after each epoch has ended during training.\n",
    "    # This saves the log-files for TensorBoard.\n",
    "    # Note that there are complications when histogram_freq=1.\n",
    "    # It might give strange errors and it also does not properly\n",
    "    # support Keras data-generators for the validation-set.\n",
    "    callback_log = keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_grads=False,\n",
    "        write_images=False)\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=20, restore_best_weights=True)\n",
    "    \n",
    "    # Use Keras to train the model.\n",
    "    history = model.fit(X_train2, y_train2,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        callbacks=[es, callback_log])\n",
    "                        \n",
    "    # Get the classification AUC on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    auc = history.history['val_auc'][-1]\n",
    "\n",
    "    # Print the classification AUC.\n",
    "    print()\n",
    "    print(\"Validation AUC: {0:.2%}\".format(auc))\n",
    "    print()\n",
    "    \n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_auc\n",
    "\n",
    "    # If the classification auc of the saved model is improved ...\n",
    "    if auc > best_auc:\n",
    "        # Save the new model to harddisk.\n",
    "        model.save(path_best_model)\n",
    "        \n",
    "        # Update the classification accuracy.\n",
    "        best_auc = auc\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    # NOTE: Scikit-optimize does minimization so it tries to\n",
    "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
    "    # Because we are interested in the HIGHEST classification\n",
    "    # auc, we need to negate this number so it can be minimized.\n",
    "    return -auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cefade7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define our initial hyperparameters for search\n",
    "default_parameters = [0, 'tanh', 10, 0.0, 0, 1, 'sigmoid', 1e-3, 0.0, 0.0, 'sigmoid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d76d399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_lstm_layers: 0\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.0\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 1\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 1.0e-03\n",
      "l1_lambda: 0.0e+00\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 3s 19ms/step - loss: 0.1535 - auc: 0.5255 - val_loss: 0.0898 - val_auc: 0.5276\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0974 - auc: 0.5590 - val_loss: 0.0891 - val_auc: 0.6048\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0966 - auc: 0.6925 - val_loss: 0.0885 - val_auc: 0.6750\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0953 - auc: 0.7176 - val_loss: 0.0875 - val_auc: 0.7354\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0937 - auc: 0.8105 - val_loss: 0.0865 - val_auc: 0.7797\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0923 - auc: 0.8195 - val_loss: 0.0853 - val_auc: 0.7830\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0896 - auc: 0.8511 - val_loss: 0.0839 - val_auc: 0.8076\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0865 - auc: 0.8489 - val_loss: 0.0829 - val_auc: 0.8056\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0829 - auc: 0.8693 - val_loss: 0.0810 - val_auc: 0.8216\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0788 - auc: 0.8859 - val_loss: 0.0783 - val_auc: 0.8255\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0755 - auc: 0.8899 - val_loss: 0.0773 - val_auc: 0.8194\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0724 - auc: 0.9040 - val_loss: 0.0762 - val_auc: 0.8346\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0684 - auc: 0.9175 - val_loss: 0.0751 - val_auc: 0.8388\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 0s 8ms/step - loss: 0.0659 - auc: 0.9180 - val_loss: 0.0747 - val_auc: 0.8290\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0639 - auc: 0.9323 - val_loss: 0.0783 - val_auc: 0.8248\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0605 - auc: 0.9385 - val_loss: 0.0754 - val_auc: 0.8443\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0572 - auc: 0.9435 - val_loss: 0.0785 - val_auc: 0.8398\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0558 - auc: 0.9429 - val_loss: 0.0797 - val_auc: 0.8448\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0528 - auc: 0.9542 - val_loss: 0.0782 - val_auc: 0.8273\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0508 - auc: 0.9608 - val_loss: 0.0783 - val_auc: 0.8363\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0487 - auc: 0.9589 - val_loss: 0.0804 - val_auc: 0.8141\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0462 - auc: 0.9604 - val_loss: 0.0808 - val_auc: 0.8260\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0444 - auc: 0.9661 - val_loss: 0.0817 - val_auc: 0.8094\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0432 - auc: 0.9643 - val_loss: 0.0834 - val_auc: 0.8111\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0424 - auc: 0.9603 - val_loss: 0.0844 - val_auc: 0.8186\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0397 - auc: 0.9686 - val_loss: 0.0858 - val_auc: 0.8155\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0385 - auc: 0.9658 - val_loss: 0.0877 - val_auc: 0.8100\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0375 - auc: 0.9665 - val_loss: 0.0888 - val_auc: 0.7958\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0357 - auc: 0.9733 - val_loss: 0.0909 - val_auc: 0.7281\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0345 - auc: 0.9740 - val_loss: 0.0911 - val_auc: 0.7226\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0340 - auc: 0.9792 - val_loss: 0.0923 - val_auc: 0.7418\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.0323 - auc: 0.9770 - val_loss: 0.0933 - val_auc: 0.7337\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.0298 - auc: 0.9836 - val_loss: 0.0958 - val_auc: 0.7394\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0288 - auc: 0.9849 - val_loss: 0.0968 - val_auc: 0.7258\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0277 - auc: 0.9858 - val_loss: 0.0990 - val_auc: 0.7114\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.0269 - auc: 0.9906 - val_loss: 0.0994 - val_auc: 0.7173\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0248 - auc: 0.9921 - val_loss: 0.1025 - val_auc: 0.6487\n",
      "Epoch 38/1000\n",
      "27/39 [===================>..........] - ETA: 0s - loss: 0.0221 - auc: 0.9986Restoring model weights from the end of the best epoch: 18.\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.0236 - auc: 0.9926 - val_loss: 0.1018 - val_auc: 0.7121\n",
      "Epoch 38: early stopping\n",
      "\n",
      "Validation AUC: 71.21%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.1\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 14\n",
      "activation_dense: softmax\n",
      "learning_rate: 7.6e-02\n",
      "l1_lambda: 1.0e-04\n",
      "l2_lambda: 1.0e-03\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 6s 45ms/step - loss: 0.1859 - auc: 0.5187 - val_loss: 0.0930 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.1016 - auc: 0.5000 - val_loss: 0.0924 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.1015 - auc: 0.5000 - val_loss: 0.0930 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 0s 13ms/step - loss: 0.1016 - auc: 0.5000 - val_loss: 0.0939 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 0.1016 - auc: 0.4934 - val_loss: 0.0936 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 0.1021 - auc: 0.4636 - val_loss: 0.0930 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 0.1019 - auc: 0.4749 - val_loss: 0.0936 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 0.1018 - auc: 0.4650 - val_loss: 0.0934 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 0.1018 - auc: 0.5000 - val_loss: 0.0932 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.1019 - auc: 0.5000 - val_loss: 0.0932 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.1019 - auc: 0.4555 - val_loss: 0.0942 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.1020 - auc: 0.4702 - val_loss: 0.0941 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 0.1019 - auc: 0.4721 - val_loss: 0.0927 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.1021 - auc: 0.5142 - val_loss: 0.0928 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.1019 - auc: 0.4579 - val_loss: 0.0943 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.1018 - auc: 0.5109 - val_loss: 0.0944 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.1019 - auc: 0.5070 - val_loss: 0.0939 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.1018 - auc: 0.5028 - val_loss: 0.0922 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.1021 - auc: 0.4782 - val_loss: 0.0938 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.1019 - auc: 0.4801 - val_loss: 0.0944 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 0.0993 - auc: 0.4929Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.1019 - auc: 0.5066 - val_loss: 0.0940 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.0\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 26\n",
      "activation_dense: tanh\n",
      "learning_rate: 1.0e-03\n",
      "l1_lambda: 1.0e-03\n",
      "l2_lambda: 1.0e-06\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 5s 23ms/step - loss: 0.5505 - auc: 0.5326 - val_loss: 0.3251 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.2319 - auc: 0.5080 - val_loss: 0.1594 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.1378 - auc: 0.5307 - val_loss: 0.1146 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.1144 - auc: 0.4751 - val_loss: 0.1018 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.1066 - auc: 0.4755 - val_loss: 0.0964 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.1024 - auc: 0.5000 - val_loss: 0.0932 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 0.0998 - auc: 0.5473 - val_loss: 0.0912 - val_auc: 0.6911\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.0982 - auc: 0.7686 - val_loss: 0.0899 - val_auc: 0.7629\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0971 - auc: 0.6823 - val_loss: 0.0890 - val_auc: 0.7035\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0959 - auc: 0.7538 - val_loss: 0.0881 - val_auc: 0.7514\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.0950 - auc: 0.7868 - val_loss: 0.0876 - val_auc: 0.7629\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.0943 - auc: 0.7917 - val_loss: 0.0871 - val_auc: 0.7802\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.0937 - auc: 0.8305 - val_loss: 0.0867 - val_auc: 0.7957\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 0s 8ms/step - loss: 0.0930 - auc: 0.8379 - val_loss: 0.0865 - val_auc: 0.7267\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.0922 - auc: 0.8420 - val_loss: 0.0861 - val_auc: 0.7273\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0914 - auc: 0.8450 - val_loss: 0.0854 - val_auc: 0.7837\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 0.0905 - auc: 0.8729 - val_loss: 0.0849 - val_auc: 0.7833\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0895 - auc: 0.8794 - val_loss: 0.0846 - val_auc: 0.8177\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 0.0884 - auc: 0.8964 - val_loss: 0.0838 - val_auc: 0.8053\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0876 - auc: 0.8981 - val_loss: 0.0833 - val_auc: 0.8193\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0865 - auc: 0.9097 - val_loss: 0.0829 - val_auc: 0.8298\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0847 - auc: 0.9073 - val_loss: 0.0822 - val_auc: 0.8012\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0830 - auc: 0.9129 - val_loss: 0.0818 - val_auc: 0.8109\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0819 - auc: 0.9220 - val_loss: 0.0811 - val_auc: 0.7915\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.0798 - auc: 0.9270 - val_loss: 0.0799 - val_auc: 0.7929\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0779 - auc: 0.9252 - val_loss: 0.0791 - val_auc: 0.8096\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0758 - auc: 0.9311 - val_loss: 0.0783 - val_auc: 0.8283\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 0.0737 - auc: 0.9385 - val_loss: 0.0776 - val_auc: 0.8307\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 0.0711 - auc: 0.9484 - val_loss: 0.0796 - val_auc: 0.8063\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0690 - auc: 0.9401 - val_loss: 0.0781 - val_auc: 0.8035\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.0669 - auc: 0.9408 - val_loss: 0.0772 - val_auc: 0.8234\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0635 - auc: 0.9482 - val_loss: 0.0765 - val_auc: 0.8085\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0608 - auc: 0.9505 - val_loss: 0.0757 - val_auc: 0.8383\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0584 - auc: 0.9505 - val_loss: 0.0798 - val_auc: 0.7526\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.0550 - auc: 0.9584 - val_loss: 0.0771 - val_auc: 0.8151\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.0520 - auc: 0.9611 - val_loss: 0.0823 - val_auc: 0.8185\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0486 - auc: 0.9591 - val_loss: 0.0807 - val_auc: 0.8223\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 0.0463 - auc: 0.9641 - val_loss: 0.0848 - val_auc: 0.7966\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 0.0423 - auc: 0.9722 - val_loss: 0.0861 - val_auc: 0.7963\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0402 - auc: 0.9616 - val_loss: 0.0878 - val_auc: 0.8036\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0381 - auc: 0.9665 - val_loss: 0.0913 - val_auc: 0.7619\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0351 - auc: 0.9736 - val_loss: 0.0933 - val_auc: 0.7610\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0333 - auc: 0.9748 - val_loss: 0.0957 - val_auc: 0.7279\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0320 - auc: 0.9759 - val_loss: 0.0983 - val_auc: 0.7240\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.0288 - auc: 0.9827 - val_loss: 0.1006 - val_auc: 0.7253\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0272 - auc: 0.9833 - val_loss: 0.1030 - val_auc: 0.7344\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0254 - auc: 0.9845 - val_loss: 0.1048 - val_auc: 0.7240\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.0231 - auc: 0.9856 - val_loss: 0.1056 - val_auc: 0.7294\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.0215 - auc: 0.9908 - val_loss: 0.1088 - val_auc: 0.6635\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.0220 - auc: 0.9915 - val_loss: 0.1111 - val_auc: 0.6790\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0183 - auc: 0.9922 - val_loss: 0.1128 - val_auc: 0.6746\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 0.0173 - auc: 0.9926 - val_loss: 0.1165 - val_auc: 0.7123\n",
      "Epoch 53/1000\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.0155 - auc: 0.9931Restoring model weights from the end of the best epoch: 33.\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.0151 - auc: 0.9933 - val_loss: 0.1182 - val_auc: 0.6617\n",
      "Epoch 53: early stopping\n",
      "\n",
      "Validation AUC: 66.17%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.1\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 14\n",
      "activation_dense: tanh\n",
      "learning_rate: 6.8e-02\n",
      "l1_lambda: 0.0e+00\n",
      "l2_lambda: 1.0e-01\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 12s 65ms/step - loss: 0.5018 - auc: 0.5000 - val_loss: 0.1111 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.1055 - auc: 0.5000 - val_loss: 0.0913 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.0993 - auc: 0.5000 - val_loss: 0.0902 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.0994 - auc: 0.5000 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.0992 - auc: 0.5000 - val_loss: 0.0912 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.0999 - auc: 0.5000 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.0979 - auc: 0.5000 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.0989 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.0980 - auc: 0.5000 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.0978 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.0984 - auc: 0.5000 - val_loss: 0.0955 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.1041 - auc: 0.5000 - val_loss: 0.0925 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.1005 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.0984 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.1015 - auc: 0.5000 - val_loss: 0.0901 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.0989 - auc: 0.5000 - val_loss: 0.0901 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.0999 - auc: 0.5000 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.0993 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0986 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0996 - auc: 0.5000 - val_loss: 0.0908 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.0983 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0993 - auc: 0.5000 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.1\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 20\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 9.3e-03\n",
      "l1_lambda: 1.0e-04\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 12s 78ms/step - loss: 0.2309 - auc: 0.5295 - val_loss: 0.0923 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.1002 - auc: 0.4960 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.0981 - auc: 0.4970 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0978 - auc: 0.4789 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.0978 - auc: 0.4714 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0979 - auc: 0.4844 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0979 - auc: 0.4855 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0980 - auc: 0.4766 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0979 - auc: 0.4803 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.0980 - auc: 0.4549 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0978 - auc: 0.4771 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0979 - auc: 0.4661 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0979 - auc: 0.4796 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0979 - auc: 0.4541 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.0978 - auc: 0.4782 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0977 - auc: 0.4799 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0978 - auc: 0.5050 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0979 - auc: 0.4995 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.0977 - auc: 0.5211 - val_loss: 0.0902 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0978 - auc: 0.5088 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.1007 - auc: 0.4725Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0978 - auc: 0.4716 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.3\n",
      "num_dense_layers: 4\n",
      "num_dense_nodes: 23\n",
      "activation_dense: relu\n",
      "learning_rate: 2.9e-02\n",
      "l1_lambda: 1.0e-04\n",
      "l2_lambda: 1.0e-02\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 10s 50ms/step - loss: 0.1676 - auc: 0.4374 - val_loss: 0.0916 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.0993 - auc: 0.5415 - val_loss: 0.0903 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.0992 - auc: 0.5199 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.1045 - auc: 0.4817 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.1026 - auc: 0.5182 - val_loss: 0.0928 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0993 - auc: 0.4928 - val_loss: 0.0916 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.1012 - auc: 0.5051 - val_loss: 0.0924 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.1023 - auc: 0.4588 - val_loss: 0.0937 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.0997 - auc: 0.4878 - val_loss: 0.0915 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.0993 - auc: 0.4602 - val_loss: 0.0915 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0986 - auc: 0.4874 - val_loss: 0.0904 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.0994 - auc: 0.5036 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.0993 - auc: 0.4915 - val_loss: 0.0912 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.0997 - auc: 0.5028 - val_loss: 0.0909 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.0995 - auc: 0.4798 - val_loss: 0.0914 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.1003 - auc: 0.4809 - val_loss: 0.0907 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0988 - auc: 0.4339 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.0983 - auc: 0.5076 - val_loss: 0.0920 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.1007 - auc: 0.4949 - val_loss: 0.0920 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.1005 - auc: 0.4502 - val_loss: 0.0906 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1000 - auc: 0.4908Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.1000 - auc: 0.4908 - val_loss: 0.0908 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.2\n",
      "num_dense_layers: 4\n",
      "num_dense_nodes: 28\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 1.1e-04\n",
      "l1_lambda: 1.0e-02\n",
      "l2_lambda: 1.0e-02\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 7s 44ms/step - loss: 0.9404 - auc: 0.5000 - val_loss: 0.8836 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.8332 - auc: 0.5000 - val_loss: 0.7822 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.7379 - auc: 0.5000 - val_loss: 0.6924 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.6538 - auc: 0.5000 - val_loss: 0.6137 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.5801 - auc: 0.5000 - val_loss: 0.5445 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.5154 - auc: 0.5000 - val_loss: 0.4838 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.4591 - auc: 0.5000 - val_loss: 0.4311 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.4101 - auc: 0.5000 - val_loss: 0.3857 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.3682 - auc: 0.5000 - val_loss: 0.3462 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.3319 - auc: 0.5000 - val_loss: 0.3129 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.3010 - auc: 0.5000 - val_loss: 0.2842 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.2746 - auc: 0.5000 - val_loss: 0.2596 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.2521 - auc: 0.5000 - val_loss: 0.2386 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.2328 - auc: 0.5000 - val_loss: 0.2207 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.2164 - auc: 0.5000 - val_loss: 0.2054 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.2023 - auc: 0.5000 - val_loss: 0.1922 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.1902 - auc: 0.5000 - val_loss: 0.1807 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.1797 - auc: 0.5000 - val_loss: 0.1708 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.1706 - auc: 0.5000 - val_loss: 0.1623 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.1628 - auc: 0.5000 - val_loss: 0.1548 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.1563 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.1559 - auc: 0.5000 - val_loss: 0.1483 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.2\n",
      "num_dense_layers: 5\n",
      "num_dense_nodes: 22\n",
      "activation_dense: tanh\n",
      "learning_rate: 3.4e-05\n",
      "l1_lambda: 1.0e-05\n",
      "l2_lambda: 1.0e-04\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 14s 88ms/step - loss: 0.6859 - auc: 0.4424 - val_loss: 0.6700 - val_auc: 0.4893\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.6526 - auc: 0.4986 - val_loss: 0.6284 - val_auc: 0.4079\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.5999 - auc: 0.4781 - val_loss: 0.5581 - val_auc: 0.4029\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.5207 - auc: 0.4821 - val_loss: 0.4689 - val_auc: 0.3802\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.4344 - auc: 0.5384 - val_loss: 0.3835 - val_auc: 0.3760\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.3572 - auc: 0.4820 - val_loss: 0.3120 - val_auc: 0.3883\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.2942 - auc: 0.4775 - val_loss: 0.2568 - val_auc: 0.3402\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.2463 - auc: 0.5004 - val_loss: 0.2159 - val_auc: 0.3656\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.2112 - auc: 0.4838 - val_loss: 0.1861 - val_auc: 0.3527\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.1849 - auc: 0.5104 - val_loss: 0.1644 - val_auc: 0.3777\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.1655 - auc: 0.5177 - val_loss: 0.1486 - val_auc: 0.4323\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.1517 - auc: 0.4610 - val_loss: 0.1366 - val_auc: 0.4023\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.1404 - auc: 0.5279 - val_loss: 0.1276 - val_auc: 0.4156\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.1326 - auc: 0.4736 - val_loss: 0.1207 - val_auc: 0.4616\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.1264 - auc: 0.5126 - val_loss: 0.1154 - val_auc: 0.4342\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.1218 - auc: 0.4522 - val_loss: 0.1112 - val_auc: 0.4397\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.1173 - auc: 0.4839 - val_loss: 0.1079 - val_auc: 0.4794\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.1147 - auc: 0.4470 - val_loss: 0.1051 - val_auc: 0.4988\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.1123 - auc: 0.4817 - val_loss: 0.1031 - val_auc: 0.4604\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.1103 - auc: 0.4611 - val_loss: 0.1013 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.1084 - auc: 0.4751 - val_loss: 0.0999 - val_auc: 0.4827\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.1070 - auc: 0.4946 - val_loss: 0.0987 - val_auc: 0.5000\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.1058 - auc: 0.5239 - val_loss: 0.0977 - val_auc: 0.4205\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.1051 - auc: 0.4836 - val_loss: 0.0969 - val_auc: 0.4988\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.1042 - auc: 0.4908 - val_loss: 0.0961 - val_auc: 0.5000\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.1035 - auc: 0.5031 - val_loss: 0.0955 - val_auc: 0.4789\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.1027 - auc: 0.5244 - val_loss: 0.0950 - val_auc: 0.4757\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.1028 - auc: 0.4942 - val_loss: 0.0946 - val_auc: 0.4996\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.1024 - auc: 0.4852 - val_loss: 0.0942 - val_auc: 0.5000\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.1020 - auc: 0.4952 - val_loss: 0.0939 - val_auc: 0.5000\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.1016 - auc: 0.4878 - val_loss: 0.0936 - val_auc: 0.5000\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.1013 - auc: 0.5066 - val_loss: 0.0934 - val_auc: 0.4178\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.1011 - auc: 0.5087 - val_loss: 0.0932 - val_auc: 0.4505\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.1011 - auc: 0.4864 - val_loss: 0.0930 - val_auc: 0.4963\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.1008 - auc: 0.5031 - val_loss: 0.0929 - val_auc: 0.4996\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.1006 - auc: 0.5309 - val_loss: 0.0928 - val_auc: 0.5000\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.1006 - auc: 0.5045 - val_loss: 0.0926 - val_auc: 0.5000\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.1004 - auc: 0.4996 - val_loss: 0.0925 - val_auc: 0.5000\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.1008 - auc: 0.4553 - val_loss: 0.0924 - val_auc: 0.5000\n",
      "Epoch 40/1000\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.1025 - auc: 0.5082Restoring model weights from the end of the best epoch: 20.\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.1003 - auc: 0.5085 - val_loss: 0.0923 - val_auc: 0.5000\n",
      "Epoch 40: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.0\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 20\n",
      "activation_dense: tanh\n",
      "learning_rate: 5.5e-05\n",
      "l1_lambda: 1.0e-03\n",
      "l2_lambda: 1.0e-03\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 11s 80ms/step - loss: 0.8685 - auc: 0.5146 - val_loss: 0.7740 - val_auc: 0.5403\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.6934 - auc: 0.4885 - val_loss: 0.6106 - val_auc: 0.4629\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.5567 - auc: 0.5331 - val_loss: 0.5035 - val_auc: 0.4442\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.4726 - auc: 0.4838 - val_loss: 0.4380 - val_auc: 0.4229\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.4207 - auc: 0.4543 - val_loss: 0.3967 - val_auc: 0.4238\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.3872 - auc: 0.4943 - val_loss: 0.3691 - val_auc: 0.4537\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.3642 - auc: 0.4631 - val_loss: 0.3491 - val_auc: 0.4704\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.3472 - auc: 0.4671 - val_loss: 0.3340 - val_auc: 0.4587\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.3339 - auc: 0.4505 - val_loss: 0.3216 - val_auc: 0.4635\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.3228 - auc: 0.4848 - val_loss: 0.3112 - val_auc: 0.4573\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.3132 - auc: 0.4858 - val_loss: 0.3019 - val_auc: 0.4804\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.3046 - auc: 0.5092 - val_loss: 0.2935 - val_auc: 0.4636\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.2967 - auc: 0.4724 - val_loss: 0.2858 - val_auc: 0.4553\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.2894 - auc: 0.4934 - val_loss: 0.2786 - val_auc: 0.4755\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.2825 - auc: 0.4993 - val_loss: 0.2718 - val_auc: 0.4970\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.2759 - auc: 0.4857 - val_loss: 0.2652 - val_auc: 0.5293\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.2695 - auc: 0.5368 - val_loss: 0.2589 - val_auc: 0.5019\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.2634 - auc: 0.5155 - val_loss: 0.2528 - val_auc: 0.4657\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.2575 - auc: 0.5181 - val_loss: 0.2470 - val_auc: 0.4799\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.2518 - auc: 0.5183 - val_loss: 0.2414 - val_auc: 0.4937\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2441 - auc: 0.5059Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.2462 - auc: 0.5017 - val_loss: 0.2359 - val_auc: 0.4780\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 47.80%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.2\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 27\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 4.9e-05\n",
      "l1_lambda: 1.0e-03\n",
      "l2_lambda: 1.0e-06\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 14s 85ms/step - loss: 1.2237 - auc: 0.5000 - val_loss: 1.1988 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 1.1736 - auc: 0.5000 - val_loss: 1.1494 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 1.1252 - auc: 0.5000 - val_loss: 1.1017 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 1.0786 - auc: 0.5000 - val_loss: 1.0558 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 1.0337 - auc: 0.5000 - val_loss: 1.0117 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.9907 - auc: 0.5000 - val_loss: 0.9694 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.9494 - auc: 0.5000 - val_loss: 0.9289 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.9099 - auc: 0.5000 - val_loss: 0.8900 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.8719 - auc: 0.5000 - val_loss: 0.8528 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.8355 - auc: 0.5000 - val_loss: 0.8171 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.8007 - auc: 0.5000 - val_loss: 0.7827 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.7673 - auc: 0.5000 - val_loss: 0.7500 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.7354 - auc: 0.5000 - val_loss: 0.7186 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.7049 - auc: 0.5000 - val_loss: 0.6885 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.6757 - auc: 0.5000 - val_loss: 0.6600 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.6479 - auc: 0.5000 - val_loss: 0.6327 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.6214 - auc: 0.5000 - val_loss: 0.6066 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.5960 - auc: 0.5000 - val_loss: 0.5817 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.5718 - auc: 0.5000 - val_loss: 0.5580 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.5489 - auc: 0.5000 - val_loss: 0.5355 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.5270 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.5270 - auc: 0.5000 - val_loss: 0.5140 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.2\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 2\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 1.8e-05\n",
      "l1_lambda: 1.0e-06\n",
      "l2_lambda: 1.0e-02\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 5s 35ms/step - loss: 0.6125 - auc: 0.5026 - val_loss: 0.6103 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 0s 13ms/step - loss: 0.6089 - auc: 0.4862 - val_loss: 0.6068 - val_auc: 0.4537\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 0.6055 - auc: 0.4946 - val_loss: 0.6034 - val_auc: 0.5037\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 0s 13ms/step - loss: 0.6023 - auc: 0.5011 - val_loss: 0.6003 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.5993 - auc: 0.4825 - val_loss: 0.5975 - val_auc: 0.4541\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 0s 13ms/step - loss: 0.5967 - auc: 0.4756 - val_loss: 0.5951 - val_auc: 0.5025\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 0.5945 - auc: 0.5002 - val_loss: 0.5931 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 0.5927 - auc: 0.5000 - val_loss: 0.5914 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 0.5912 - auc: 0.5000 - val_loss: 0.5900 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 0.5899 - auc: 0.4955 - val_loss: 0.5888 - val_auc: 0.4235\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.5887 - auc: 0.4542 - val_loss: 0.5877 - val_auc: 0.4633\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 0.5877 - auc: 0.4844 - val_loss: 0.5866 - val_auc: 0.4979\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 0s 13ms/step - loss: 0.5867 - auc: 0.4992 - val_loss: 0.5857 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 0.5857 - auc: 0.5000 - val_loss: 0.5848 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 0.5848 - auc: 0.5000 - val_loss: 0.5839 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 0.5840 - auc: 0.5000 - val_loss: 0.5830 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.5831 - auc: 0.5000 - val_loss: 0.5822 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.5823 - auc: 0.5000 - val_loss: 0.5814 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 0.5815 - auc: 0.5000 - val_loss: 0.5806 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 0.5807 - auc: 0.5000 - val_loss: 0.5798 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 0.5799 - auc: 0.4837 - val_loss: 0.5790 - val_auc: 0.4856\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 0.5792 - auc: 0.4934 - val_loss: 0.5782 - val_auc: 0.5000\n",
      "Epoch 23/1000\n",
      "35/39 [=========================>....] - ETA: 0s - loss: 0.5782 - auc: 0.4999Restoring model weights from the end of the best epoch: 3.\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.5784 - auc: 0.4999 - val_loss: 0.5775 - val_auc: 0.5000\n",
      "Epoch 23: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.3\n",
      "num_dense_layers: 4\n",
      "num_dense_nodes: 23\n",
      "activation_dense: tanh\n",
      "learning_rate: 3.7e-05\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-03\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 6s 35ms/step - loss: 0.6207 - auc: 0.5000 - val_loss: 0.5031 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.4291 - auc: 0.5000 - val_loss: 0.3542 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.3107 - auc: 0.5000 - val_loss: 0.2651 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.2424 - auc: 0.5000 - val_loss: 0.2142 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.2022 - auc: 0.5000 - val_loss: 0.1824 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.1761 - auc: 0.5000 - val_loss: 0.1612 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.1583 - auc: 0.5000 - val_loss: 0.1458 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.1454 - auc: 0.5000 - val_loss: 0.1347 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.1358 - auc: 0.5000 - val_loss: 0.1261 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.1284 - auc: 0.5000 - val_loss: 0.1196 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.1226 - auc: 0.5000 - val_loss: 0.1143 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.1181 - auc: 0.5000 - val_loss: 0.1101 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.1144 - auc: 0.5000 - val_loss: 0.1067 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.1115 - auc: 0.5000 - val_loss: 0.1040 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.1092 - auc: 0.5000 - val_loss: 0.1016 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.1073 - auc: 0.5000 - val_loss: 0.0999 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.1057 - auc: 0.5000 - val_loss: 0.0983 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.1044 - auc: 0.5000 - val_loss: 0.0971 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.1033 - auc: 0.5000 - val_loss: 0.0960 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.1024 - auc: 0.5000 - val_loss: 0.0951 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "36/39 [==========================>...] - ETA: 0s - loss: 0.1027 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.1016 - auc: 0.5000 - val_loss: 0.0943 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.1\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 1\n",
      "activation_dense: relu\n",
      "learning_rate: 1.0e-01\n",
      "l1_lambda: 0.0e+00\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 4s 28ms/step - loss: 0.2703 - auc: 0.4844 - val_loss: 0.1095 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.1039 - auc: 0.5321 - val_loss: 0.0922 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.0985 - auc: 0.4721 - val_loss: 0.0903 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0977 - auc: 0.4380 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.0977 - auc: 0.4423 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.0976 - auc: 0.5047 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0977 - auc: 0.4707 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.0976 - auc: 0.4920 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0977 - auc: 0.4849 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0976 - auc: 0.4995 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.0977 - auc: 0.4465 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.0977 - auc: 0.4621 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0978 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.0976 - auc: 0.4934 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0977 - auc: 0.4579 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0977 - auc: 0.4631 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0977 - auc: 0.4778 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0979 - auc: 0.4752Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.0977 - auc: 0.4763 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.2\n",
      "num_dense_layers: 5\n",
      "num_dense_nodes: 1\n",
      "activation_dense: tanh\n",
      "learning_rate: 1.7e-02\n",
      "l1_lambda: 1.0e-06\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 6s 41ms/step - loss: 0.3633 - auc: 0.5000 - val_loss: 0.1816 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.1462 - auc: 0.5000 - val_loss: 0.1171 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.1135 - auc: 0.5000 - val_loss: 0.1008 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.1039 - auc: 0.5000 - val_loss: 0.0948 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.1002 - auc: 0.5000 - val_loss: 0.0921 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0988 - auc: 0.5000 - val_loss: 0.0908 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0981 - auc: 0.5000 - val_loss: 0.0903 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0979 - auc: 0.5000 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.0983 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 3\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.5\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 30\n",
      "activation_dense: tanh\n",
      "learning_rate: 1.0e-05\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-01\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 14s 90ms/step - loss: 19.0215 - auc: 0.5000 - val_loss: 18.9908 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 18.9624 - auc: 0.5000 - val_loss: 18.9317 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 18.9033 - auc: 0.5000 - val_loss: 18.8727 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 18.8444 - auc: 0.5000 - val_loss: 18.8138 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 18.7857 - auc: 0.5000 - val_loss: 18.7550 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 18.7270 - auc: 0.5000 - val_loss: 18.6964 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 18.6685 - auc: 0.5000 - val_loss: 18.6377 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 18.6099 - auc: 0.5000 - val_loss: 18.5791 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 18.5514 - auc: 0.5000 - val_loss: 18.5206 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 18.4931 - auc: 0.5000 - val_loss: 18.4622 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 18.4349 - auc: 0.5000 - val_loss: 18.4039 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 18.3766 - auc: 0.5000 - val_loss: 18.3455 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 18.3184 - auc: 0.5000 - val_loss: 18.2873 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 18.2603 - auc: 0.5000 - val_loss: 18.2292 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 18.2025 - auc: 0.5000 - val_loss: 18.1713 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 18.1448 - auc: 0.5000 - val_loss: 18.1136 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 18.0871 - auc: 0.5000 - val_loss: 18.0560 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 18.0297 - auc: 0.5000 - val_loss: 17.9985 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 17.9725 - auc: 0.5000 - val_loss: 17.9413 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 17.9155 - auc: 0.5000 - val_loss: 17.8842 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 17.8583 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 17.8583 - auc: 0.5000 - val_loss: 17.8274 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.1\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 30\n",
      "activation_dense: softmax\n",
      "learning_rate: 2.1e-04\n",
      "l1_lambda: 1.0e-03\n",
      "l2_lambda: 1.0e-01\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 3s 28ms/step - loss: 0.6308 - auc: 0.4532 - val_loss: 0.4790 - val_auc: 0.4310\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.4281 - auc: 0.4284 - val_loss: 0.3887 - val_auc: 0.4048\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.3730 - auc: 0.4027 - val_loss: 0.3553 - val_auc: 0.4112\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.3473 - auc: 0.3808 - val_loss: 0.3351 - val_auc: 0.4149\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.3299 - auc: 0.3938 - val_loss: 0.3200 - val_auc: 0.4234\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.3161 - auc: 0.4058 - val_loss: 0.3074 - val_auc: 0.3908\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.3045 - auc: 0.4368 - val_loss: 0.2964 - val_auc: 0.4481\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.2941 - auc: 0.4122 - val_loss: 0.2865 - val_auc: 0.4918\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.2847 - auc: 0.4115 - val_loss: 0.2774 - val_auc: 0.4210\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.2760 - auc: 0.3994 - val_loss: 0.2690 - val_auc: 0.4134\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.2679 - auc: 0.4600 - val_loss: 0.2612 - val_auc: 0.4395\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.2604 - auc: 0.4378 - val_loss: 0.2538 - val_auc: 0.4575\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.2533 - auc: 0.4495 - val_loss: 0.2468 - val_auc: 0.4926\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.2465 - auc: 0.4340 - val_loss: 0.2402 - val_auc: 0.5115\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.2402 - auc: 0.4437 - val_loss: 0.2340 - val_auc: 0.4549\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.2342 - auc: 0.4861 - val_loss: 0.2280 - val_auc: 0.4205\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.2284 - auc: 0.4622 - val_loss: 0.2224 - val_auc: 0.4200\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.2230 - auc: 0.4496 - val_loss: 0.2170 - val_auc: 0.4583\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.2178 - auc: 0.5239 - val_loss: 0.2119 - val_auc: 0.4996\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.2129 - auc: 0.4628 - val_loss: 0.2070 - val_auc: 0.4983\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.2082 - auc: 0.5098 - val_loss: 0.2024 - val_auc: 0.4967\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.2037 - auc: 0.4965 - val_loss: 0.1979 - val_auc: 0.5087\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.1994 - auc: 0.4840 - val_loss: 0.1937 - val_auc: 0.4972\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.1953 - auc: 0.4874 - val_loss: 0.1896 - val_auc: 0.4632\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.1914 - auc: 0.4637 - val_loss: 0.1857 - val_auc: 0.4996\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.1877 - auc: 0.4610 - val_loss: 0.1820 - val_auc: 0.4979\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.1841 - auc: 0.4577 - val_loss: 0.1785 - val_auc: 0.4815\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.1807 - auc: 0.4899 - val_loss: 0.1751 - val_auc: 0.5000\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.1774 - auc: 0.4530 - val_loss: 0.1718 - val_auc: 0.4996\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.1742 - auc: 0.4805 - val_loss: 0.1687 - val_auc: 0.4348\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.1712 - auc: 0.4999 - val_loss: 0.1657 - val_auc: 0.5000\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.1684 - auc: 0.4558 - val_loss: 0.1628 - val_auc: 0.4950\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.1656 - auc: 0.4992 - val_loss: 0.1600 - val_auc: 0.5000\n",
      "Epoch 34/1000\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.1616 - auc: 0.4706Restoring model weights from the end of the best epoch: 14.\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.1629 - auc: 0.4667 - val_loss: 0.1574 - val_auc: 0.4996\n",
      "Epoch 34: early stopping\n",
      "\n",
      "Validation AUC: 49.96%\n",
      "\n",
      "num_lstm_layers: 3\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 5\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 4\n",
      "num_dense_nodes: 19\n",
      "activation_dense: softmax\n",
      "learning_rate: 7.5e-04\n",
      "l1_lambda: 1.0e-03\n",
      "l2_lambda: 1.0e-01\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 15s 105ms/step - loss: 2.0345 - auc: 0.5000 - val_loss: 1.8739 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 1.7449 - auc: 0.5000 - val_loss: 1.6138 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 1.5083 - auc: 0.5000 - val_loss: 1.4004 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 1.3133 - auc: 0.5000 - val_loss: 1.2234 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 1.1511 - auc: 0.5000 - val_loss: 1.0759 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 1.0155 - auc: 0.5000 - val_loss: 0.9520 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.9015 - auc: 0.5000 - val_loss: 0.8476 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.8051 - auc: 0.5000 - val_loss: 0.7590 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.7231 - auc: 0.5000 - val_loss: 0.6837 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.6533 - auc: 0.5000 - val_loss: 0.6192 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.5934 - auc: 0.5000 - val_loss: 0.5637 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.5418 - auc: 0.5000 - val_loss: 0.5159 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.4972 - auc: 0.5000 - val_loss: 0.4743 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.4584 - auc: 0.5000 - val_loss: 0.4380 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.4244 - auc: 0.5000 - val_loss: 0.4062 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.3946 - auc: 0.5000 - val_loss: 0.3782 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.3683 - auc: 0.5000 - val_loss: 0.3534 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.3450 - auc: 0.5000 - val_loss: 0.3314 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.3242 - auc: 0.5000 - val_loss: 0.3117 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.3057 - auc: 0.5000 - val_loss: 0.2941 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2890 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.2890 - auc: 0.5000 - val_loss: 0.2781 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.5\n",
      "num_dense_layers: 4\n",
      "num_dense_nodes: 17\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 5.9e-02\n",
      "l1_lambda: 0.0e+00\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 8s 50ms/step - loss: 0.1282 - auc: 0.4501 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0976 - auc: 0.5265 - val_loss: 0.0912 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0981 - auc: 0.4953 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.0976 - auc: 0.5180 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0977 - auc: 0.5142 - val_loss: 0.0906 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0981 - auc: 0.5257 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0983 - auc: 0.4886 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0978 - auc: 0.4626 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0989 - auc: 0.4341 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0982 - auc: 0.4604 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0989 - auc: 0.4531 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1002 - auc: 0.4585 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0983 - auc: 0.5216 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1000 - auc: 0.4807 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0987 - auc: 0.4673 - val_loss: 0.0910 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0979 - auc: 0.4736 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0973 - auc: 0.5637 - val_loss: 0.0936 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0985 - auc: 0.5291 - val_loss: 0.0933 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0982 - auc: 0.4494 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0991 - auc: 0.4466 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0994 - auc: 0.4809Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0994 - auc: 0.4809 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 5\n",
      "activation_dense: relu\n",
      "learning_rate: 2.1e-04\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 4s 41ms/step - loss: 0.4302 - auc: 0.5223 - val_loss: 0.2134 - val_auc: 0.4515\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.1768 - auc: 0.4768 - val_loss: 0.1476 - val_auc: 0.4419\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.1425 - auc: 0.4431 - val_loss: 0.1292 - val_auc: 0.4362\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.1297 - auc: 0.4459 - val_loss: 0.1198 - val_auc: 0.4689\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.1223 - auc: 0.4463 - val_loss: 0.1139 - val_auc: 0.4434\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.1175 - auc: 0.4316 - val_loss: 0.1096 - val_auc: 0.4803\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.1140 - auc: 0.4330 - val_loss: 0.1065 - val_auc: 0.4359\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.1114 - auc: 0.4700 - val_loss: 0.1041 - val_auc: 0.4859\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.1093 - auc: 0.4274 - val_loss: 0.1021 - val_auc: 0.4867\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.1076 - auc: 0.4579 - val_loss: 0.1004 - val_auc: 0.4434\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.1062 - auc: 0.4530 - val_loss: 0.0991 - val_auc: 0.4590\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.1050 - auc: 0.4601 - val_loss: 0.0979 - val_auc: 0.5033\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.1040 - auc: 0.4944 - val_loss: 0.0969 - val_auc: 0.5021\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.1031 - auc: 0.4672 - val_loss: 0.0960 - val_auc: 0.4562\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.1024 - auc: 0.4680 - val_loss: 0.0953 - val_auc: 0.4826\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.1018 - auc: 0.4924 - val_loss: 0.0946 - val_auc: 0.4686\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.1012 - auc: 0.5017 - val_loss: 0.0940 - val_auc: 0.5057\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.1007 - auc: 0.4999 - val_loss: 0.0936 - val_auc: 0.4737\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1003 - auc: 0.4825 - val_loss: 0.0931 - val_auc: 0.4616\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.1000 - auc: 0.4997 - val_loss: 0.0927 - val_auc: 0.5025\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0996 - auc: 0.5173 - val_loss: 0.0924 - val_auc: 0.4628\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0994 - auc: 0.5124 - val_loss: 0.0920 - val_auc: 0.4922\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0991 - auc: 0.4862 - val_loss: 0.0918 - val_auc: 0.5227\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0989 - auc: 0.4927 - val_loss: 0.0915 - val_auc: 0.4901\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0987 - auc: 0.4900 - val_loss: 0.0913 - val_auc: 0.4992\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0986 - auc: 0.4915 - val_loss: 0.0911 - val_auc: 0.4542\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0984 - auc: 0.4997 - val_loss: 0.0910 - val_auc: 0.4715\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0983 - auc: 0.5059 - val_loss: 0.0908 - val_auc: 0.4986\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0982 - auc: 0.5405 - val_loss: 0.0907 - val_auc: 0.5093\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0981 - auc: 0.5690 - val_loss: 0.0906 - val_auc: 0.5542\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0980 - auc: 0.5395 - val_loss: 0.0904 - val_auc: 0.5488\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0979 - auc: 0.5144 - val_loss: 0.0903 - val_auc: 0.5046\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0978 - auc: 0.5265 - val_loss: 0.0903 - val_auc: 0.5182\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0977 - auc: 0.5267 - val_loss: 0.0902 - val_auc: 0.5277\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0977 - auc: 0.5091 - val_loss: 0.0901 - val_auc: 0.4880\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0976 - auc: 0.4938 - val_loss: 0.0900 - val_auc: 0.4926\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0975 - auc: 0.4957 - val_loss: 0.0899 - val_auc: 0.4942\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0974 - auc: 0.5005 - val_loss: 0.0899 - val_auc: 0.4934\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0974 - auc: 0.5116 - val_loss: 0.0898 - val_auc: 0.4942\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0973 - auc: 0.5171 - val_loss: 0.0897 - val_auc: 0.4946\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0971 - auc: 0.5264 - val_loss: 0.0896 - val_auc: 0.4930\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0969 - auc: 0.5682 - val_loss: 0.0895 - val_auc: 0.5120\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0966 - auc: 0.5765 - val_loss: 0.0892 - val_auc: 0.5541\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0962 - auc: 0.6232 - val_loss: 0.0890 - val_auc: 0.6225\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.0958 - auc: 0.7037 - val_loss: 0.0887 - val_auc: 0.6326\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0954 - auc: 0.7447 - val_loss: 0.0885 - val_auc: 0.7308\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0947 - auc: 0.7904 - val_loss: 0.0881 - val_auc: 0.7580\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0942 - auc: 0.7912 - val_loss: 0.0878 - val_auc: 0.7407\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0938 - auc: 0.7965 - val_loss: 0.0875 - val_auc: 0.7466\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0934 - auc: 0.8008 - val_loss: 0.0873 - val_auc: 0.7525\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0930 - auc: 0.8123 - val_loss: 0.0870 - val_auc: 0.7563\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0926 - auc: 0.8207 - val_loss: 0.0866 - val_auc: 0.7734\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0920 - auc: 0.8320 - val_loss: 0.0862 - val_auc: 0.7852\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0912 - auc: 0.8485 - val_loss: 0.0858 - val_auc: 0.7835\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0904 - auc: 0.8388 - val_loss: 0.0853 - val_auc: 0.7891\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0896 - auc: 0.8547 - val_loss: 0.0848 - val_auc: 0.7968\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0887 - auc: 0.8441 - val_loss: 0.0844 - val_auc: 0.7942\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0879 - auc: 0.8510 - val_loss: 0.0839 - val_auc: 0.7829\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0872 - auc: 0.8331 - val_loss: 0.0835 - val_auc: 0.7782\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0864 - auc: 0.8504 - val_loss: 0.0831 - val_auc: 0.7948\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0855 - auc: 0.8543 - val_loss: 0.0826 - val_auc: 0.7820\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0847 - auc: 0.8465 - val_loss: 0.0821 - val_auc: 0.7892\n",
      "Epoch 63/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0839 - auc: 0.8587 - val_loss: 0.0818 - val_auc: 0.8118\n",
      "Epoch 64/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0830 - auc: 0.8545 - val_loss: 0.0813 - val_auc: 0.8032\n",
      "Epoch 65/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0823 - auc: 0.8609 - val_loss: 0.0809 - val_auc: 0.8089\n",
      "Epoch 66/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0817 - auc: 0.8695 - val_loss: 0.0806 - val_auc: 0.8196\n",
      "Epoch 67/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0807 - auc: 0.8687 - val_loss: 0.0802 - val_auc: 0.8218\n",
      "Epoch 68/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0798 - auc: 0.8732 - val_loss: 0.0798 - val_auc: 0.8298\n",
      "Epoch 69/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0790 - auc: 0.8777 - val_loss: 0.0794 - val_auc: 0.8079\n",
      "Epoch 70/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0783 - auc: 0.8740 - val_loss: 0.0792 - val_auc: 0.8327\n",
      "Epoch 71/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.0772 - auc: 0.8790 - val_loss: 0.0787 - val_auc: 0.7936\n",
      "Epoch 72/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.0765 - auc: 0.8852 - val_loss: 0.0784 - val_auc: 0.7984\n",
      "Epoch 73/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0757 - auc: 0.8794 - val_loss: 0.0781 - val_auc: 0.8033\n",
      "Epoch 74/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0749 - auc: 0.8833 - val_loss: 0.0778 - val_auc: 0.8095\n",
      "Epoch 75/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0741 - auc: 0.8820 - val_loss: 0.0775 - val_auc: 0.8138\n",
      "Epoch 76/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0735 - auc: 0.8840 - val_loss: 0.0772 - val_auc: 0.8180\n",
      "Epoch 77/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0727 - auc: 0.8889 - val_loss: 0.0770 - val_auc: 0.8194\n",
      "Epoch 78/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0721 - auc: 0.8916 - val_loss: 0.0767 - val_auc: 0.8252\n",
      "Epoch 79/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0713 - auc: 0.8922 - val_loss: 0.0764 - val_auc: 0.8370\n",
      "Epoch 80/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0705 - auc: 0.8965 - val_loss: 0.0764 - val_auc: 0.8296\n",
      "Epoch 81/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0699 - auc: 0.8991 - val_loss: 0.0760 - val_auc: 0.8285\n",
      "Epoch 82/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0691 - auc: 0.8972 - val_loss: 0.0759 - val_auc: 0.8388\n",
      "Epoch 83/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0684 - auc: 0.9037 - val_loss: 0.0757 - val_auc: 0.8256\n",
      "Epoch 84/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0676 - auc: 0.9076 - val_loss: 0.0756 - val_auc: 0.8239\n",
      "Epoch 85/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0670 - auc: 0.9043 - val_loss: 0.0755 - val_auc: 0.8260\n",
      "Epoch 86/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0662 - auc: 0.9039 - val_loss: 0.0754 - val_auc: 0.8336\n",
      "Epoch 87/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0655 - auc: 0.9077 - val_loss: 0.0753 - val_auc: 0.8385\n",
      "Epoch 88/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0648 - auc: 0.9057 - val_loss: 0.0752 - val_auc: 0.8354\n",
      "Epoch 89/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0643 - auc: 0.9072 - val_loss: 0.0752 - val_auc: 0.8448\n",
      "Epoch 90/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0636 - auc: 0.9039 - val_loss: 0.0752 - val_auc: 0.8465\n",
      "Epoch 91/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0629 - auc: 0.9058 - val_loss: 0.0752 - val_auc: 0.8329\n",
      "Epoch 92/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0624 - auc: 0.9066 - val_loss: 0.0752 - val_auc: 0.8394\n",
      "Epoch 93/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0617 - auc: 0.9045 - val_loss: 0.0755 - val_auc: 0.8510\n",
      "Epoch 94/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0609 - auc: 0.9114 - val_loss: 0.0751 - val_auc: 0.8417\n",
      "Epoch 95/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0604 - auc: 0.9138 - val_loss: 0.0752 - val_auc: 0.8415\n",
      "Epoch 96/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0598 - auc: 0.9134 - val_loss: 0.0753 - val_auc: 0.8380\n",
      "Epoch 97/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0594 - auc: 0.9188 - val_loss: 0.0753 - val_auc: 0.8344\n",
      "Epoch 98/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0587 - auc: 0.9183 - val_loss: 0.0757 - val_auc: 0.8432\n",
      "Epoch 99/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0580 - auc: 0.9152 - val_loss: 0.0755 - val_auc: 0.8364\n",
      "Epoch 100/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0579 - auc: 0.9206 - val_loss: 0.0757 - val_auc: 0.8349\n",
      "Epoch 101/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0570 - auc: 0.9197 - val_loss: 0.0759 - val_auc: 0.8382\n",
      "Epoch 102/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0567 - auc: 0.9222 - val_loss: 0.0759 - val_auc: 0.8358\n",
      "Epoch 103/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0563 - auc: 0.9203 - val_loss: 0.0764 - val_auc: 0.8485\n",
      "Epoch 104/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.0555 - auc: 0.9245 - val_loss: 0.0762 - val_auc: 0.8383\n",
      "Epoch 105/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0549 - auc: 0.9261 - val_loss: 0.0766 - val_auc: 0.8464\n",
      "Epoch 106/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0544 - auc: 0.9247 - val_loss: 0.0765 - val_auc: 0.8407\n",
      "Epoch 107/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0538 - auc: 0.9252 - val_loss: 0.0768 - val_auc: 0.8424\n",
      "Epoch 108/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0534 - auc: 0.9289 - val_loss: 0.0770 - val_auc: 0.8323\n",
      "Epoch 109/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0532 - auc: 0.9292 - val_loss: 0.0772 - val_auc: 0.8274\n",
      "Epoch 110/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0523 - auc: 0.9300 - val_loss: 0.0774 - val_auc: 0.8340\n",
      "Epoch 111/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0521 - auc: 0.9350 - val_loss: 0.0777 - val_auc: 0.8352\n",
      "Epoch 112/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0514 - auc: 0.9310 - val_loss: 0.0779 - val_auc: 0.8303\n",
      "Epoch 113/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0509 - auc: 0.9302Restoring model weights from the end of the best epoch: 93.\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0509 - auc: 0.9302 - val_loss: 0.0782 - val_auc: 0.8290\n",
      "Epoch 113: early stopping\n",
      "\n",
      "Validation AUC: 82.90%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 5\n",
      "dropout_rate: 0.2\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 27\n",
      "activation_dense: tanh\n",
      "learning_rate: 5.3e-05\n",
      "l1_lambda: 0.0e+00\n",
      "l2_lambda: 1.0e-06\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 6s 44ms/step - loss: 0.6681 - auc: 0.5000 - val_loss: 0.6620 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.6576 - auc: 0.5000 - val_loss: 0.6523 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.6487 - auc: 0.5000 - val_loss: 0.6441 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.6410 - auc: 0.5000 - val_loss: 0.6367 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.6340 - auc: 0.5000 - val_loss: 0.6301 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.6278 - auc: 0.5000 - val_loss: 0.6242 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.6222 - auc: 0.5000 - val_loss: 0.6189 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.6170 - auc: 0.5000 - val_loss: 0.6139 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.6122 - auc: 0.5000 - val_loss: 0.6093 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.6077 - auc: 0.5000 - val_loss: 0.6049 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.6034 - auc: 0.5000 - val_loss: 0.6006 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.5992 - auc: 0.5000 - val_loss: 0.5965 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.5951 - auc: 0.5000 - val_loss: 0.5925 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.5912 - auc: 0.5000 - val_loss: 0.5885 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.5873 - auc: 0.5000 - val_loss: 0.5847 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.5834 - auc: 0.5000 - val_loss: 0.5809 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.5797 - auc: 0.5000 - val_loss: 0.5771 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.5759 - auc: 0.5000 - val_loss: 0.5734 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.5723 - auc: 0.5000 - val_loss: 0.5698 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.5687 - auc: 0.5000 - val_loss: 0.5661 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.5651 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.5651 - auc: 0.5000 - val_loss: 0.5626 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 19\n",
      "activation_dense: relu\n",
      "learning_rate: 5.7e-04\n",
      "l1_lambda: 1.0e-03\n",
      "l2_lambda: 1.0e-04\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 9s 74ms/step - loss: 0.5703 - auc: 0.5024 - val_loss: 0.3324 - val_auc: 0.3575\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.2446 - auc: 0.5372 - val_loss: 0.1851 - val_auc: 0.4616\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1740 - auc: 0.5034 - val_loss: 0.1524 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1512 - auc: 0.4539 - val_loss: 0.1343 - val_auc: 0.4059\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1357 - auc: 0.4988 - val_loss: 0.1212 - val_auc: 0.4859\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1241 - auc: 0.4991 - val_loss: 0.1110 - val_auc: 0.4983\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1155 - auc: 0.4892 - val_loss: 0.1039 - val_auc: 0.4539\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1089 - auc: 0.5270 - val_loss: 0.0979 - val_auc: 0.6049\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1030 - auc: 0.6966 - val_loss: 0.0932 - val_auc: 0.6585\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0989 - auc: 0.7199 - val_loss: 0.0897 - val_auc: 0.7139\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0950 - auc: 0.7635 - val_loss: 0.0869 - val_auc: 0.7795\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.0911 - auc: 0.7803 - val_loss: 0.0841 - val_auc: 0.7562\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0850 - auc: 0.8404 - val_loss: 0.0823 - val_auc: 0.7605\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0793 - auc: 0.8434 - val_loss: 0.0785 - val_auc: 0.8014\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0761 - auc: 0.8526 - val_loss: 0.0807 - val_auc: 0.8007\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0724 - auc: 0.8502 - val_loss: 0.0803 - val_auc: 0.8107\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0704 - auc: 0.8845 - val_loss: 0.0795 - val_auc: 0.8290\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0678 - auc: 0.8817 - val_loss: 0.0803 - val_auc: 0.8415\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0661 - auc: 0.8879 - val_loss: 0.0812 - val_auc: 0.8051\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0653 - auc: 0.8909 - val_loss: 0.0799 - val_auc: 0.8190\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.0651 - auc: 0.8831 - val_loss: 0.0817 - val_auc: 0.8089\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0618 - auc: 0.9205 - val_loss: 0.0836 - val_auc: 0.8001\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0611 - auc: 0.9152 - val_loss: 0.0841 - val_auc: 0.8018\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0583 - auc: 0.9116 - val_loss: 0.0856 - val_auc: 0.7973\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0580 - auc: 0.9215 - val_loss: 0.0873 - val_auc: 0.7918\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0555 - auc: 0.9296 - val_loss: 0.0895 - val_auc: 0.7736\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0537 - auc: 0.9340 - val_loss: 0.0891 - val_auc: 0.7703\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0504 - auc: 0.9247 - val_loss: 0.0936 - val_auc: 0.7359\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0523 - auc: 0.9287 - val_loss: 0.0915 - val_auc: 0.7722\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0513 - auc: 0.9331 - val_loss: 0.0927 - val_auc: 0.7491\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0506 - auc: 0.9470 - val_loss: 0.0976 - val_auc: 0.6783\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0476 - auc: 0.9400 - val_loss: 0.0947 - val_auc: 0.6928\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0457 - auc: 0.9485 - val_loss: 0.0941 - val_auc: 0.7334\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0451 - auc: 0.9606 - val_loss: 0.0979 - val_auc: 0.7010\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.0415 - auc: 0.9562 - val_loss: 0.1021 - val_auc: 0.6164\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0397 - auc: 0.9560 - val_loss: 0.0971 - val_auc: 0.6638\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0398 - auc: 0.9591 - val_loss: 0.1002 - val_auc: 0.6656\n",
      "Epoch 38/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0324 - auc: 0.9813Restoring model weights from the end of the best epoch: 18.\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0323 - auc: 0.9818 - val_loss: 0.1050 - val_auc: 0.6125\n",
      "Epoch 38: early stopping\n",
      "\n",
      "Validation AUC: 61.25%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.3\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 30\n",
      "activation_dense: tanh\n",
      "learning_rate: 2.1e-04\n",
      "l1_lambda: 1.0e-05\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 8s 70ms/step - loss: 0.5526 - auc: 0.5080 - val_loss: 0.3089 - val_auc: 0.4627\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.1896 - auc: 0.4792 - val_loss: 0.1158 - val_auc: 0.4134\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1091 - auc: 0.5057 - val_loss: 0.0953 - val_auc: 0.4967\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.1008 - auc: 0.4622 - val_loss: 0.0919 - val_auc: 0.4769\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0993 - auc: 0.4866 - val_loss: 0.0913 - val_auc: 0.4996\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0990 - auc: 0.4912 - val_loss: 0.0910 - val_auc: 0.4876\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0987 - auc: 0.5265 - val_loss: 0.0907 - val_auc: 0.5467\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0985 - auc: 0.5662 - val_loss: 0.0904 - val_auc: 0.6282\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0980 - auc: 0.6618 - val_loss: 0.0899 - val_auc: 0.6948\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0973 - auc: 0.7149 - val_loss: 0.0894 - val_auc: 0.6965\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0965 - auc: 0.7077 - val_loss: 0.0888 - val_auc: 0.6842\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0954 - auc: 0.7659 - val_loss: 0.0877 - val_auc: 0.7536\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0936 - auc: 0.7904 - val_loss: 0.0863 - val_auc: 0.7304\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0906 - auc: 0.8299 - val_loss: 0.0846 - val_auc: 0.7898\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.0876 - auc: 0.8414 - val_loss: 0.0817 - val_auc: 0.7869\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0825 - auc: 0.8361 - val_loss: 0.0796 - val_auc: 0.7511\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0781 - auc: 0.8754 - val_loss: 0.0783 - val_auc: 0.7914\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0766 - auc: 0.8693 - val_loss: 0.0778 - val_auc: 0.8139\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0741 - auc: 0.8605 - val_loss: 0.0774 - val_auc: 0.7761\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0711 - auc: 0.8856 - val_loss: 0.0769 - val_auc: 0.8154\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0721 - auc: 0.8767 - val_loss: 0.0766 - val_auc: 0.8225\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 2s 38ms/step - loss: 0.0690 - auc: 0.8822 - val_loss: 0.0766 - val_auc: 0.8000\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0652 - auc: 0.8907 - val_loss: 0.0774 - val_auc: 0.7949\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0638 - auc: 0.9065 - val_loss: 0.0785 - val_auc: 0.8112\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0627 - auc: 0.9184 - val_loss: 0.0779 - val_auc: 0.8037\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0614 - auc: 0.9029 - val_loss: 0.0797 - val_auc: 0.8201\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0602 - auc: 0.9104 - val_loss: 0.0808 - val_auc: 0.8159\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.0569 - auc: 0.9263 - val_loss: 0.0832 - val_auc: 0.8180\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0555 - auc: 0.9218 - val_loss: 0.0835 - val_auc: 0.8163\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0511 - auc: 0.9267 - val_loss: 0.0861 - val_auc: 0.7913\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0490 - auc: 0.9492 - val_loss: 0.0895 - val_auc: 0.7960\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0469 - auc: 0.9239 - val_loss: 0.0913 - val_auc: 0.7559\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0471 - auc: 0.9364 - val_loss: 0.0902 - val_auc: 0.7731\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0492 - auc: 0.9358 - val_loss: 0.0910 - val_auc: 0.7644\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0442 - auc: 0.9477 - val_loss: 0.0942 - val_auc: 0.7924\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0446 - auc: 0.9520 - val_loss: 0.0946 - val_auc: 0.7742\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0421 - auc: 0.9372 - val_loss: 0.1006 - val_auc: 0.7688\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0425 - auc: 0.9429 - val_loss: 0.0972 - val_auc: 0.6907\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0367 - auc: 0.9590 - val_loss: 0.0999 - val_auc: 0.7157\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0372 - auc: 0.9612 - val_loss: 0.1047 - val_auc: 0.7384\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0371 - auc: 0.9525Restoring model weights from the end of the best epoch: 21.\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0371 - auc: 0.9525 - val_loss: 0.1017 - val_auc: 0.7332\n",
      "Epoch 41: early stopping\n",
      "\n",
      "Validation AUC: 73.32%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.0\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 1\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 1.3e-04\n",
      "l1_lambda: 1.0e-06\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 8s 47ms/step - loss: 0.3457 - auc: 0.5000 - val_loss: 0.3419 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.3422 - auc: 0.5031 - val_loss: 0.3384 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.3387 - auc: 0.5000 - val_loss: 0.3349 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.3353 - auc: 0.5069 - val_loss: 0.3314 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.3318 - auc: 0.5000 - val_loss: 0.3280 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.3284 - auc: 0.4990 - val_loss: 0.3245 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.3250 - auc: 0.4632 - val_loss: 0.3211 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.3216 - auc: 0.5000 - val_loss: 0.3177 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.3182 - auc: 0.4801 - val_loss: 0.3143 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.3148 - auc: 0.5000 - val_loss: 0.3109 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.3115 - auc: 0.4867 - val_loss: 0.3075 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.3082 - auc: 0.5000 - val_loss: 0.3042 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.3049 - auc: 0.4905 - val_loss: 0.3009 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.3016 - auc: 0.5000 - val_loss: 0.2976 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.2983 - auc: 0.5108 - val_loss: 0.2943 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.2951 - auc: 0.5000 - val_loss: 0.2911 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.2920 - auc: 0.4825 - val_loss: 0.2879 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.2888 - auc: 0.5000 - val_loss: 0.2848 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.2858 - auc: 0.4806 - val_loss: 0.2817 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.2827 - auc: 0.5000 - val_loss: 0.2787 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2803 - auc: 0.4874Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.2797 - auc: 0.4886 - val_loss: 0.2757 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.5\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 19\n",
      "activation_dense: softmax\n",
      "learning_rate: 2.5e-04\n",
      "l1_lambda: 0.0e+00\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 5s 41ms/step - loss: 0.6072 - auc: 0.5360 - val_loss: 0.5789 - val_auc: 0.5002\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.5662 - auc: 0.4406 - val_loss: 0.5529 - val_auc: 0.4897\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.5443 - auc: 0.4909 - val_loss: 0.5330 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.5230 - auc: 0.5024 - val_loss: 0.5124 - val_auc: 0.5203\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.5058 - auc: 0.4496 - val_loss: 0.4972 - val_auc: 0.4538\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.4916 - auc: 0.5101 - val_loss: 0.4839 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.4790 - auc: 0.4936 - val_loss: 0.4718 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.4676 - auc: 0.5338 - val_loss: 0.4609 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.4571 - auc: 0.4806 - val_loss: 0.4506 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.4472 - auc: 0.5123 - val_loss: 0.4411 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.4380 - auc: 0.4988 - val_loss: 0.4321 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.4293 - auc: 0.5024 - val_loss: 0.4236 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.4210 - auc: 0.5314 - val_loss: 0.4155 - val_auc: 0.4688\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.4131 - auc: 0.5020 - val_loss: 0.4077 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.4056 - auc: 0.4859 - val_loss: 0.4002 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.3983 - auc: 0.5001 - val_loss: 0.3931 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.3913 - auc: 0.5015 - val_loss: 0.3862 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.3846 - auc: 0.4996 - val_loss: 0.3795 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.3781 - auc: 0.4920 - val_loss: 0.3731 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.3719 - auc: 0.4957 - val_loss: 0.3670 - val_auc: 0.4905\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.3659 - auc: 0.4999 - val_loss: 0.3610 - val_auc: 0.5000\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.3600 - auc: 0.4823 - val_loss: 0.3552 - val_auc: 0.5000\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.3544 - auc: 0.5001 - val_loss: 0.3496 - val_auc: 0.5000\n",
      "Epoch 24/1000\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.3482 - auc: 0.4957Restoring model weights from the end of the best epoch: 4.\n",
      "39/39 [==============================] - 1s 21ms/step - loss: 0.3489 - auc: 0.4854 - val_loss: 0.3442 - val_auc: 0.5000\n",
      "Epoch 24: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 28\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 2.3e-04\n",
      "l1_lambda: 1.0e-05\n",
      "l2_lambda: 1.0e-03\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 9s 72ms/step - loss: 1.0682 - auc: 0.5011 - val_loss: 0.9483 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.8481 - auc: 0.5408 - val_loss: 0.7481 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.6680 - auc: 0.4919 - val_loss: 0.5881 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.5278 - auc: 0.5004 - val_loss: 0.4669 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.4234 - auc: 0.4932 - val_loss: 0.3780 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.3474 - auc: 0.5093 - val_loss: 0.3136 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.2922 - auc: 0.4977 - val_loss: 0.2663 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.2516 - auc: 0.4714 - val_loss: 0.2313 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.2213 - auc: 0.5182 - val_loss: 0.2049 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1983 - auc: 0.4759 - val_loss: 0.1845 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1805 - auc: 0.4774 - val_loss: 0.1686 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1665 - auc: 0.5061 - val_loss: 0.1560 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1554 - auc: 0.5071 - val_loss: 0.1458 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1464 - auc: 0.5034 - val_loss: 0.1376 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1391 - auc: 0.4739 - val_loss: 0.1307 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.1330 - auc: 0.4952 - val_loss: 0.1251 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.1280 - auc: 0.5142 - val_loss: 0.1204 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1238 - auc: 0.5114 - val_loss: 0.1164 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1202 - auc: 0.4962 - val_loss: 0.1130 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1172 - auc: 0.5000 - val_loss: 0.1101 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1147 - auc: 0.5014Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1147 - auc: 0.5014 - val_loss: 0.1076 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.1\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 10\n",
      "activation_dense: softmax\n",
      "learning_rate: 3.0e-04\n",
      "l1_lambda: 1.0e-03\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 4s 40ms/step - loss: 0.5759 - auc: 0.4651 - val_loss: 0.5005 - val_auc: 0.4183\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.4742 - auc: 0.4668 - val_loss: 0.4540 - val_auc: 0.4428\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.4450 - auc: 0.4919 - val_loss: 0.4340 - val_auc: 0.4950\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.4281 - auc: 0.4938 - val_loss: 0.4194 - val_auc: 0.4996\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.4148 - auc: 0.4885 - val_loss: 0.4072 - val_auc: 0.4835\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.4036 - auc: 0.5024 - val_loss: 0.3968 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.3940 - auc: 0.4775 - val_loss: 0.3879 - val_auc: 0.4107\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.3856 - auc: 0.4924 - val_loss: 0.3798 - val_auc: 0.4963\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.3779 - auc: 0.4862 - val_loss: 0.3724 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.3707 - auc: 0.4481 - val_loss: 0.3653 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.3639 - auc: 0.4978 - val_loss: 0.3587 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.3574 - auc: 0.4931 - val_loss: 0.3523 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.3513 - auc: 0.4991 - val_loss: 0.3462 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.3453 - auc: 0.5030 - val_loss: 0.3403 - val_auc: 0.4979\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.3396 - auc: 0.4999 - val_loss: 0.3346 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.3340 - auc: 0.4991 - val_loss: 0.3291 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.3286 - auc: 0.5459 - val_loss: 0.3238 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.3234 - auc: 0.4879 - val_loss: 0.3186 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.3184 - auc: 0.4925 - val_loss: 0.3136 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.3134 - auc: 0.4940 - val_loss: 0.3087 - val_auc: 0.4959\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.3087 - auc: 0.4999 - val_loss: 0.3039 - val_auc: 0.5000\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.3040 - auc: 0.5211 - val_loss: 0.2993 - val_auc: 0.5000\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.2995 - auc: 0.4698 - val_loss: 0.2948 - val_auc: 0.5000\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.2950 - auc: 0.5000 - val_loss: 0.2904 - val_auc: 0.4640\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.2907 - auc: 0.5005 - val_loss: 0.2861 - val_auc: 0.5000\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2865 - auc: 0.5483Restoring model weights from the end of the best epoch: 6.\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.2865 - auc: 0.5483 - val_loss: 0.2819 - val_auc: 0.5000\n",
      "Epoch 26: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 3\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.1\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 5\n",
      "activation_dense: relu\n",
      "learning_rate: 4.2e-02\n",
      "l1_lambda: 0.0e+00\n",
      "l2_lambda: 1.0e-06\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 12s 69ms/step - loss: 0.4237 - auc: 0.5000 - val_loss: 0.2356 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 49ms/step - loss: 0.1800 - auc: 0.5000 - val_loss: 0.1365 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1266 - auc: 0.5000 - val_loss: 0.1098 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.1103 - auc: 0.5000 - val_loss: 0.0998 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.1037 - auc: 0.5000 - val_loss: 0.0949 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.1007 - auc: 0.5000 - val_loss: 0.0925 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0992 - auc: 0.5000 - val_loss: 0.0913 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.0984 - auc: 0.5000 - val_loss: 0.0907 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.0981 - auc: 0.5000 - val_loss: 0.0902 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0978 - auc: 0.5000 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.0978 - auc: 0.5000 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0976 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 3\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.0\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 6\n",
      "activation_dense: relu\n",
      "learning_rate: 1.3e-03\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 10s 58ms/step - loss: 33.3740 - auc: 0.5000 - val_loss: 23.9226 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 17.4578 - auc: 0.5000 - val_loss: 11.6610 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 8.0843 - auc: 0.5000 - val_loss: 5.0308 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 3.3648 - auc: 0.5000 - val_loss: 1.9902 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 1.3128 - auc: 0.5000 - val_loss: 0.7766 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.5259 - auc: 0.5000 - val_loss: 0.3314 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.2606 - auc: 0.5000 - val_loss: 0.1962 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1869 - auc: 0.5000 - val_loss: 0.1676 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.1738 - auc: 0.5000 - val_loss: 0.1656 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1729 - auc: 0.5000 - val_loss: 0.1651 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.1731 - auc: 0.5000 - val_loss: 0.1657 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1727 - auc: 0.5000 - val_loss: 0.1641 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1726 - auc: 0.5000 - val_loss: 0.1643 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1720 - auc: 0.5000 - val_loss: 0.1627 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.1717 - auc: 0.5000 - val_loss: 0.1629 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1709 - auc: 0.5000 - val_loss: 0.1632 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1700 - auc: 0.5000 - val_loss: 0.1633 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1691 - auc: 0.5000 - val_loss: 0.1618 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1680 - auc: 0.5000 - val_loss: 0.1617 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1665 - auc: 0.5000 - val_loss: 0.1588 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1648 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1648 - auc: 0.5000 - val_loss: 0.1588 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.0\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 5\n",
      "activation_dense: relu\n",
      "learning_rate: 1.1e-04\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 8s 50ms/step - loss: 28.7943 - auc: 0.5147 - val_loss: 28.0566 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 27.3867 - auc: 0.5110 - val_loss: 26.6716 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 26.0224 - auc: 0.5029 - val_loss: 25.3292 - val_auc: 0.4144\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 24.7205 - auc: 0.4927 - val_loss: 24.0732 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 23.4882 - auc: 0.5000 - val_loss: 22.8647 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 22.3035 - auc: 0.5000 - val_loss: 21.7045 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 21.1627 - auc: 0.5000 - val_loss: 20.5851 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 20.0634 - auc: 0.5066 - val_loss: 19.5081 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 19.0050 - auc: 0.5000 - val_loss: 18.4689 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 17.9865 - auc: 0.5000 - val_loss: 17.4724 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 17.0091 - auc: 0.5000 - val_loss: 16.5163 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 16.0717 - auc: 0.5156 - val_loss: 15.5986 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 15.1726 - auc: 0.5000 - val_loss: 14.7194 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 14.3113 - auc: 0.5000 - val_loss: 13.8773 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 13.4881 - auc: 0.5000 - val_loss: 13.0738 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 12.7024 - auc: 0.5000 - val_loss: 12.3079 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 11.9543 - auc: 0.5237 - val_loss: 11.5780 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 11.2400 - auc: 0.5000 - val_loss: 10.8793 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 10.5551 - auc: 0.5000 - val_loss: 10.2114 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 9.9052 - auc: 0.5000 - val_loss: 9.5804 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 9.2896 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 9.2896 - auc: 0.5000 - val_loss: 8.9808 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 19\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 2.1e-03\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-06\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 9s 56ms/step - loss: 18.9490 - auc: 0.5159 - val_loss: 10.1368 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 5.7384 - auc: 0.4903 - val_loss: 2.4890 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 1.2874 - auc: 0.4683 - val_loss: 0.5340 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.3373 - auc: 0.5360 - val_loss: 0.2156 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1972 - auc: 0.5033 - val_loss: 0.1793 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1840 - auc: 0.5062 - val_loss: 0.1756 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1827 - auc: 0.4997 - val_loss: 0.1746 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1827 - auc: 0.5000 - val_loss: 0.1749 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1825 - auc: 0.5000 - val_loss: 0.1768 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1823 - auc: 0.4798 - val_loss: 0.1751 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1825 - auc: 0.5000 - val_loss: 0.1747 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1825 - auc: 0.4452 - val_loss: 0.1744 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1825 - auc: 0.5030 - val_loss: 0.1738 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1821 - auc: 0.5087 - val_loss: 0.1757 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1826 - auc: 0.4318 - val_loss: 0.1743 - val_auc: 0.5434\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1824 - auc: 0.5289 - val_loss: 0.1726 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1823 - auc: 0.5111 - val_loss: 0.1726 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1825 - auc: 0.5197 - val_loss: 0.1755 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1823 - auc: 0.4984 - val_loss: 0.1746 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1823 - auc: 0.5023 - val_loss: 0.1749 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1824 - auc: 0.5169 - val_loss: 0.1741 - val_auc: 0.5000\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1824 - auc: 0.5158 - val_loss: 0.1742 - val_auc: 0.6762\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1824 - auc: 0.5080 - val_loss: 0.1743 - val_auc: 0.5000\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1824 - auc: 0.4402 - val_loss: 0.1742 - val_auc: 0.5000\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1823 - auc: 0.5248 - val_loss: 0.1758 - val_auc: 0.5000\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1824 - auc: 0.5395 - val_loss: 0.1746 - val_auc: 0.5000\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1822 - auc: 0.5487 - val_loss: 0.1738 - val_auc: 0.6331\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1823 - auc: 0.5076 - val_loss: 0.1756 - val_auc: 0.5000\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1825 - auc: 0.5214 - val_loss: 0.1752 - val_auc: 0.6314\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1820 - auc: 0.6276 - val_loss: 0.1751 - val_auc: 0.7157\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1821 - auc: 0.6348 - val_loss: 0.1744 - val_auc: 0.5000\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1819 - auc: 0.7169 - val_loss: 0.1749 - val_auc: 0.7860\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1818 - auc: 0.5646 - val_loss: 0.1730 - val_auc: 0.5000\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1814 - auc: 0.5647 - val_loss: 0.1742 - val_auc: 0.7361\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1808 - auc: 0.6170 - val_loss: 0.1729 - val_auc: 0.7394\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1799 - auc: 0.7566 - val_loss: 0.1709 - val_auc: 0.7975\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1781 - auc: 0.8228 - val_loss: 0.1703 - val_auc: 0.8108\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1772 - auc: 0.8084 - val_loss: 0.1707 - val_auc: 0.7831\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1758 - auc: 0.8142 - val_loss: 0.1701 - val_auc: 0.7362\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1729 - auc: 0.8550 - val_loss: 0.1653 - val_auc: 0.8223\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1707 - auc: 0.8696 - val_loss: 0.1632 - val_auc: 0.7972\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1686 - auc: 0.8767 - val_loss: 0.1637 - val_auc: 0.8034\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1665 - auc: 0.8698 - val_loss: 0.1658 - val_auc: 0.6961\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1648 - auc: 0.8452 - val_loss: 0.1653 - val_auc: 0.7909\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1606 - auc: 0.8966 - val_loss: 0.1654 - val_auc: 0.7858\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1584 - auc: 0.8878 - val_loss: 0.1630 - val_auc: 0.7889\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1583 - auc: 0.8835 - val_loss: 0.1593 - val_auc: 0.8009\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1557 - auc: 0.8775 - val_loss: 0.1642 - val_auc: 0.7377\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1532 - auc: 0.8899 - val_loss: 0.1651 - val_auc: 0.7548\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1500 - auc: 0.8876 - val_loss: 0.1608 - val_auc: 0.8056\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1461 - auc: 0.9092 - val_loss: 0.1711 - val_auc: 0.6495\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1496 - auc: 0.8644 - val_loss: 0.1700 - val_auc: 0.6441\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1495 - auc: 0.8773 - val_loss: 0.1640 - val_auc: 0.8148\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1443 - auc: 0.9209 - val_loss: 0.1703 - val_auc: 0.7959\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1437 - auc: 0.9068 - val_loss: 0.1671 - val_auc: 0.7990\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1440 - auc: 0.9138 - val_loss: 0.1668 - val_auc: 0.8168\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1425 - auc: 0.9099 - val_loss: 0.1662 - val_auc: 0.7900\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1441 - auc: 0.9090 - val_loss: 0.1700 - val_auc: 0.7812\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1397 - auc: 0.9188 - val_loss: 0.1746 - val_auc: 0.7543\n",
      "Epoch 60/1000\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.1399 - auc: 0.9223Restoring model weights from the end of the best epoch: 40.\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1380 - auc: 0.9228 - val_loss: 0.1678 - val_auc: 0.7441\n",
      "Epoch 60: early stopping\n",
      "\n",
      "Validation AUC: 74.41%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.3\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 28\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 5.6e-04\n",
      "l1_lambda: 1.0e-05\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 8s 71ms/step - loss: 0.7138 - auc: 0.5136 - val_loss: 0.5175 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.3997 - auc: 0.4410 - val_loss: 0.2973 - val_auc: 0.4988\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2472 - auc: 0.4909 - val_loss: 0.2004 - val_auc: 0.4219\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1805 - auc: 0.4646 - val_loss: 0.1559 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1481 - auc: 0.5491 - val_loss: 0.1327 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1305 - auc: 0.5274 - val_loss: 0.1191 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1201 - auc: 0.5163 - val_loss: 0.1106 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1134 - auc: 0.5218 - val_loss: 0.1049 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1090 - auc: 0.4903 - val_loss: 0.1011 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1060 - auc: 0.4902 - val_loss: 0.0983 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1039 - auc: 0.4844 - val_loss: 0.0964 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1023 - auc: 0.4808 - val_loss: 0.0949 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1012 - auc: 0.5000 - val_loss: 0.0938 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1003 - auc: 0.5000 - val_loss: 0.0929 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0997 - auc: 0.4871 - val_loss: 0.0923 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0992 - auc: 0.5000 - val_loss: 0.0918 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0988 - auc: 0.5000 - val_loss: 0.0913 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0985 - auc: 0.5000 - val_loss: 0.0910 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0983 - auc: 0.5000 - val_loss: 0.0907 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0981 - auc: 0.4949 - val_loss: 0.0905 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0980 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0980 - auc: 0.5000 - val_loss: 0.0904 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 26\n",
      "activation_dense: relu\n",
      "learning_rate: 6.2e-02\n",
      "l1_lambda: 1.0e-02\n",
      "l2_lambda: 1.0e-01\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 9s 60ms/step - loss: 0.5558 - auc: 0.5000 - val_loss: 0.1759 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1631 - auc: 0.5000 - val_loss: 0.1424 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1608 - auc: 0.5000 - val_loss: 0.1418 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1627 - auc: 0.5000 - val_loss: 0.1609 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1665 - auc: 0.5000 - val_loss: 0.1594 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1651 - auc: 0.5000 - val_loss: 0.1642 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1673 - auc: 0.5000 - val_loss: 0.1572 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.1681 - auc: 0.5000 - val_loss: 0.1428 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1672 - auc: 0.5000 - val_loss: 0.1550 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1675 - auc: 0.5000 - val_loss: 0.1652 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.1683 - auc: 0.5000 - val_loss: 0.1699 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1682 - auc: 0.5000 - val_loss: 0.1552 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1697 - auc: 0.5000 - val_loss: 0.1416 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1692 - auc: 0.5000 - val_loss: 0.1538 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1684 - auc: 0.5000 - val_loss: 0.1701 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.1678 - auc: 0.5000 - val_loss: 0.1715 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1710 - auc: 0.5000 - val_loss: 0.1527 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1704 - auc: 0.5000 - val_loss: 0.1465 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1701 - auc: 0.5000 - val_loss: 0.1620 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1693 - auc: 0.5000 - val_loss: 0.1687 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.1693 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1699 - auc: 0.5000 - val_loss: 0.1635 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.0\n",
      "num_dense_layers: 4\n",
      "num_dense_nodes: 18\n",
      "activation_dense: softmax\n",
      "learning_rate: 1.1e-02\n",
      "l1_lambda: 0.0e+00\n",
      "l2_lambda: 1.0e-04\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 9s 68ms/step - loss: 0.5873 - auc: 0.5000 - val_loss: 0.3911 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.2911 - auc: 0.5000 - val_loss: 0.2039 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1608 - auc: 0.5000 - val_loss: 0.1200 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1134 - auc: 0.5000 - val_loss: 0.0994 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.1029 - auc: 0.5000 - val_loss: 0.0937 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0997 - auc: 0.5000 - val_loss: 0.0917 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0986 - auc: 0.5000 - val_loss: 0.0907 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0980 - auc: 0.5000 - val_loss: 0.0902 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0978 - auc: 0.5000 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.0950 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 3\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 8\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 6.8e-05\n",
      "l1_lambda: 1.0e-02\n",
      "l2_lambda: 1.0e-02\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 11s 65ms/step - loss: 2.4660 - auc: 0.5403 - val_loss: 2.4391 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 2.4149 - auc: 0.5116 - val_loss: 2.3884 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 2.3648 - auc: 0.4807 - val_loss: 2.3388 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 2.3157 - auc: 0.5065 - val_loss: 2.2901 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 2.2674 - auc: 0.5158 - val_loss: 2.2421 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 2.2199 - auc: 0.4729 - val_loss: 2.1950 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 2.1733 - auc: 0.4783 - val_loss: 2.1487 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 2.1274 - auc: 0.4919 - val_loss: 2.1031 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 2.0823 - auc: 0.4870 - val_loss: 2.0584 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 2.0378 - auc: 0.5005 - val_loss: 2.0142 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 1.9940 - auc: 0.5240 - val_loss: 1.9707 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 1.9510 - auc: 0.5000 - val_loss: 1.9280 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 1.9087 - auc: 0.4877 - val_loss: 1.8860 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 1.8670 - auc: 0.4933 - val_loss: 1.8446 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 1.8261 - auc: 0.4778 - val_loss: 1.8039 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 1.7857 - auc: 0.5212 - val_loss: 1.7639 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 1.7460 - auc: 0.5170 - val_loss: 1.7244 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 1.7069 - auc: 0.4895 - val_loss: 1.6856 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 1.6686 - auc: 0.4829 - val_loss: 1.6475 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 1.6307 - auc: 0.4955 - val_loss: 1.6098 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.5934 - auc: 0.5146Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 1.5934 - auc: 0.5146 - val_loss: 1.5728 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.0\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 17\n",
      "activation_dense: relu\n",
      "learning_rate: 3.7e-02\n",
      "l1_lambda: 0.0e+00\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 11s 81ms/step - loss: 0.1477 - auc: 0.5113 - val_loss: 0.0933 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1016 - auc: 0.4871 - val_loss: 0.0926 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0990 - auc: 0.4997 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0990 - auc: 0.5270 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0985 - auc: 0.5270 - val_loss: 0.0908 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0999 - auc: 0.4348 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1001 - auc: 0.4734 - val_loss: 0.0946 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0992 - auc: 0.5217 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0984 - auc: 0.4840 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0987 - auc: 0.4968 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0994 - auc: 0.4820 - val_loss: 0.0938 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0988 - auc: 0.4929 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1011 - auc: 0.4699 - val_loss: 0.0924 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0996 - auc: 0.4875 - val_loss: 0.0902 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0996 - auc: 0.4966 - val_loss: 0.0926 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0992 - auc: 0.5453 - val_loss: 0.0913 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0987 - auc: 0.5114 - val_loss: 0.0901 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0978 - auc: 0.5247 - val_loss: 0.0909 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0984 - auc: 0.4653 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0985 - auc: 0.4849 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0991 - auc: 0.4925Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0980 - auc: 0.4943 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 23\n",
      "activation_dense: softmax\n",
      "learning_rate: 8.0e-04\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 4s 46ms/step - loss: 0.6156 - auc: 0.4939 - val_loss: 0.5600 - val_auc: 0.4988\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.5261 - auc: 0.5139 - val_loss: 0.4919 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.4698 - auc: 0.5003 - val_loss: 0.4468 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.4322 - auc: 0.4739 - val_loss: 0.4153 - val_auc: 0.4860\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.4044 - auc: 0.5209 - val_loss: 0.3905 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.3818 - auc: 0.4571 - val_loss: 0.3693 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.3620 - auc: 0.5047 - val_loss: 0.3507 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.3443 - auc: 0.4941 - val_loss: 0.3337 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.3280 - auc: 0.5384 - val_loss: 0.3179 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.3130 - auc: 0.4557 - val_loss: 0.3033 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.2991 - auc: 0.4957 - val_loss: 0.2899 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.2863 - auc: 0.4876 - val_loss: 0.2775 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.2746 - auc: 0.4905 - val_loss: 0.2662 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.2638 - auc: 0.5185 - val_loss: 0.2558 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.2538 - auc: 0.4746 - val_loss: 0.2461 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.2446 - auc: 0.4915 - val_loss: 0.2372 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.2361 - auc: 0.5435 - val_loss: 0.2290 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.2283 - auc: 0.5421 - val_loss: 0.2212 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.2209 - auc: 0.5134 - val_loss: 0.2141 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.2141 - auc: 0.5024 - val_loss: 0.2074 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.2077 - auc: 0.5117 - val_loss: 0.2011 - val_auc: 0.5000\n",
      "Epoch 22/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2019 - auc: 0.4891Restoring model weights from the end of the best epoch: 2.\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.2018 - auc: 0.4896 - val_loss: 0.1953 - val_auc: 0.5000\n",
      "Epoch 22: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.0\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 25\n",
      "activation_dense: softmax\n",
      "learning_rate: 2.5e-03\n",
      "l1_lambda: 1.0e-03\n",
      "l2_lambda: 1.0e-06\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 8s 52ms/step - loss: 0.6639 - auc: 0.4488 - val_loss: 0.5676 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.4927 - auc: 0.5372 - val_loss: 0.4221 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.3805 - auc: 0.4463 - val_loss: 0.3389 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.3124 - auc: 0.5001 - val_loss: 0.2822 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.2636 - auc: 0.5068 - val_loss: 0.2396 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.2243 - auc: 0.4937 - val_loss: 0.2019 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.1914 - auc: 0.4761 - val_loss: 0.1744 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.1693 - auc: 0.4834 - val_loss: 0.1564 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.1544 - auc: 0.5135 - val_loss: 0.1439 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1436 - auc: 0.5375 - val_loss: 0.1343 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.1354 - auc: 0.4912 - val_loss: 0.1269 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.1289 - auc: 0.5260 - val_loss: 0.1210 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.1238 - auc: 0.5331 - val_loss: 0.1162 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.1197 - auc: 0.4981 - val_loss: 0.1122 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.1163 - auc: 0.4924 - val_loss: 0.1090 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.1135 - auc: 0.4934 - val_loss: 0.1063 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.1111 - auc: 0.5175 - val_loss: 0.1040 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.1091 - auc: 0.5000 - val_loss: 0.1021 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.1075 - auc: 0.5038 - val_loss: 0.1004 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.1061 - auc: 0.5000 - val_loss: 0.0991 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1049 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.1049 - auc: 0.5000 - val_loss: 0.0979 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.3\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 27\n",
      "activation_dense: softmax\n",
      "learning_rate: 5.3e-05\n",
      "l1_lambda: 1.0e-05\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 9s 74ms/step - loss: 0.6814 - auc: 0.4891 - val_loss: 0.6800 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.6789 - auc: 0.5000 - val_loss: 0.6776 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.6765 - auc: 0.5000 - val_loss: 0.6751 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.6740 - auc: 0.5000 - val_loss: 0.6726 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.6716 - auc: 0.5080 - val_loss: 0.6702 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.6691 - auc: 0.5000 - val_loss: 0.6678 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.6667 - auc: 0.5000 - val_loss: 0.6653 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.6643 - auc: 0.5000 - val_loss: 0.6629 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.6619 - auc: 0.5009 - val_loss: 0.6605 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.6595 - auc: 0.5000 - val_loss: 0.6581 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.6571 - auc: 0.5000 - val_loss: 0.6557 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.6548 - auc: 0.5000 - val_loss: 0.6533 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.6524 - auc: 0.5118 - val_loss: 0.6510 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.6500 - auc: 0.5000 - val_loss: 0.6486 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.6477 - auc: 0.5000 - val_loss: 0.6463 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.6453 - auc: 0.5000 - val_loss: 0.6439 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.6430 - auc: 0.5251 - val_loss: 0.6416 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.6407 - auc: 0.5000 - val_loss: 0.6392 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.6384 - auc: 0.5000 - val_loss: 0.6369 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.6361 - auc: 0.5000 - val_loss: 0.6346 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.6338 - auc: 0.5098Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.6338 - auc: 0.5071 - val_loss: 0.6323 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.3\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 26\n",
      "activation_dense: tanh\n",
      "learning_rate: 2.3e-04\n",
      "l1_lambda: 1.0e-04\n",
      "l2_lambda: 1.0e-06\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 5s 50ms/step - loss: 0.2872 - auc: 0.4818 - val_loss: 0.1448 - val_auc: 0.5346\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.1252 - auc: 0.4337 - val_loss: 0.1030 - val_auc: 0.5377\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.1043 - auc: 0.5034 - val_loss: 0.0938 - val_auc: 0.5120\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0995 - auc: 0.4976 - val_loss: 0.0911 - val_auc: 0.4996\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0982 - auc: 0.4628 - val_loss: 0.0900 - val_auc: 0.4988\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0977 - auc: 0.4987 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0974 - auc: 0.5002 - val_loss: 0.0894 - val_auc: 0.5091\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0971 - auc: 0.5252 - val_loss: 0.0892 - val_auc: 0.5846\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0967 - auc: 0.6432 - val_loss: 0.0888 - val_auc: 0.6256\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0958 - auc: 0.7031 - val_loss: 0.0882 - val_auc: 0.6768\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0933 - auc: 0.7594 - val_loss: 0.0866 - val_auc: 0.7195\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0893 - auc: 0.7715 - val_loss: 0.0839 - val_auc: 0.7595\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0839 - auc: 0.8101 - val_loss: 0.0823 - val_auc: 0.7599\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0793 - auc: 0.8352 - val_loss: 0.0803 - val_auc: 0.7731\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0751 - auc: 0.8533 - val_loss: 0.0799 - val_auc: 0.7991\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0702 - auc: 0.8688 - val_loss: 0.0811 - val_auc: 0.8007\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0662 - auc: 0.8680 - val_loss: 0.0817 - val_auc: 0.8252\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0615 - auc: 0.8844 - val_loss: 0.0837 - val_auc: 0.7559\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0563 - auc: 0.8883 - val_loss: 0.0854 - val_auc: 0.7791\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0523 - auc: 0.9156 - val_loss: 0.0863 - val_auc: 0.7505\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0479 - auc: 0.9304 - val_loss: 0.0883 - val_auc: 0.7956\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0427 - auc: 0.9412 - val_loss: 0.0972 - val_auc: 0.7835\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0406 - auc: 0.9500 - val_loss: 0.0914 - val_auc: 0.7475\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0369 - auc: 0.9548 - val_loss: 0.0954 - val_auc: 0.7505\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0337 - auc: 0.9468 - val_loss: 0.1001 - val_auc: 0.7510\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0303 - auc: 0.9545 - val_loss: 0.1046 - val_auc: 0.6885\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0278 - auc: 0.9582 - val_loss: 0.1039 - val_auc: 0.6971\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0240 - auc: 0.9672 - val_loss: 0.1072 - val_auc: 0.7064\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0215 - auc: 0.9703 - val_loss: 0.1145 - val_auc: 0.7035\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0192 - auc: 0.9681 - val_loss: 0.1180 - val_auc: 0.6912\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0170 - auc: 0.9742 - val_loss: 0.1215 - val_auc: 0.6953\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0144 - auc: 0.9757 - val_loss: 0.1279 - val_auc: 0.6760\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0129 - auc: 0.9767 - val_loss: 0.1255 - val_auc: 0.6631\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0109 - auc: 0.9924 - val_loss: 0.1409 - val_auc: 0.6716\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0098 - auc: 0.9928 - val_loss: 0.1282 - val_auc: 0.6044\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0078 - auc: 0.9991 - val_loss: 0.1428 - val_auc: 0.6369\n",
      "Epoch 37/1000\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.0066 - auc: 0.9993Restoring model weights from the end of the best epoch: 17.\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0065 - auc: 0.9993 - val_loss: 0.1361 - val_auc: 0.6295\n",
      "Epoch 37: early stopping\n",
      "\n",
      "Validation AUC: 62.95%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.3\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 29\n",
      "activation_dense: softmax\n",
      "learning_rate: 6.5e-05\n",
      "l1_lambda: 1.0e-04\n",
      "l2_lambda: 1.0e-03\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 9s 79ms/step - loss: 0.7583 - auc: 0.5000 - val_loss: 0.7556 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.7532 - auc: 0.5000 - val_loss: 0.7506 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 50ms/step - loss: 0.7481 - auc: 0.5345 - val_loss: 0.7456 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.7432 - auc: 0.5000 - val_loss: 0.7407 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.7384 - auc: 0.5000 - val_loss: 0.7359 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.7336 - auc: 0.4744 - val_loss: 0.7312 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.7290 - auc: 0.5000 - val_loss: 0.7266 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.7244 - auc: 0.5000 - val_loss: 0.7220 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.7199 - auc: 0.5000 - val_loss: 0.7175 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.7155 - auc: 0.5000 - val_loss: 0.7131 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.7111 - auc: 0.5000 - val_loss: 0.7088 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.7069 - auc: 0.5000 - val_loss: 0.7046 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.7026 - auc: 0.5137 - val_loss: 0.7004 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.6985 - auc: 0.5000 - val_loss: 0.6962 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.6944 - auc: 0.5000 - val_loss: 0.6922 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.6904 - auc: 0.4948 - val_loss: 0.6882 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.6864 - auc: 0.5000 - val_loss: 0.6842 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.6825 - auc: 0.5000 - val_loss: 0.6803 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.6787 - auc: 0.5095 - val_loss: 0.6765 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.6749 - auc: 0.5000 - val_loss: 0.6727 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.6711 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.6711 - auc: 0.5000 - val_loss: 0.6690 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.3\n",
      "num_dense_layers: 4\n",
      "num_dense_nodes: 29\n",
      "activation_dense: tanh\n",
      "learning_rate: 4.4e-03\n",
      "l1_lambda: 1.0e-06\n",
      "l2_lambda: 1.0e-03\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 12s 86ms/step - loss: 0.1820 - auc: 0.5134 - val_loss: 0.0933 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1000 - auc: 0.4483 - val_loss: 0.0905 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0984 - auc: 0.4789 - val_loss: 0.0913 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0981 - auc: 0.4720 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.0978 - auc: 0.4661 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0981 - auc: 0.4801 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0981 - auc: 0.5042 - val_loss: 0.0904 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0979 - auc: 0.4998 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0981 - auc: 0.4845 - val_loss: 0.0901 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0980 - auc: 0.4926 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0977 - auc: 0.5126 - val_loss: 0.0901 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0978 - auc: 0.5121 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0980 - auc: 0.4793 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0979 - auc: 0.4566 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0978 - auc: 0.5222 - val_loss: 0.0913 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0982 - auc: 0.4564 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0979 - auc: 0.4525 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0978 - auc: 0.4678 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0977 - auc: 0.4671 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0980 - auc: 0.4915 - val_loss: 0.0902 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0989 - auc: 0.4994Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.0978 - auc: 0.5032 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.0\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 16\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 1.3e-05\n",
      "l1_lambda: 1.0e-06\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 8s 67ms/step - loss: 0.2234 - auc: 0.5105 - val_loss: 0.2192 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.2214 - auc: 0.5000 - val_loss: 0.2172 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.2195 - auc: 0.5002 - val_loss: 0.2153 - val_auc: 0.4827\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.2175 - auc: 0.4975 - val_loss: 0.2132 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.2155 - auc: 0.5000 - val_loss: 0.2112 - val_auc: 0.5017\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.2135 - auc: 0.4460 - val_loss: 0.2090 - val_auc: 0.4992\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.2114 - auc: 0.5000 - val_loss: 0.2069 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.2092 - auc: 0.4860 - val_loss: 0.2047 - val_auc: 0.4617\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.2070 - auc: 0.4928 - val_loss: 0.2025 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.2049 - auc: 0.4909 - val_loss: 0.2003 - val_auc: 0.4570\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.2028 - auc: 0.4323 - val_loss: 0.1982 - val_auc: 0.4979\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.2007 - auc: 0.4999 - val_loss: 0.1961 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1986 - auc: 0.4637 - val_loss: 0.1941 - val_auc: 0.4687\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1967 - auc: 0.4881 - val_loss: 0.1921 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1948 - auc: 0.5000 - val_loss: 0.1902 - val_auc: 0.5012\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1929 - auc: 0.4996 - val_loss: 0.1884 - val_auc: 0.4876\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1911 - auc: 0.4977 - val_loss: 0.1865 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1894 - auc: 0.5005 - val_loss: 0.1848 - val_auc: 0.4616\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1877 - auc: 0.4525 - val_loss: 0.1831 - val_auc: 0.4971\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1860 - auc: 0.4996 - val_loss: 0.1814 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1843 - auc: 0.5001 - val_loss: 0.1797 - val_auc: 0.4570\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1827 - auc: 0.4597 - val_loss: 0.1781 - val_auc: 0.4955\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1811 - auc: 0.4995 - val_loss: 0.1765 - val_auc: 0.5000\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1796 - auc: 0.5000 - val_loss: 0.1749 - val_auc: 0.5000\n",
      "Epoch 25/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.1782 - auc: 0.4651Restoring model weights from the end of the best epoch: 5.\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1781 - auc: 0.4662 - val_loss: 0.1734 - val_auc: 0.4810\n",
      "Epoch 25: early stopping\n",
      "\n",
      "Validation AUC: 48.10%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 18\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 1.6e-03\n",
      "l1_lambda: 1.0e-03\n",
      "l2_lambda: 1.0e-06\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 12s 100ms/step - loss: 0.6011 - auc: 0.5252 - val_loss: 0.3005 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 50ms/step - loss: 0.2200 - auc: 0.5221 - val_loss: 0.1490 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.1294 - auc: 0.4881 - val_loss: 0.1038 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.1051 - auc: 0.4994 - val_loss: 0.0937 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.1002 - auc: 0.5004 - val_loss: 0.0918 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0993 - auc: 0.5000 - val_loss: 0.0914 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0991 - auc: 0.5000 - val_loss: 0.0912 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.0991 - auc: 0.5000 - val_loss: 0.0912 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 50ms/step - loss: 0.0991 - auc: 0.4657 - val_loss: 0.0910 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 50ms/step - loss: 0.0990 - auc: 0.4896 - val_loss: 0.0911 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.0990 - auc: 0.5060 - val_loss: 0.0911 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0990 - auc: 0.5007 - val_loss: 0.0910 - val_auc: 0.7043\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0990 - auc: 0.5227 - val_loss: 0.0910 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0990 - auc: 0.4894 - val_loss: 0.0911 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0990 - auc: 0.5104 - val_loss: 0.0910 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.0990 - auc: 0.5187 - val_loss: 0.0910 - val_auc: 0.5355\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.0990 - auc: 0.5101 - val_loss: 0.0911 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0988 - auc: 0.5480 - val_loss: 0.0906 - val_auc: 0.7769\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 50ms/step - loss: 0.0981 - auc: 0.6659 - val_loss: 0.0898 - val_auc: 0.7169\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 0.0933 - auc: 0.7550 - val_loss: 0.0854 - val_auc: 0.7772\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.0886 - auc: 0.7569 - val_loss: 0.0837 - val_auc: 0.7701\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0847 - auc: 0.8043 - val_loss: 0.0881 - val_auc: 0.6142\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 0.0878 - auc: 0.7742 - val_loss: 0.0839 - val_auc: 0.7789\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.0791 - auc: 0.8321 - val_loss: 0.0865 - val_auc: 0.6682\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.0761 - auc: 0.7990 - val_loss: 0.0860 - val_auc: 0.6632\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0780 - auc: 0.8307 - val_loss: 0.0886 - val_auc: 0.6591\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0789 - auc: 0.8248 - val_loss: 0.0854 - val_auc: 0.6643\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0777 - auc: 0.8004 - val_loss: 0.0896 - val_auc: 0.6533\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.0773 - auc: 0.8421 - val_loss: 0.0860 - val_auc: 0.6623\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0773 - auc: 0.8282 - val_loss: 0.0893 - val_auc: 0.6530\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0763 - auc: 0.8161 - val_loss: 0.0870 - val_auc: 0.6757\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.0737 - auc: 0.8445 - val_loss: 0.0911 - val_auc: 0.6486\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0761 - auc: 0.8361 - val_loss: 0.0884 - val_auc: 0.6903\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0704 - auc: 0.8277 - val_loss: 0.0922 - val_auc: 0.6498\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0693 - auc: 0.8592 - val_loss: 0.0893 - val_auc: 0.6588\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 3s 67ms/step - loss: 0.0700 - auc: 0.8498 - val_loss: 0.0933 - val_auc: 0.6056\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 3s 65ms/step - loss: 0.0678 - auc: 0.8553 - val_loss: 0.0927 - val_auc: 0.6602\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 3s 69ms/step - loss: 0.0709 - auc: 0.8565 - val_loss: 0.0874 - val_auc: 0.6995\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 3s 67ms/step - loss: 0.0648 - auc: 0.8702 - val_loss: 0.0944 - val_auc: 0.6183\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 3s 65ms/step - loss: 0.0688 - auc: 0.8702 - val_loss: 0.0936 - val_auc: 0.7513\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 3s 65ms/step - loss: 0.0642 - auc: 0.8874 - val_loss: 0.0857 - val_auc: 0.7839\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 0.0640 - auc: 0.8878 - val_loss: 0.0916 - val_auc: 0.7873\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0614 - auc: 0.8989 - val_loss: 0.0885 - val_auc: 0.8013\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0592 - auc: 0.8901 - val_loss: 0.0950 - val_auc: 0.8040\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0604 - auc: 0.9102 - val_loss: 0.0877 - val_auc: 0.7838\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.0593 - auc: 0.9048 - val_loss: 0.0905 - val_auc: 0.7706\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.0536 - auc: 0.9297 - val_loss: 0.0912 - val_auc: 0.7866\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.0543 - auc: 0.9196 - val_loss: 0.0937 - val_auc: 0.7449\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.0541 - auc: 0.9227 - val_loss: 0.0916 - val_auc: 0.7597\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.0564 - auc: 0.9273 - val_loss: 0.0934 - val_auc: 0.7963\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0542 - auc: 0.9202 - val_loss: 0.0872 - val_auc: 0.8178\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0490 - auc: 0.9246 - val_loss: 0.0998 - val_auc: 0.6190\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0451 - auc: 0.9300 - val_loss: 0.0926 - val_auc: 0.7456\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0435 - auc: 0.9328 - val_loss: 0.1018 - val_auc: 0.6268\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 2s 59ms/step - loss: 0.0451 - auc: 0.9300 - val_loss: 0.0994 - val_auc: 0.6479\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 3s 67ms/step - loss: 0.0481 - auc: 0.9233 - val_loss: 0.0932 - val_auc: 0.7382\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.0488 - auc: 0.9296 - val_loss: 0.0961 - val_auc: 0.6889\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0453 - auc: 0.9323 - val_loss: 0.0964 - val_auc: 0.7062\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 2s 50ms/step - loss: 0.0404 - auc: 0.9486 - val_loss: 0.0990 - val_auc: 0.6180\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.0414 - auc: 0.9484 - val_loss: 0.1001 - val_auc: 0.6607\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0421 - auc: 0.9453 - val_loss: 0.0902 - val_auc: 0.7656\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.0453 - auc: 0.9307 - val_loss: 0.1000 - val_auc: 0.6410\n",
      "Epoch 63/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0412 - auc: 0.9381 - val_loss: 0.0918 - val_auc: 0.7849\n",
      "Epoch 64/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0409 - auc: 0.9419 - val_loss: 0.0942 - val_auc: 0.7364\n",
      "Epoch 65/1000\n",
      "39/39 [==============================] - 2s 60ms/step - loss: 0.0368 - auc: 0.9449 - val_loss: 0.0978 - val_auc: 0.6515\n",
      "Epoch 66/1000\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 0.0394 - auc: 0.9455 - val_loss: 0.0976 - val_auc: 0.7037\n",
      "Epoch 67/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0411 - auc: 0.9521 - val_loss: 0.0989 - val_auc: 0.6172\n",
      "Epoch 68/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0387 - auc: 0.9518 - val_loss: 0.1001 - val_auc: 0.6193\n",
      "Epoch 69/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0348 - auc: 0.9590 - val_loss: 0.1018 - val_auc: 0.6723\n",
      "Epoch 70/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0381 - auc: 0.9544 - val_loss: 0.0991 - val_auc: 0.6874\n",
      "Epoch 71/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0321 - auc: 0.9630Restoring model weights from the end of the best epoch: 51.\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.0339 - auc: 0.9622 - val_loss: 0.1050 - val_auc: 0.6560\n",
      "Epoch 71: early stopping\n",
      "\n",
      "Validation AUC: 65.60%\n",
      "\n",
      "num_lstm_layers: 3\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.0\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 23\n",
      "activation_dense: tanh\n",
      "learning_rate: 5.0e-02\n",
      "l1_lambda: 1.0e-04\n",
      "l2_lambda: 1.0e-04\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 11s 81ms/step - loss: 0.1305 - auc: 0.4788 - val_loss: 0.0936 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1026 - auc: 0.4705 - val_loss: 0.0932 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1033 - auc: 0.5128 - val_loss: 0.0946 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1107 - auc: 0.4740 - val_loss: 0.1053 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1026 - auc: 0.5146 - val_loss: 0.0935 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1020 - auc: 0.4942 - val_loss: 0.0947 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1038 - auc: 0.4789 - val_loss: 0.0942 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1066 - auc: 0.5297 - val_loss: 0.0986 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1049 - auc: 0.4963 - val_loss: 0.0937 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1073 - auc: 0.4424 - val_loss: 0.1052 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1051 - auc: 0.5003 - val_loss: 0.0949 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1042 - auc: 0.5520 - val_loss: 0.0941 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1049 - auc: 0.5020 - val_loss: 0.0935 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1020 - auc: 0.5254 - val_loss: 0.0997 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1023 - auc: 0.5080 - val_loss: 0.0949 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1025 - auc: 0.4690 - val_loss: 0.0954 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1035 - auc: 0.5380 - val_loss: 0.0987 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1040 - auc: 0.5243 - val_loss: 0.0925 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1046 - auc: 0.5336 - val_loss: 0.0939 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1094 - auc: 0.5047 - val_loss: 0.0967 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1082 - auc: 0.4775Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1082 - auc: 0.4775 - val_loss: 0.0975 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.3\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 2\n",
      "activation_dense: relu\n",
      "learning_rate: 2.0e-04\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 7s 57ms/step - loss: 14.4228 - auc: 0.5000 - val_loss: 13.8091 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 13.2603 - auc: 0.5203 - val_loss: 12.6784 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 12.1611 - auc: 0.5000 - val_loss: 11.6159 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 11.1335 - auc: 0.5095 - val_loss: 10.6239 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 10.1724 - auc: 0.5000 - val_loss: 9.6945 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 9.2692 - auc: 0.5000 - val_loss: 8.8211 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 8.4260 - auc: 0.4792 - val_loss: 8.0083 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 7.6398 - auc: 0.5000 - val_loss: 7.2506 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 6.9081 - auc: 0.5000 - val_loss: 6.5489 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 6.2356 - auc: 0.4797 - val_loss: 5.9052 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 5.6151 - auc: 0.5000 - val_loss: 5.3110 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 5.0475 - auc: 0.4891 - val_loss: 4.7733 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 4.5358 - auc: 0.5000 - val_loss: 4.2855 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 4.0688 - auc: 0.5000 - val_loss: 3.8418 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 3.6446 - auc: 0.4792 - val_loss: 3.4388 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 3.2590 - auc: 0.5000 - val_loss: 3.0728 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 2.9130 - auc: 0.5000 - val_loss: 2.7471 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 2.6046 - auc: 0.5104 - val_loss: 2.4566 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 2.3308 - auc: 0.5000 - val_loss: 2.1997 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 2.0869 - auc: 0.4962 - val_loss: 1.9681 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.8672 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 1.8659 - auc: 0.5000 - val_loss: 1.7584 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 19\n",
      "activation_dense: relu\n",
      "learning_rate: 2.2e-04\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 8s 69ms/step - loss: 5.4897 - auc: 0.4577 - val_loss: 5.2301 - val_auc: 0.3441\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 5.0252 - auc: 0.4922 - val_loss: 4.8005 - val_auc: 0.3720\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 4.6063 - auc: 0.4985 - val_loss: 4.3954 - val_auc: 0.4587\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 4.2160 - auc: 0.4291 - val_loss: 4.0211 - val_auc: 0.4686\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 3.8573 - auc: 0.4981 - val_loss: 3.6790 - val_auc: 0.4645\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 3.5303 - auc: 0.5049 - val_loss: 3.3673 - val_auc: 0.4814\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 3.2339 - auc: 0.4601 - val_loss: 3.0857 - val_auc: 0.4955\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 2.9669 - auc: 0.4522 - val_loss: 2.8318 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 2.7226 - auc: 0.4416 - val_loss: 2.5958 - val_auc: 0.4880\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 2.4949 - auc: 0.4906 - val_loss: 2.3776 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 2.2875 - auc: 0.4947 - val_loss: 2.1796 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 2.0986 - auc: 0.5037 - val_loss: 2.0002 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 1.9258 - auc: 0.5211 - val_loss: 1.8339 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 1.7667 - auc: 0.4683 - val_loss: 1.6812 - val_auc: 0.4996\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 1.6182 - auc: 0.4993 - val_loss: 1.5368 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 1.4785 - auc: 0.5002 - val_loss: 1.4027 - val_auc: 0.4371\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 1.3515 - auc: 0.4587 - val_loss: 1.2823 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 1.2359 - auc: 0.4999 - val_loss: 1.1708 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 1.1272 - auc: 0.5000 - val_loss: 1.0655 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 1.0257 - auc: 0.5000 - val_loss: 0.9684 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.9320 - auc: 0.5000 - val_loss: 0.8780 - val_auc: 0.5000\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.8456 - auc: 0.5000 - val_loss: 0.7954 - val_auc: 0.5000\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.7654 - auc: 0.5000 - val_loss: 0.7182 - val_auc: 0.5000\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.6917 - auc: 0.5492 - val_loss: 0.6487 - val_auc: 0.5091\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.6257 - auc: 0.5604 - val_loss: 0.5867 - val_auc: 0.5805\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.5670 - auc: 0.6648 - val_loss: 0.5309 - val_auc: 0.6197\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.5125 - auc: 0.6817 - val_loss: 0.4784 - val_auc: 0.6601\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.4641 - auc: 0.7228 - val_loss: 0.4347 - val_auc: 0.7162\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.4229 - auc: 0.7349 - val_loss: 0.3953 - val_auc: 0.7188\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.3848 - auc: 0.7486 - val_loss: 0.3590 - val_auc: 0.7282\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.3504 - auc: 0.7505 - val_loss: 0.3268 - val_auc: 0.7435\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.3198 - auc: 0.7630 - val_loss: 0.2978 - val_auc: 0.7410\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.2923 - auc: 0.7581 - val_loss: 0.2714 - val_auc: 0.7377\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.2658 - auc: 0.7854 - val_loss: 0.2453 - val_auc: 0.7290\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.2402 - auc: 0.7971 - val_loss: 0.2211 - val_auc: 0.7463\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2154 - auc: 0.7887 - val_loss: 0.1973 - val_auc: 0.7740\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1934 - auc: 0.8095 - val_loss: 0.1776 - val_auc: 0.7681\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1756 - auc: 0.7967 - val_loss: 0.1610 - val_auc: 0.7971\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1587 - auc: 0.8208 - val_loss: 0.1472 - val_auc: 0.7587\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1452 - auc: 0.8405 - val_loss: 0.1349 - val_auc: 0.7919\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1342 - auc: 0.8428 - val_loss: 0.1255 - val_auc: 0.8034\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1259 - auc: 0.8375 - val_loss: 0.1188 - val_auc: 0.8006\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1184 - auc: 0.8483 - val_loss: 0.1135 - val_auc: 0.7882\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1125 - auc: 0.8609 - val_loss: 0.1088 - val_auc: 0.7987\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1085 - auc: 0.8585 - val_loss: 0.1055 - val_auc: 0.8086\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1045 - auc: 0.8834 - val_loss: 0.1025 - val_auc: 0.8151\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1002 - auc: 0.8808 - val_loss: 0.0994 - val_auc: 0.8273\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0970 - auc: 0.8703 - val_loss: 0.0961 - val_auc: 0.8077\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0939 - auc: 0.8841 - val_loss: 0.0940 - val_auc: 0.8242\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0908 - auc: 0.8730 - val_loss: 0.0918 - val_auc: 0.8038\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0874 - auc: 0.8819 - val_loss: 0.0897 - val_auc: 0.8162\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0833 - auc: 0.8928 - val_loss: 0.0878 - val_auc: 0.8120\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0851 - auc: 0.8628 - val_loss: 0.0867 - val_auc: 0.8119\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0793 - auc: 0.8961 - val_loss: 0.0870 - val_auc: 0.8016\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0780 - auc: 0.8916 - val_loss: 0.0850 - val_auc: 0.8192\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0771 - auc: 0.8826 - val_loss: 0.0832 - val_auc: 0.8192\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0762 - auc: 0.8908 - val_loss: 0.0825 - val_auc: 0.8159\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0741 - auc: 0.8925 - val_loss: 0.0811 - val_auc: 0.8369\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0723 - auc: 0.9047 - val_loss: 0.0803 - val_auc: 0.8458\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0694 - auc: 0.9080 - val_loss: 0.0818 - val_auc: 0.8382\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0696 - auc: 0.9075 - val_loss: 0.0794 - val_auc: 0.8455\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0681 - auc: 0.9070 - val_loss: 0.0781 - val_auc: 0.8485\n",
      "Epoch 63/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0645 - auc: 0.9139 - val_loss: 0.0790 - val_auc: 0.8161\n",
      "Epoch 64/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0651 - auc: 0.9116 - val_loss: 0.0790 - val_auc: 0.8163\n",
      "Epoch 65/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0636 - auc: 0.9185 - val_loss: 0.0778 - val_auc: 0.8172\n",
      "Epoch 66/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0629 - auc: 0.9083 - val_loss: 0.0791 - val_auc: 0.8271\n",
      "Epoch 67/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0626 - auc: 0.9183 - val_loss: 0.0808 - val_auc: 0.8115\n",
      "Epoch 68/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0613 - auc: 0.9168 - val_loss: 0.0782 - val_auc: 0.8247\n",
      "Epoch 69/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0612 - auc: 0.9162 - val_loss: 0.0800 - val_auc: 0.8251\n",
      "Epoch 70/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0586 - auc: 0.9236 - val_loss: 0.0819 - val_auc: 0.8272\n",
      "Epoch 71/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0579 - auc: 0.9391 - val_loss: 0.0835 - val_auc: 0.8187\n",
      "Epoch 72/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0580 - auc: 0.9275 - val_loss: 0.0811 - val_auc: 0.8322\n",
      "Epoch 73/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0582 - auc: 0.9099 - val_loss: 0.0814 - val_auc: 0.8287\n",
      "Epoch 74/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0568 - auc: 0.9286 - val_loss: 0.0836 - val_auc: 0.8360\n",
      "Epoch 75/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0575 - auc: 0.9318 - val_loss: 0.0835 - val_auc: 0.8189\n",
      "Epoch 76/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0560 - auc: 0.9222 - val_loss: 0.0827 - val_auc: 0.8333\n",
      "Epoch 77/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0536 - auc: 0.9357 - val_loss: 0.0858 - val_auc: 0.7818\n",
      "Epoch 78/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0554 - auc: 0.9395 - val_loss: 0.0836 - val_auc: 0.8290\n",
      "Epoch 79/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0545 - auc: 0.9337 - val_loss: 0.0872 - val_auc: 0.7686\n",
      "Epoch 80/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0543 - auc: 0.9392 - val_loss: 0.0845 - val_auc: 0.8316\n",
      "Epoch 81/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0515 - auc: 0.9400 - val_loss: 0.0864 - val_auc: 0.8125\n",
      "Epoch 82/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0532 - auc: 0.9507Restoring model weights from the end of the best epoch: 62.\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0528 - auc: 0.9513 - val_loss: 0.0877 - val_auc: 0.7828\n",
      "Epoch 82: early stopping\n",
      "\n",
      "Validation AUC: 78.28%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 12\n",
      "activation_dense: relu\n",
      "learning_rate: 4.2e-04\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 6s 55ms/step - loss: 13.8484 - auc: 0.5767 - val_loss: 12.4888 - val_auc: 0.4191\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 11.3502 - auc: 0.5161 - val_loss: 10.1750 - val_auc: 0.4992\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 9.1868 - auc: 0.4950 - val_loss: 8.1770 - val_auc: 0.4996\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 7.3475 - auc: 0.4956 - val_loss: 6.5025 - val_auc: 0.4971\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 5.8096 - auc: 0.5096 - val_loss: 5.0984 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 4.5258 - auc: 0.5205 - val_loss: 3.9411 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 3.4784 - auc: 0.4898 - val_loss: 3.0074 - val_auc: 0.4509\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 2.6395 - auc: 0.4873 - val_loss: 2.2652 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 1.9790 - auc: 0.5032 - val_loss: 1.6859 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 1.4723 - auc: 0.5167 - val_loss: 1.2508 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 1.0908 - auc: 0.5279 - val_loss: 0.9247 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.8080 - auc: 0.4959 - val_loss: 0.6853 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.6049 - auc: 0.5337 - val_loss: 0.5134 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.4511 - auc: 0.5072 - val_loss: 0.3792 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.3355 - auc: 0.5265 - val_loss: 0.2809 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.2509 - auc: 0.4613 - val_loss: 0.2098 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1899 - auc: 0.5000 - val_loss: 0.1589 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1489 - auc: 0.4550 - val_loss: 0.1278 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1250 - auc: 0.5000 - val_loss: 0.1107 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1138 - auc: 0.5070 - val_loss: 0.1045 - val_auc: 0.4996\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1102 - auc: 0.4999 - val_loss: 0.1026 - val_auc: 0.5000\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.1091 - auc: 0.5000 - val_loss: 0.1018 - val_auc: 0.5000\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1086 - auc: 0.5000 - val_loss: 0.1011 - val_auc: 0.5000\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1081 - auc: 0.5488 - val_loss: 0.1008 - val_auc: 0.5000\n",
      "Epoch 25/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.1080 - auc: 0.5000Restoring model weights from the end of the best epoch: 5.\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1078 - auc: 0.5000 - val_loss: 0.1004 - val_auc: 0.5000\n",
      "Epoch 25: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 5\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 5.5e-03\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-04\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 13s 115ms/step - loss: 17.1784 - auc: 0.5149 - val_loss: 2.6418 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 50ms/step - loss: 1.2406 - auc: 0.4929 - val_loss: 0.6687 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.6204 - auc: 0.4670 - val_loss: 0.5763 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.6000 - auc: 0.4951 - val_loss: 0.5889 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.5981 - auc: 0.5304 - val_loss: 0.5842 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.5971 - auc: 0.5538 - val_loss: 0.5978 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 50ms/step - loss: 0.5962 - auc: 0.5937 - val_loss: 0.5836 - val_auc: 0.5955\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 3s 67ms/step - loss: 0.5957 - auc: 0.6083 - val_loss: 0.5899 - val_auc: 0.7792\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.5929 - auc: 0.6971 - val_loss: 0.5880 - val_auc: 0.7824\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.5874 - auc: 0.7525 - val_loss: 0.5912 - val_auc: 0.7298\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.5830 - auc: 0.7926 - val_loss: 0.5871 - val_auc: 0.7866\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.5826 - auc: 0.8229 - val_loss: 0.5747 - val_auc: 0.8010\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.5779 - auc: 0.8267 - val_loss: 0.5689 - val_auc: 0.7974\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.5765 - auc: 0.8434 - val_loss: 0.5912 - val_auc: 0.8185\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.5826 - auc: 0.7860 - val_loss: 0.5574 - val_auc: 0.8079\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.5765 - auc: 0.8214 - val_loss: 0.5793 - val_auc: 0.8108\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.5735 - auc: 0.8509 - val_loss: 0.5711 - val_auc: 0.8022\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.5727 - auc: 0.8340 - val_loss: 0.5811 - val_auc: 0.8114\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.5708 - auc: 0.8645 - val_loss: 0.5837 - val_auc: 0.7413\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.5720 - auc: 0.8451 - val_loss: 0.5669 - val_auc: 0.8026\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.5733 - auc: 0.8442 - val_loss: 0.5787 - val_auc: 0.7809\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.5723 - auc: 0.8632 - val_loss: 0.5733 - val_auc: 0.8034\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.5718 - auc: 0.8525 - val_loss: 0.5613 - val_auc: 0.7989\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.5731 - auc: 0.8377 - val_loss: 0.5833 - val_auc: 0.8090\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.5694 - auc: 0.8643 - val_loss: 0.5799 - val_auc: 0.8047\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.5682 - auc: 0.8605 - val_loss: 0.5882 - val_auc: 0.7351\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 2s 50ms/step - loss: 0.5686 - auc: 0.8664 - val_loss: 0.5788 - val_auc: 0.7492\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.5729 - auc: 0.8526 - val_loss: 0.5753 - val_auc: 0.8013\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.5684 - auc: 0.8712 - val_loss: 0.5756 - val_auc: 0.7689\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.5650 - auc: 0.8738 - val_loss: 0.5822 - val_auc: 0.7883\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.5679 - auc: 0.8650 - val_loss: 0.5754 - val_auc: 0.7189\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.5679 - auc: 0.8757 - val_loss: 0.5809 - val_auc: 0.7335\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.5671 - auc: 0.8558 - val_loss: 0.5665 - val_auc: 0.7621\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.5694 - auc: 0.8597Restoring model weights from the end of the best epoch: 14.\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.5694 - auc: 0.8597 - val_loss: 0.5805 - val_auc: 0.8131\n",
      "Epoch 34: early stopping\n",
      "\n",
      "Validation AUC: 81.31%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 5\n",
      "num_dense_nodes: 5\n",
      "activation_dense: relu\n",
      "learning_rate: 4.2e-04\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 8s 68ms/step - loss: 24.3668 - auc: 0.4751 - val_loss: 21.5368 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 19.1412 - auc: 0.5245 - val_loss: 16.6703 - val_auc: 0.4926\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 49ms/step - loss: 14.6100 - auc: 0.4666 - val_loss: 12.5271 - val_auc: 0.4262\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 49ms/step - loss: 10.8696 - auc: 0.4604 - val_loss: 9.2125 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 7.9296 - auc: 0.5248 - val_loss: 6.6490 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 5.6686 - auc: 0.4648 - val_loss: 4.7028 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 3.9815 - auc: 0.4817 - val_loss: 3.2613 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 2.7366 - auc: 0.5000 - val_loss: 2.2126 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 1.8350 - auc: 0.5000 - val_loss: 1.4576 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 1.1982 - auc: 0.5001 - val_loss: 0.9358 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 50ms/step - loss: 0.7640 - auc: 0.5000 - val_loss: 0.5911 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 49ms/step - loss: 0.4884 - auc: 0.4431 - val_loss: 0.3781 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.3149 - auc: 0.5000 - val_loss: 0.2427 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.2107 - auc: 0.5351 - val_loss: 0.1694 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.1617 - auc: 0.4976 - val_loss: 0.1396 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.1384 - auc: 0.5469 - val_loss: 0.1225 - val_auc: 0.6600\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.1263 - auc: 0.6618 - val_loss: 0.1142 - val_auc: 0.7120\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.1200 - auc: 0.5901 - val_loss: 0.1092 - val_auc: 0.5897\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.1169 - auc: 0.6241 - val_loss: 0.1086 - val_auc: 0.6346\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.1164 - auc: 0.6992 - val_loss: 0.1085 - val_auc: 0.7608\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.1163 - auc: 0.6441 - val_loss: 0.1088 - val_auc: 0.5690\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.1162 - auc: 0.6720 - val_loss: 0.1083 - val_auc: 0.6519\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.1159 - auc: 0.7046 - val_loss: 0.1082 - val_auc: 0.6546\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.1158 - auc: 0.6107 - val_loss: 0.1081 - val_auc: 0.7575\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.1154 - auc: 0.7165 - val_loss: 0.1080 - val_auc: 0.7431\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.1151 - auc: 0.7862 - val_loss: 0.1074 - val_auc: 0.7563\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.1148 - auc: 0.8068 - val_loss: 0.1072 - val_auc: 0.7695\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.1143 - auc: 0.7737 - val_loss: 0.1069 - val_auc: 0.7693\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.1139 - auc: 0.8016 - val_loss: 0.1068 - val_auc: 0.7611\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.1129 - auc: 0.8202 - val_loss: 0.1057 - val_auc: 0.7884\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.1119 - auc: 0.8374 - val_loss: 0.1052 - val_auc: 0.7976\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 2s 49ms/step - loss: 0.1102 - auc: 0.8576 - val_loss: 0.1039 - val_auc: 0.7965\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.1076 - auc: 0.8683 - val_loss: 0.1025 - val_auc: 0.7971\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.1058 - auc: 0.8755 - val_loss: 0.1012 - val_auc: 0.7995\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.1030 - auc: 0.8745 - val_loss: 0.1006 - val_auc: 0.8198\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 2s 49ms/step - loss: 0.0996 - auc: 0.8733 - val_loss: 0.0986 - val_auc: 0.8182\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.0972 - auc: 0.8807 - val_loss: 0.0983 - val_auc: 0.7862\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.0944 - auc: 0.8783 - val_loss: 0.0964 - val_auc: 0.8017\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.0928 - auc: 0.8834 - val_loss: 0.0983 - val_auc: 0.8165\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 2s 49ms/step - loss: 0.0905 - auc: 0.8944 - val_loss: 0.0976 - val_auc: 0.8417\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.0866 - auc: 0.8996 - val_loss: 0.0955 - val_auc: 0.8135\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.0859 - auc: 0.9024 - val_loss: 0.0955 - val_auc: 0.8210\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.0835 - auc: 0.9073 - val_loss: 0.0957 - val_auc: 0.8117\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 2s 50ms/step - loss: 0.0828 - auc: 0.9118 - val_loss: 0.0962 - val_auc: 0.8289\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 2s 50ms/step - loss: 0.0831 - auc: 0.9146 - val_loss: 0.0959 - val_auc: 0.8291\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.0799 - auc: 0.9270 - val_loss: 0.0966 - val_auc: 0.8308\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.0808 - auc: 0.9188 - val_loss: 0.0967 - val_auc: 0.8297\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.0796 - auc: 0.9136 - val_loss: 0.0991 - val_auc: 0.8185\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.0779 - auc: 0.9207 - val_loss: 0.0983 - val_auc: 0.8354\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.0777 - auc: 0.9231 - val_loss: 0.0984 - val_auc: 0.8349\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.0775 - auc: 0.9236 - val_loss: 0.0993 - val_auc: 0.8156\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.0760 - auc: 0.9350 - val_loss: 0.0994 - val_auc: 0.8270\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 2s 49ms/step - loss: 0.0744 - auc: 0.9389 - val_loss: 0.1013 - val_auc: 0.7964\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.0768 - auc: 0.9182 - val_loss: 0.1035 - val_auc: 0.7978\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.0747 - auc: 0.9282 - val_loss: 0.1023 - val_auc: 0.7952\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.0751 - auc: 0.9248 - val_loss: 0.1044 - val_auc: 0.7998\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 2s 49ms/step - loss: 0.0763 - auc: 0.9257 - val_loss: 0.1048 - val_auc: 0.7998\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.0744 - auc: 0.9247 - val_loss: 0.1025 - val_auc: 0.8052\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 2s 49ms/step - loss: 0.0737 - auc: 0.9417 - val_loss: 0.1041 - val_auc: 0.7895\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0710 - auc: 0.9461Restoring model weights from the end of the best epoch: 40.\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0710 - auc: 0.9461 - val_loss: 0.1061 - val_auc: 0.7931\n",
      "Epoch 60: early stopping\n",
      "\n",
      "Validation AUC: 79.31%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 5\n",
      "activation_dense: relu\n",
      "learning_rate: 1.3e-04\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 9s 71ms/step - loss: 28.7482 - auc: 0.5126 - val_loss: 27.9147 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 27.1611 - auc: 0.4660 - val_loss: 26.3565 - val_auc: 0.4687\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 25.6296 - auc: 0.5324 - val_loss: 24.8537 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 24.1562 - auc: 0.4928 - val_loss: 23.4117 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 22.7442 - auc: 0.5024 - val_loss: 22.0314 - val_auc: 0.4396\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 21.3934 - auc: 0.5646 - val_loss: 20.7107 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 20.0998 - auc: 0.4833 - val_loss: 19.4444 - val_auc: 0.4039\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 18.8612 - auc: 0.5054 - val_loss: 18.2354 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 17.6777 - auc: 0.4989 - val_loss: 17.0767 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 16.5457 - auc: 0.5159 - val_loss: 15.9747 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 15.4678 - auc: 0.5191 - val_loss: 14.9192 - val_auc: 0.4996\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 14.4345 - auc: 0.5404 - val_loss: 13.9098 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 13.4491 - auc: 0.5247 - val_loss: 12.9514 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 12.5189 - auc: 0.4486 - val_loss: 12.0497 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 11.6400 - auc: 0.5197 - val_loss: 11.1953 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 10.8128 - auc: 0.4981 - val_loss: 10.3968 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 10.0355 - auc: 0.4957 - val_loss: 9.6397 - val_auc: 0.4202\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 9.2973 - auc: 0.5018 - val_loss: 8.9222 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 8.6000 - auc: 0.5332 - val_loss: 8.2472 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 7.9462 - auc: 0.5060 - val_loss: 7.6154 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 7.3334 - auc: 0.4798Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 7.3334 - auc: 0.4798 - val_loss: 7.0222 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 5\n",
      "dropout_rate: 0.0\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 5\n",
      "activation_dense: relu\n",
      "learning_rate: 2.3e-03\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 4s 41ms/step - loss: 0.6734 - auc: 0.5000 - val_loss: 0.6517 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.6331 - auc: 0.5000 - val_loss: 0.6130 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.5960 - auc: 0.5000 - val_loss: 0.5770 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.5615 - auc: 0.5000 - val_loss: 0.5437 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.5294 - auc: 0.5000 - val_loss: 0.5128 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.4997 - auc: 0.5000 - val_loss: 0.4840 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.4721 - auc: 0.5000 - val_loss: 0.4574 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.4466 - auc: 0.5000 - val_loss: 0.4327 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.4229 - auc: 0.5000 - val_loss: 0.4098 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.4011 - auc: 0.5000 - val_loss: 0.3887 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.3808 - auc: 0.5000 - val_loss: 0.3691 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.3621 - auc: 0.5000 - val_loss: 0.3510 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.3447 - auc: 0.5000 - val_loss: 0.3341 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.3287 - auc: 0.5000 - val_loss: 0.3185 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.3138 - auc: 0.5000 - val_loss: 0.3041 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.3000 - auc: 0.5000 - val_loss: 0.2908 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.2872 - auc: 0.5000 - val_loss: 0.2783 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.2753 - auc: 0.5000 - val_loss: 0.2668 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.2643 - auc: 0.5000 - val_loss: 0.2560 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.2540 - auc: 0.5000 - val_loss: 0.2461 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.2455 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.2445 - auc: 0.5000 - val_loss: 0.2368 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 4\n",
      "num_dense_nodes: 5\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 6.5e-04\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 9s 77ms/step - loss: 13.0389 - auc: 0.5084 - val_loss: 11.1610 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 9.6287 - auc: 0.5173 - val_loss: 8.0982 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 6.8848 - auc: 0.4919 - val_loss: 5.6858 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 4.7745 - auc: 0.4697 - val_loss: 3.8920 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 3.2351 - auc: 0.5055 - val_loss: 2.6089 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 2.1648 - auc: 0.5168 - val_loss: 1.7313 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 1.4306 - auc: 0.5070 - val_loss: 1.1418 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.9468 - auc: 0.5418 - val_loss: 0.7609 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.6378 - auc: 0.4859 - val_loss: 0.5180 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.4427 - auc: 0.4959 - val_loss: 0.3658 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.3148 - auc: 0.4857 - val_loss: 0.2606 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.2358 - auc: 0.5094 - val_loss: 0.2067 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1951 - auc: 0.5055 - val_loss: 0.1794 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1773 - auc: 0.4617 - val_loss: 0.1676 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1680 - auc: 0.4716 - val_loss: 0.1590 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1605 - auc: 0.4859 - val_loss: 0.1523 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1541 - auc: 0.5525 - val_loss: 0.1467 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1487 - auc: 0.5416 - val_loss: 0.1413 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1440 - auc: 0.5014 - val_loss: 0.1372 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1400 - auc: 0.5227 - val_loss: 0.1324 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1365 - auc: 0.5222Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1365 - auc: 0.5222 - val_loss: 0.1291 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.5\n",
      "num_dense_layers: 4\n",
      "num_dense_nodes: 12\n",
      "activation_dense: relu\n",
      "learning_rate: 1.1e-03\n",
      "l1_lambda: 1.0e-05\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 5s 41ms/step - loss: 0.5818 - auc: 0.4872 - val_loss: 0.4661 - val_auc: 0.4699\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.3464 - auc: 0.5146 - val_loss: 0.2162 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.1503 - auc: 0.5313 - val_loss: 0.1030 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.1009 - auc: 0.5377 - val_loss: 0.0905 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0979 - auc: 0.4466 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0978 - auc: 0.5037 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0977 - auc: 0.4426 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0977 - auc: 0.4990 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0977 - auc: 0.4564 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0978 - auc: 0.4662 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0977 - auc: 0.4640 - val_loss: 0.0896 - val_auc: 0.4996\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.0977 - auc: 0.4816 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0978 - auc: 0.5138 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0979 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0975 - auc: 0.5012 - val_loss: 0.0894 - val_auc: 0.4996\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0971 - auc: 0.6074 - val_loss: 0.0882 - val_auc: 0.7296\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0947 - auc: 0.7552 - val_loss: 0.0867 - val_auc: 0.7606\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0923 - auc: 0.8365 - val_loss: 0.0853 - val_auc: 0.7836\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0893 - auc: 0.8592 - val_loss: 0.0835 - val_auc: 0.7886\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0847 - auc: 0.8648 - val_loss: 0.0804 - val_auc: 0.7869\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0791 - auc: 0.8876 - val_loss: 0.0779 - val_auc: 0.7960\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0721 - auc: 0.8866 - val_loss: 0.0766 - val_auc: 0.7933\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0642 - auc: 0.9180 - val_loss: 0.0779 - val_auc: 0.8156\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0584 - auc: 0.9234 - val_loss: 0.0793 - val_auc: 0.8351\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0518 - auc: 0.9389 - val_loss: 0.0826 - val_auc: 0.7956\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0487 - auc: 0.9429 - val_loss: 0.0905 - val_auc: 0.7541\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0465 - auc: 0.9355 - val_loss: 0.0954 - val_auc: 0.7215\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0414 - auc: 0.9523 - val_loss: 0.0945 - val_auc: 0.7910\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0379 - auc: 0.9517 - val_loss: 0.0983 - val_auc: 0.7872\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0351 - auc: 0.9530 - val_loss: 0.1007 - val_auc: 0.7361\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0327 - auc: 0.9511 - val_loss: 0.1097 - val_auc: 0.7628\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0274 - auc: 0.9675 - val_loss: 0.1109 - val_auc: 0.6894\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0253 - auc: 0.9741 - val_loss: 0.1098 - val_auc: 0.7120\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0253 - auc: 0.9706 - val_loss: 0.1139 - val_auc: 0.6551\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0221 - auc: 0.9773 - val_loss: 0.1165 - val_auc: 0.6939\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0178 - auc: 0.9753 - val_loss: 0.1333 - val_auc: 0.7016\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0142 - auc: 0.9769 - val_loss: 0.1653 - val_auc: 0.6962\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0179 - auc: 0.9813 - val_loss: 0.1385 - val_auc: 0.6545\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0124 - auc: 0.9777 - val_loss: 0.1472 - val_auc: 0.6542\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0118 - auc: 0.9825 - val_loss: 0.1412 - val_auc: 0.6418\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0100 - auc: 0.9831 - val_loss: 0.1513 - val_auc: 0.6560\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0084 - auc: 0.9885 - val_loss: 0.1537 - val_auc: 0.6387\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0075 - auc: 0.9889 - val_loss: 0.1412 - val_auc: 0.6059\n",
      "Epoch 45/1000\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.0075 - auc: 0.9884Restoring model weights from the end of the best epoch: 25.\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0073 - auc: 0.9892 - val_loss: 0.1569 - val_auc: 0.6244\n",
      "Epoch 45: early stopping\n",
      "\n",
      "Validation AUC: 62.44%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 22\n",
      "activation_dense: relu\n",
      "learning_rate: 2.9e-03\n",
      "l1_lambda: 1.0e-04\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 5s 49ms/step - loss: 0.1451 - auc: 0.5000 - val_loss: 0.0926 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0984 - auc: 0.5000 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0975 - auc: 0.5000 - val_loss: 0.0894 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0969 - auc: 0.5000 - val_loss: 0.0889 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0964 - auc: 0.5000 - val_loss: 0.0885 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0958 - auc: 0.5000 - val_loss: 0.0882 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0950 - auc: 0.5000 - val_loss: 0.0876 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0940 - auc: 0.5000 - val_loss: 0.0870 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0929 - auc: 0.5000 - val_loss: 0.0862 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0910 - auc: 0.5000 - val_loss: 0.0863 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0900 - auc: 0.5000 - val_loss: 0.0841 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0870 - auc: 0.5000 - val_loss: 0.0835 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0848 - auc: 0.5000 - val_loss: 0.0839 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0822 - auc: 0.5000 - val_loss: 0.0816 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0798 - auc: 0.5000 - val_loss: 0.0811 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0769 - auc: 0.5000 - val_loss: 0.0789 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0738 - auc: 0.5000 - val_loss: 0.0788 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0698 - auc: 0.5000 - val_loss: 0.0775 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0670 - auc: 0.5000 - val_loss: 0.0773 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0646 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0646 - auc: 0.5000 - val_loss: 0.0769 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 14\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 9.8e-05\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 9s 72ms/step - loss: 28.5199 - auc: 0.5126 - val_loss: 27.8790 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 27.2980 - auc: 0.5336 - val_loss: 26.6757 - val_auc: 0.4810\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 26.1136 - auc: 0.4741 - val_loss: 25.5115 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 24.9659 - auc: 0.4853 - val_loss: 24.3801 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 23.8501 - auc: 0.4727 - val_loss: 23.2821 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 22.7699 - auc: 0.4974 - val_loss: 22.2201 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 21.7255 - auc: 0.4540 - val_loss: 21.1941 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 20.7158 - auc: 0.4761 - val_loss: 20.2011 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 19.7386 - auc: 0.5278 - val_loss: 19.2409 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 18.7961 - auc: 0.4977 - val_loss: 18.3192 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 17.8921 - auc: 0.4394 - val_loss: 17.4320 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 17.0196 - auc: 0.5066 - val_loss: 16.5750 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 16.1758 - auc: 0.5075 - val_loss: 15.7453 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 15.3604 - auc: 0.4810 - val_loss: 14.9456 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 14.5762 - auc: 0.4704 - val_loss: 14.1768 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 13.8220 - auc: 0.4982 - val_loss: 13.4378 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 13.0962 - auc: 0.5015 - val_loss: 12.7271 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 12.3997 - auc: 0.4935 - val_loss: 12.0454 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 11.7320 - auc: 0.4823 - val_loss: 11.3924 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 11.0907 - auc: 0.4955 - val_loss: 10.7627 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 10.4775 - auc: 0.5309Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 10.4736 - auc: 0.5313 - val_loss: 10.1589 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 3\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.2\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 8\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 1.3e-03\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 17s 142ms/step - loss: 57.1478 - auc: 0.5053 - val_loss: 36.4695 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 61ms/step - loss: 24.0577 - auc: 0.4356 - val_loss: 13.6840 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 3s 64ms/step - loss: 8.3077 - auc: 0.4595 - val_loss: 4.1606 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 3s 66ms/step - loss: 2.3845 - auc: 0.4681 - val_loss: 1.1102 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 63ms/step - loss: 0.6808 - auc: 0.5380 - val_loss: 0.4018 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 3s 66ms/step - loss: 0.3351 - auc: 0.4683 - val_loss: 0.2805 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 0.2784 - auc: 0.4741 - val_loss: 0.2657 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 59ms/step - loss: 0.2742 - auc: 0.4620 - val_loss: 0.2654 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 3s 65ms/step - loss: 0.2734 - auc: 0.4711 - val_loss: 0.2683 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 3s 65ms/step - loss: 0.2741 - auc: 0.4415 - val_loss: 0.2662 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 3s 69ms/step - loss: 0.2734 - auc: 0.4868 - val_loss: 0.2638 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 3s 65ms/step - loss: 0.2731 - auc: 0.5126 - val_loss: 0.2627 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 63ms/step - loss: 0.2738 - auc: 0.4978 - val_loss: 0.2623 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 3s 65ms/step - loss: 0.2727 - auc: 0.5275 - val_loss: 0.2662 - val_auc: 0.7369\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 3s 67ms/step - loss: 0.2729 - auc: 0.5394 - val_loss: 0.2640 - val_auc: 0.7291\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 63ms/step - loss: 0.2719 - auc: 0.5786 - val_loss: 0.2631 - val_auc: 0.6389\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 62ms/step - loss: 0.2699 - auc: 0.7095 - val_loss: 0.2620 - val_auc: 0.7492\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 3s 66ms/step - loss: 0.2675 - auc: 0.7542 - val_loss: 0.2610 - val_auc: 0.6759\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 3s 68ms/step - loss: 0.2635 - auc: 0.7705 - val_loss: 0.2541 - val_auc: 0.8097\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 3s 66ms/step - loss: 0.2539 - auc: 0.8593 - val_loss: 0.2516 - val_auc: 0.8269\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 2s 64ms/step - loss: 0.2499 - auc: 0.8679 - val_loss: 0.2493 - val_auc: 0.8052\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 2s 64ms/step - loss: 0.2468 - auc: 0.8822 - val_loss: 0.2516 - val_auc: 0.8121\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 3s 68ms/step - loss: 0.2475 - auc: 0.8833 - val_loss: 0.2566 - val_auc: 0.8058\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 2s 59ms/step - loss: 0.2422 - auc: 0.8928 - val_loss: 0.2598 - val_auc: 0.7907\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 2s 61ms/step - loss: 0.2404 - auc: 0.9067 - val_loss: 0.2557 - val_auc: 0.7989\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 3s 66ms/step - loss: 0.2383 - auc: 0.9074 - val_loss: 0.2599 - val_auc: 0.8309\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 3s 65ms/step - loss: 0.2391 - auc: 0.8952 - val_loss: 0.2590 - val_auc: 0.7873\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 3s 65ms/step - loss: 0.2339 - auc: 0.9136 - val_loss: 0.2623 - val_auc: 0.7876\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 3s 67ms/step - loss: 0.2327 - auc: 0.9174 - val_loss: 0.2674 - val_auc: 0.7037\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 2s 64ms/step - loss: 0.2305 - auc: 0.9236 - val_loss: 0.2615 - val_auc: 0.8085\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 2s 64ms/step - loss: 0.2291 - auc: 0.9349 - val_loss: 0.2655 - val_auc: 0.7374\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 3s 65ms/step - loss: 0.2308 - auc: 0.9102 - val_loss: 0.2649 - val_auc: 0.7245\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 2s 63ms/step - loss: 0.2269 - auc: 0.9193 - val_loss: 0.2619 - val_auc: 0.7358\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 2s 64ms/step - loss: 0.2261 - auc: 0.9204 - val_loss: 0.2643 - val_auc: 0.7357\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 3s 64ms/step - loss: 0.2210 - auc: 0.9269 - val_loss: 0.2624 - val_auc: 0.7879\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 2s 63ms/step - loss: 0.2199 - auc: 0.9368 - val_loss: 0.2646 - val_auc: 0.7820\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 2s 59ms/step - loss: 0.2199 - auc: 0.9182 - val_loss: 0.2712 - val_auc: 0.7308\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 2s 65ms/step - loss: 0.2179 - auc: 0.9341 - val_loss: 0.2687 - val_auc: 0.6763\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 3s 66ms/step - loss: 0.2149 - auc: 0.9447 - val_loss: 0.2717 - val_auc: 0.6646\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 2s 62ms/step - loss: 0.2138 - auc: 0.9363 - val_loss: 0.2691 - val_auc: 0.6638\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 2s 61ms/step - loss: 0.2085 - auc: 0.9422 - val_loss: 0.2735 - val_auc: 0.6311\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 2s 61ms/step - loss: 0.2108 - auc: 0.9516 - val_loss: 0.2677 - val_auc: 0.6754\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 0.2097 - auc: 0.9488 - val_loss: 0.2698 - val_auc: 0.6656\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 2s 64ms/step - loss: 0.2059 - auc: 0.9584 - val_loss: 0.2768 - val_auc: 0.6519\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 2s 61ms/step - loss: 0.2088 - auc: 0.9478 - val_loss: 0.2782 - val_auc: 0.6113\n",
      "Epoch 46/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2111 - auc: 0.9520Restoring model weights from the end of the best epoch: 26.\n",
      "39/39 [==============================] - 2s 62ms/step - loss: 0.2107 - auc: 0.9522 - val_loss: 0.2678 - val_auc: 0.6585\n",
      "Epoch 46: early stopping\n",
      "\n",
      "Validation AUC: 65.85%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.5\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 6\n",
      "activation_dense: relu\n",
      "learning_rate: 1.4e-03\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-06\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 4s 51ms/step - loss: 0.3151 - auc: 0.5000 - val_loss: 0.1767 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1443 - auc: 0.5000 - val_loss: 0.1114 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1076 - auc: 0.5000 - val_loss: 0.0940 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0994 - auc: 0.5000 - val_loss: 0.0904 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0979 - auc: 0.5000 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0974 - auc: 0.5000 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0973 - auc: 0.5000 - val_loss: 0.0893 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0969 - auc: 0.5000 - val_loss: 0.0890 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0965 - auc: 0.5000 - val_loss: 0.0887 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0962 - auc: 0.5000 - val_loss: 0.0885 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0955 - auc: 0.5000 - val_loss: 0.0878 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0943 - auc: 0.5000 - val_loss: 0.0870 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0930 - auc: 0.5000 - val_loss: 0.0862 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0917 - auc: 0.5000 - val_loss: 0.0855 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0901 - auc: 0.5000 - val_loss: 0.0843 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0874 - auc: 0.5000 - val_loss: 0.0833 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0840 - auc: 0.5000 - val_loss: 0.0843 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0817 - auc: 0.5000 - val_loss: 0.0791 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0763 - auc: 0.5000 - val_loss: 0.0804 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0721 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0731 - auc: 0.5000 - val_loss: 0.0763 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.2\n",
      "num_dense_layers: 4\n",
      "num_dense_nodes: 5\n",
      "activation_dense: relu\n",
      "learning_rate: 2.4e-05\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-02\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 6s 56ms/step - loss: 0.6760 - auc: 0.3275 - val_loss: 0.6586 - val_auc: 0.3231\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.6438 - auc: 0.3878 - val_loss: 0.6270 - val_auc: 0.3724\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.6118 - auc: 0.3865 - val_loss: 0.5931 - val_auc: 0.3816\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.5741 - auc: 0.4319 - val_loss: 0.5499 - val_auc: 0.4247\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.5290 - auc: 0.4583 - val_loss: 0.5041 - val_auc: 0.4220\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.4861 - auc: 0.3808 - val_loss: 0.4643 - val_auc: 0.4143\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.4496 - auc: 0.4000 - val_loss: 0.4310 - val_auc: 0.4113\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.4191 - auc: 0.3984 - val_loss: 0.4033 - val_auc: 0.4230\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.3934 - auc: 0.3658 - val_loss: 0.3792 - val_auc: 0.3960\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.3709 - auc: 0.3865 - val_loss: 0.3581 - val_auc: 0.3844\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.3509 - auc: 0.3712 - val_loss: 0.3392 - val_auc: 0.3927\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.3332 - auc: 0.3744 - val_loss: 0.3225 - val_auc: 0.3797\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.3176 - auc: 0.3960 - val_loss: 0.3077 - val_auc: 0.3817\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.3037 - auc: 0.3688 - val_loss: 0.2944 - val_auc: 0.3858\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.2912 - auc: 0.4023 - val_loss: 0.2825 - val_auc: 0.3860\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.2799 - auc: 0.4093 - val_loss: 0.2716 - val_auc: 0.3738\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.2695 - auc: 0.3747 - val_loss: 0.2616 - val_auc: 0.3840\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.2600 - auc: 0.4259 - val_loss: 0.2524 - val_auc: 0.4050\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.2512 - auc: 0.4066 - val_loss: 0.2438 - val_auc: 0.3954\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 31ms/step - loss: 0.2429 - auc: 0.4303 - val_loss: 0.2358 - val_auc: 0.4010\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.2352 - auc: 0.4381 - val_loss: 0.2282 - val_auc: 0.3770\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.2279 - auc: 0.4521 - val_loss: 0.2210 - val_auc: 0.4183\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.2210 - auc: 0.3971 - val_loss: 0.2142 - val_auc: 0.3692\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2145 - auc: 0.4381Restoring model weights from the end of the best epoch: 4.\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.2145 - auc: 0.4381 - val_loss: 0.2079 - val_auc: 0.3957\n",
      "Epoch 24: early stopping\n",
      "\n",
      "Validation AUC: 39.57%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.5\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 5\n",
      "activation_dense: relu\n",
      "learning_rate: 6.9e-03\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 4s 40ms/step - loss: 0.1791 - auc: 0.4329 - val_loss: 0.0974 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0988 - auc: 0.5194 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0976 - auc: 0.5123 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0977 - auc: 0.4560 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0977 - auc: 0.4461 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0977 - auc: 0.4479 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0978 - auc: 0.4465 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0978 - auc: 0.4726 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0978 - auc: 0.4494 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0980 - auc: 0.4404 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0978 - auc: 0.4901 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0981 - auc: 0.4834 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.0977 - auc: 0.4749 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0976 - auc: 0.5128 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0981 - auc: 0.4814 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0979 - auc: 0.4565 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0982 - auc: 0.4640 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0975 - auc: 0.4855Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0981 - auc: 0.4849 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 5\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 4\n",
      "num_dense_nodes: 26\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 8.1e-02\n",
      "l1_lambda: 1.0e-06\n",
      "l2_lambda: 1.0e-04\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 10s 68ms/step - loss: 0.1523 - auc: 0.4840 - val_loss: 0.0909 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0990 - auc: 0.4767 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0980 - auc: 0.4754 - val_loss: 0.0902 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0988 - auc: 0.4477 - val_loss: 0.0902 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0982 - auc: 0.4626 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0984 - auc: 0.5059 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0984 - auc: 0.4955 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0982 - auc: 0.5010 - val_loss: 0.0908 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0983 - auc: 0.4957 - val_loss: 0.0908 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0978 - auc: 0.5527 - val_loss: 0.0942 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0985 - auc: 0.4980 - val_loss: 0.0901 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0981 - auc: 0.4938 - val_loss: 0.0912 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0984 - auc: 0.4942 - val_loss: 0.0904 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0984 - auc: 0.5005 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0981 - auc: 0.4782 - val_loss: 0.0927 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1004 - auc: 0.4750 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0986 - auc: 0.4671 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0989 - auc: 0.5572 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0991 - auc: 0.4918 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0982 - auc: 0.5008 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0966 - auc: 0.4887Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0989 - auc: 0.4857 - val_loss: 0.0896 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.1\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 5\n",
      "activation_dense: relu\n",
      "learning_rate: 2.1e-03\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-06\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 4s 52ms/step - loss: 0.1488 - auc: 0.4724 - val_loss: 0.0901 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0978 - auc: 0.4658 - val_loss: 0.0895 - val_auc: 0.5976\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0974 - auc: 0.5777 - val_loss: 0.0892 - val_auc: 0.5347\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0969 - auc: 0.5308 - val_loss: 0.0888 - val_auc: 0.6853\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0958 - auc: 0.7171 - val_loss: 0.0879 - val_auc: 0.7352\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0947 - auc: 0.7648 - val_loss: 0.0869 - val_auc: 0.7422\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0930 - auc: 0.8251 - val_loss: 0.0860 - val_auc: 0.7731\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0905 - auc: 0.8684 - val_loss: 0.0841 - val_auc: 0.8100\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0874 - auc: 0.8965 - val_loss: 0.0826 - val_auc: 0.8053\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0839 - auc: 0.9181 - val_loss: 0.0836 - val_auc: 0.7948\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0811 - auc: 0.9083 - val_loss: 0.0790 - val_auc: 0.8045\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0762 - auc: 0.9314 - val_loss: 0.0780 - val_auc: 0.8041\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0732 - auc: 0.9354 - val_loss: 0.0767 - val_auc: 0.8344\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0677 - auc: 0.9480 - val_loss: 0.0764 - val_auc: 0.8301\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0634 - auc: 0.9503 - val_loss: 0.0779 - val_auc: 0.8177\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0581 - auc: 0.9612 - val_loss: 0.0775 - val_auc: 0.8199\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0536 - auc: 0.9696 - val_loss: 0.0797 - val_auc: 0.8223\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0494 - auc: 0.9721 - val_loss: 0.0823 - val_auc: 0.8122\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0454 - auc: 0.9793 - val_loss: 0.0939 - val_auc: 0.6964\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0422 - auc: 0.9768 - val_loss: 0.0947 - val_auc: 0.6959\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0382 - auc: 0.9783 - val_loss: 0.0929 - val_auc: 0.7175\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0365 - auc: 0.9795 - val_loss: 0.0968 - val_auc: 0.7037\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0332 - auc: 0.9855 - val_loss: 0.1006 - val_auc: 0.7044\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0297 - auc: 0.9880 - val_loss: 0.1033 - val_auc: 0.6671\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0279 - auc: 0.9892 - val_loss: 0.1075 - val_auc: 0.6683\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0238 - auc: 0.9957 - val_loss: 0.1132 - val_auc: 0.7021\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0225 - auc: 0.9966 - val_loss: 0.1158 - val_auc: 0.6657\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0206 - auc: 0.9970 - val_loss: 0.1194 - val_auc: 0.6654\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0195 - auc: 0.9976 - val_loss: 0.1226 - val_auc: 0.6601\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0159 - auc: 0.9986 - val_loss: 0.1236 - val_auc: 0.6572\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0149 - auc: 0.9986 - val_loss: 0.1322 - val_auc: 0.6518\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0121 - auc: 0.9993 - val_loss: 0.1351 - val_auc: 0.6561\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0102 - auc: 0.9995Restoring model weights from the end of the best epoch: 13.\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0102 - auc: 0.9995 - val_loss: 0.1422 - val_auc: 0.6557\n",
      "Epoch 33: early stopping\n",
      "\n",
      "Validation AUC: 65.57%\n",
      "\n",
      "num_lstm_layers: 3\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.3\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 5\n",
      "activation_dense: relu\n",
      "learning_rate: 8.3e-05\n",
      "l1_lambda: 1.0e-06\n",
      "l2_lambda: 1.0e-02\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 11s 85ms/step - loss: 1.5338 - auc: 0.4884 - val_loss: 1.4938 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 1.4584 - auc: 0.5167 - val_loss: 1.4202 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 1.3866 - auc: 0.4615 - val_loss: 1.3500 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 1.3181 - auc: 0.4630 - val_loss: 1.2829 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 1.2524 - auc: 0.4910 - val_loss: 1.2186 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 1.1921 - auc: 0.4960 - val_loss: 1.1621 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 1.1369 - auc: 0.5128 - val_loss: 1.1080 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 1.0837 - auc: 0.5047 - val_loss: 1.0556 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 59ms/step - loss: 1.0322 - auc: 0.5081 - val_loss: 1.0046 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.9820 - auc: 0.4969 - val_loss: 0.9548 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.9330 - auc: 0.5382 - val_loss: 0.9063 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.8854 - auc: 0.4826 - val_loss: 0.8591 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 0.8391 - auc: 0.4732 - val_loss: 0.8134 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.7945 - auc: 0.5022 - val_loss: 0.7695 - val_auc: 0.4791\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.7518 - auc: 0.4986 - val_loss: 0.7274 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.7110 - auc: 0.5189 - val_loss: 0.6875 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 0.6724 - auc: 0.5538 - val_loss: 0.6500 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.6361 - auc: 0.4873 - val_loss: 0.6146 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 0.6021 - auc: 0.5016 - val_loss: 0.5816 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.5704 - auc: 0.4475 - val_loss: 0.5509 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.5408 - auc: 0.4748Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 0.5409 - auc: 0.4760 - val_loss: 0.5223 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 3\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 4\n",
      "num_dense_nodes: 5\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 4.1e-04\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-03\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 16s 128ms/step - loss: 71.7991 - auc: 0.5037 - val_loss: 63.6976 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 56.9246 - auc: 0.4845 - val_loss: 50.0145 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 64ms/step - loss: 44.3166 - auc: 0.4623 - val_loss: 38.5510 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 3s 67ms/step - loss: 33.8898 - auc: 0.4865 - val_loss: 29.2129 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 59ms/step - loss: 25.4599 - auc: 0.5278 - val_loss: 21.6929 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 3s 65ms/step - loss: 18.7169 - auc: 0.4588 - val_loss: 15.7560 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 63ms/step - loss: 13.4730 - auc: 0.5183 - val_loss: 11.2294 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 3s 65ms/step - loss: 9.5078 - auc: 0.4976 - val_loss: 7.8282 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 59ms/step - loss: 6.5596 - auc: 0.5008 - val_loss: 5.3371 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 3s 66ms/step - loss: 4.4533 - auc: 0.5330 - val_loss: 3.6090 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 3s 65ms/step - loss: 2.9975 - auc: 0.4815 - val_loss: 2.4106 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 64ms/step - loss: 2.0043 - auc: 0.5436 - val_loss: 1.6247 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 63ms/step - loss: 1.3563 - auc: 0.5042 - val_loss: 1.1014 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 3s 65ms/step - loss: 0.9335 - auc: 0.4923 - val_loss: 0.7739 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 3s 68ms/step - loss: 0.6692 - auc: 0.4927 - val_loss: 0.5702 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 3s 67ms/step - loss: 0.5126 - auc: 0.4885 - val_loss: 0.4558 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 3s 64ms/step - loss: 0.4222 - auc: 0.5192 - val_loss: 0.3846 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 3s 67ms/step - loss: 0.3644 - auc: 0.5384 - val_loss: 0.3398 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 3s 68ms/step - loss: 0.3274 - auc: 0.5312 - val_loss: 0.3097 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 3s 66ms/step - loss: 0.3024 - auc: 0.5373 - val_loss: 0.2907 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2867 - auc: 0.5010Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 3s 68ms/step - loss: 0.2867 - auc: 0.5010 - val_loss: 0.2768 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 3\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 5\n",
      "dropout_rate: 0.1\n",
      "num_dense_layers: 5\n",
      "num_dense_nodes: 10\n",
      "activation_dense: relu\n",
      "learning_rate: 1.7e-02\n",
      "l1_lambda: 1.0e-06\n",
      "l2_lambda: 1.0e-01\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 12s 85ms/step - loss: 0.5265 - auc: 0.5000 - val_loss: 0.1029 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1030 - auc: 0.5000 - val_loss: 0.0904 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0984 - auc: 0.5000 - val_loss: 0.0923 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0986 - auc: 0.5000 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0994 - auc: 0.5000 - val_loss: 0.0901 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0985 - auc: 0.5000 - val_loss: 0.0909 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0981 - auc: 0.5000 - val_loss: 0.0911 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1014 - auc: 0.5000 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0982 - auc: 0.5000 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0978 - auc: 0.5000 - val_loss: 0.0904 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0991 - auc: 0.5000 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0988 - auc: 0.5000 - val_loss: 0.0904 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0980 - auc: 0.5000 - val_loss: 0.0910 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0989 - auc: 0.5000 - val_loss: 0.0909 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0992 - auc: 0.5000 - val_loss: 0.0917 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1001 - auc: 0.5000 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0991 - auc: 0.5000 - val_loss: 0.0912 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0987 - auc: 0.5000 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0994 - auc: 0.5000 - val_loss: 0.0904 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0994 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0981 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0981 - auc: 0.5000 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 5\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 5.8e-05\n",
      "l1_lambda: 1.0e-04\n",
      "l2_lambda: 1.0e-04\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 12s 107ms/step - loss: 0.7308 - auc: 0.5000 - val_loss: 0.7022 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.6786 - auc: 0.5000 - val_loss: 0.6324 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.6056 - auc: 0.5000 - val_loss: 0.5437 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.5195 - auc: 0.5000 - val_loss: 0.4490 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 62ms/step - loss: 0.4320 - auc: 0.5000 - val_loss: 0.3611 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 61ms/step - loss: 0.3542 - auc: 0.5000 - val_loss: 0.2889 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 0.2891 - auc: 0.5000 - val_loss: 0.2352 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.2413 - auc: 0.5000 - val_loss: 0.1985 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.2086 - auc: 0.5000 - val_loss: 0.1744 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 0.1862 - auc: 0.5000 - val_loss: 0.1591 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.1706 - auc: 0.5000 - val_loss: 0.1490 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.1611 - auc: 0.5000 - val_loss: 0.1423 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 60ms/step - loss: 0.1540 - auc: 0.5000 - val_loss: 0.1376 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.1479 - auc: 0.5000 - val_loss: 0.1342 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 60ms/step - loss: 0.1450 - auc: 0.5000 - val_loss: 0.1315 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 0.1419 - auc: 0.5000 - val_loss: 0.1295 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.1396 - auc: 0.5000 - val_loss: 0.1277 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.1371 - auc: 0.5000 - val_loss: 0.1262 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 0.1361 - auc: 0.5000 - val_loss: 0.1249 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 0.1349 - auc: 0.5000 - val_loss: 0.1236 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.1334 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.1324 - auc: 0.5000 - val_loss: 0.1225 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 3\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 4\n",
      "num_dense_nodes: 14\n",
      "activation_dense: relu\n",
      "learning_rate: 1.7e-04\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 12s 89ms/step - loss: 76.0598 - auc: 0.5028 - val_loss: 72.5490 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 69.4226 - auc: 0.4860 - val_loss: 66.1127 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 63ms/step - loss: 63.1712 - auc: 0.5014 - val_loss: 60.0575 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 60ms/step - loss: 57.2909 - auc: 0.4655 - val_loss: 54.3667 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 51.7820 - auc: 0.5046 - val_loss: 49.0522 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 60ms/step - loss: 46.6511 - auc: 0.5087 - val_loss: 44.1202 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 41.9046 - auc: 0.4966 - val_loss: 39.5668 - val_auc: 0.5004\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 59ms/step - loss: 37.5248 - auc: 0.4567 - val_loss: 35.3678 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 63ms/step - loss: 33.4878 - auc: 0.5507 - val_loss: 31.5079 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 62ms/step - loss: 29.7959 - auc: 0.4802 - val_loss: 27.9864 - val_auc: 0.4975\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 60ms/step - loss: 26.4285 - auc: 0.5247 - val_loss: 24.7883 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 23.3742 - auc: 0.5253 - val_loss: 21.8806 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 59ms/step - loss: 20.6002 - auc: 0.4499 - val_loss: 19.2488 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 59ms/step - loss: 18.0977 - auc: 0.5024 - val_loss: 16.8787 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 63ms/step - loss: 15.8377 - auc: 0.5180 - val_loss: 14.7346 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 13.7984 - auc: 0.4974 - val_loss: 12.8088 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 11.9760 - auc: 0.4816 - val_loss: 11.0970 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 59ms/step - loss: 10.3583 - auc: 0.4992 - val_loss: 9.5752 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 60ms/step - loss: 8.9145 - auc: 0.5109 - val_loss: 8.2132 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 7.6378 - auc: 0.4634 - val_loss: 7.0303 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 6.5314 - auc: 0.4700 - val_loss: 6.0026 - val_auc: 0.5000\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 2s 59ms/step - loss: 5.5714 - auc: 0.4570 - val_loss: 5.1111 - val_auc: 0.5000\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 4.7406 - auc: 0.5066 - val_loss: 4.3434 - val_auc: 0.5000\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 4.0256 - auc: 0.5036 - val_loss: 3.6810 - val_auc: 0.5000\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 2s 60ms/step - loss: 3.4092 - auc: 0.4757 - val_loss: 3.1133 - val_auc: 0.5000\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 2.8812 - auc: 0.5039 - val_loss: 2.6260 - val_auc: 0.5000\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 2.4261 - auc: 0.4841Restoring model weights from the end of the best epoch: 7.\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 2.4261 - auc: 0.4841 - val_loss: 2.2057 - val_auc: 0.5000\n",
      "Epoch 27: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 3\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 19\n",
      "activation_dense: relu\n",
      "learning_rate: 7.7e-05\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 14s 109ms/step - loss: 15.7466 - auc: 0.4978 - val_loss: 15.5556 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 15.3807 - auc: 0.5202 - val_loss: 15.1914 - val_auc: 0.4983\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 50ms/step - loss: 15.0181 - auc: 0.4603 - val_loss: 14.8292 - val_auc: 0.4785\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 14.6588 - auc: 0.4897 - val_loss: 14.4717 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 14.3045 - auc: 0.5276 - val_loss: 14.1185 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 13.9535 - auc: 0.4574 - val_loss: 13.7680 - val_auc: 0.4453\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 13.6059 - auc: 0.5024 - val_loss: 13.4207 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 13.2614 - auc: 0.5067 - val_loss: 13.0761 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 12.9190 - auc: 0.5114 - val_loss: 12.7336 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 12.5791 - auc: 0.4810 - val_loss: 12.3911 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 12.2404 - auc: 0.4664 - val_loss: 12.0495 - val_auc: 0.4773\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 11.9005 - auc: 0.4758 - val_loss: 11.7093 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 11.5661 - auc: 0.4711 - val_loss: 11.3711 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 11.2331 - auc: 0.4838 - val_loss: 11.0372 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 10.9033 - auc: 0.5106 - val_loss: 10.7091 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 10.5818 - auc: 0.4564 - val_loss: 10.3896 - val_auc: 0.4942\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 10.2693 - auc: 0.4959 - val_loss: 10.0803 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 9.9663 - auc: 0.4815 - val_loss: 9.7832 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 9.6727 - auc: 0.5543 - val_loss: 9.4973 - val_auc: 0.4996\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 9.3908 - auc: 0.5088 - val_loss: 9.2208 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 9.1180 - auc: 0.4580Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 9.1180 - auc: 0.4580 - val_loss: 8.9552 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.3\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 5\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 1.1e-02\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 13s 105ms/step - loss: 11.7423 - auc: 0.4371 - val_loss: 2.1233 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 1.4265 - auc: 0.4214 - val_loss: 1.1633 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 1.1359 - auc: 0.4825 - val_loss: 1.0723 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 60ms/step - loss: 1.1297 - auc: 0.5099 - val_loss: 1.1173 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 1.1269 - auc: 0.5000 - val_loss: 1.1187 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 1.1266 - auc: 0.4451 - val_loss: 1.1262 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 1.1288 - auc: 0.4797 - val_loss: 1.0826 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 1.1277 - auc: 0.4896 - val_loss: 1.1232 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 1.1261 - auc: 0.5000 - val_loss: 1.1261 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 59ms/step - loss: 1.1245 - auc: 0.5000 - val_loss: 1.1600 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 1.1279 - auc: 0.4882 - val_loss: 1.1382 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 1.1285 - auc: 0.4574 - val_loss: 1.1175 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 1.1256 - auc: 0.4579 - val_loss: 1.1031 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 1.1278 - auc: 0.4413 - val_loss: 1.1851 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 1.1290 - auc: 0.5000 - val_loss: 1.0578 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 1.1246 - auc: 0.4858 - val_loss: 1.1151 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 1.1272 - auc: 0.5038 - val_loss: 1.1230 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 1.1290 - auc: 0.5000 - val_loss: 1.1150 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 1.1256 - auc: 0.4692 - val_loss: 1.1176 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 59ms/step - loss: 1.1272 - auc: 0.4759 - val_loss: 1.1161 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 1.1290 - auc: 0.4539Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 1.1293 - auc: 0.4541 - val_loss: 1.1057 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.2\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 2\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 2.8e-03\n",
      "l1_lambda: 1.0e-05\n",
      "l2_lambda: 1.0e-04\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 12s 97ms/step - loss: 0.4420 - auc: 0.5463 - val_loss: 0.2683 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.2519 - auc: 0.4883 - val_loss: 0.2317 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.2220 - auc: 0.4725 - val_loss: 0.2057 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.1990 - auc: 0.5188 - val_loss: 0.1852 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.1807 - auc: 0.5087 - val_loss: 0.1686 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.1661 - auc: 0.5208 - val_loss: 0.1554 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.1544 - auc: 0.4851 - val_loss: 0.1446 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.1449 - auc: 0.5450 - val_loss: 0.1360 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.1372 - auc: 0.4971 - val_loss: 0.1287 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.1308 - auc: 0.4973 - val_loss: 0.1228 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.1256 - auc: 0.4697 - val_loss: 0.1178 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.1212 - auc: 0.5222 - val_loss: 0.1138 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.1176 - auc: 0.4748 - val_loss: 0.1103 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.1146 - auc: 0.4886 - val_loss: 0.1073 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.1120 - auc: 0.5000 - val_loss: 0.1049 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.1099 - auc: 0.4806 - val_loss: 0.1028 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.1081 - auc: 0.5000 - val_loss: 0.1010 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.1066 - auc: 0.5005 - val_loss: 0.0995 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.1053 - auc: 0.5000 - val_loss: 0.0982 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.1041 - auc: 0.5212 - val_loss: 0.0971 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.1020 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.1032 - auc: 0.5000 - val_loss: 0.0961 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 11\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 5.4e-03\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 13s 110ms/step - loss: 17.4106 - auc: 0.4937 - val_loss: 2.6444 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 1.2437 - auc: 0.5082 - val_loss: 0.6724 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 60ms/step - loss: 0.6181 - auc: 0.5115 - val_loss: 0.5750 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 62ms/step - loss: 0.5932 - auc: 0.5038 - val_loss: 0.5836 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.5935 - auc: 0.5000 - val_loss: 0.5843 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.5928 - auc: 0.5000 - val_loss: 0.5898 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 0.5919 - auc: 0.5000 - val_loss: 0.5764 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.5927 - auc: 0.4577 - val_loss: 0.5916 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 0.5926 - auc: 0.4920 - val_loss: 0.5946 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.5923 - auc: 0.4891 - val_loss: 0.5899 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.5924 - auc: 0.4839 - val_loss: 0.5886 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.5929 - auc: 0.4697 - val_loss: 0.5888 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 0.5930 - auc: 0.5000 - val_loss: 0.5788 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 0.5916 - auc: 0.4839 - val_loss: 0.5982 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 0.5933 - auc: 0.4830 - val_loss: 0.5670 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.5925 - auc: 0.4432 - val_loss: 0.5818 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.5919 - auc: 0.5000 - val_loss: 0.5912 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.5925 - auc: 0.4721 - val_loss: 0.5949 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 59ms/step - loss: 0.5932 - auc: 0.5000 - val_loss: 0.5824 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.5920 - auc: 0.4834 - val_loss: 0.5821 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.5931 - auc: 0.4811Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.5931 - auc: 0.4811 - val_loss: 0.5806 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 5\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 4\n",
      "num_dense_nodes: 5\n",
      "activation_dense: softmax\n",
      "learning_rate: 7.5e-03\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-04\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 12s 90ms/step - loss: 2.1320 - auc: 0.4661 - val_loss: 0.8540 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.4619 - auc: 0.5026 - val_loss: 0.2540 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.2115 - auc: 0.4903 - val_loss: 0.1766 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1677 - auc: 0.4769 - val_loss: 0.1510 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1482 - auc: 0.4619 - val_loss: 0.1355 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 38ms/step - loss: 0.1372 - auc: 0.4965 - val_loss: 0.1279 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1304 - auc: 0.5412 - val_loss: 0.1202 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1258 - auc: 0.4612 - val_loss: 0.1173 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1230 - auc: 0.5066 - val_loss: 0.1142 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1209 - auc: 0.5005 - val_loss: 0.1139 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1196 - auc: 0.4801 - val_loss: 0.1121 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1187 - auc: 0.5000 - val_loss: 0.1106 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1182 - auc: 0.5000 - val_loss: 0.1095 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1177 - auc: 0.4797 - val_loss: 0.1116 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1177 - auc: 0.5000 - val_loss: 0.1091 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1174 - auc: 0.5000 - val_loss: 0.1111 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1173 - auc: 0.5000 - val_loss: 0.1087 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1172 - auc: 0.5000 - val_loss: 0.1101 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1175 - auc: 0.5000 - val_loss: 0.1088 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1172 - auc: 0.5000 - val_loss: 0.1096 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.1188 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1173 - auc: 0.5000 - val_loss: 0.1080 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 5\n",
      "num_dense_nodes: 1\n",
      "activation_dense: relu\n",
      "learning_rate: 2.3e-02\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 10s 77ms/step - loss: 1.5533 - auc: 0.5000 - val_loss: 0.5693 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.4368 - auc: 0.5000 - val_loss: 0.3503 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.3165 - auc: 0.5000 - val_loss: 0.2768 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2738 - auc: 0.5000 - val_loss: 0.2591 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2505 - auc: 0.5000 - val_loss: 0.2321 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.2379 - auc: 0.5000 - val_loss: 0.2358 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.2314 - auc: 0.5000 - val_loss: 0.2103 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.2260 - auc: 0.5000 - val_loss: 0.2232 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2235 - auc: 0.5000 - val_loss: 0.2165 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 38ms/step - loss: 0.2208 - auc: 0.5000 - val_loss: 0.2222 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 38ms/step - loss: 0.2197 - auc: 0.5000 - val_loss: 0.2125 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2199 - auc: 0.5000 - val_loss: 0.2092 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2175 - auc: 0.5000 - val_loss: 0.2103 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.2184 - auc: 0.5000 - val_loss: 0.2220 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2178 - auc: 0.5000 - val_loss: 0.1975 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2169 - auc: 0.5000 - val_loss: 0.2142 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2181 - auc: 0.5000 - val_loss: 0.2114 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.2175 - auc: 0.5000 - val_loss: 0.2106 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.2171 - auc: 0.5000 - val_loss: 0.2048 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.2165 - auc: 0.5000 - val_loss: 0.2140 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.2183 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.2179 - auc: 0.5000 - val_loss: 0.2006 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.2\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 22\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 3.8e-03\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-04\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 12s 94ms/step - loss: 6.1345 - auc: 0.4877 - val_loss: 2.6676 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 1.3268 - auc: 0.4981 - val_loss: 0.4665 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.2633 - auc: 0.5000 - val_loss: 0.1521 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.1436 - auc: 0.4999 - val_loss: 0.1271 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.1345 - auc: 0.5149 - val_loss: 0.1266 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1345 - auc: 0.4784 - val_loss: 0.1271 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1341 - auc: 0.5000 - val_loss: 0.1255 - val_auc: 0.7373\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1338 - auc: 0.5530 - val_loss: 0.1265 - val_auc: 0.5339\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1333 - auc: 0.6428 - val_loss: 0.1243 - val_auc: 0.7538\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.1300 - auc: 0.7628 - val_loss: 0.1219 - val_auc: 0.6940\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.1246 - auc: 0.7804 - val_loss: 0.1207 - val_auc: 0.6423\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1229 - auc: 0.7706 - val_loss: 0.1217 - val_auc: 0.6450\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1212 - auc: 0.7612 - val_loss: 0.1195 - val_auc: 0.6485\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1181 - auc: 0.8011 - val_loss: 0.1223 - val_auc: 0.7302\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1167 - auc: 0.7881 - val_loss: 0.1226 - val_auc: 0.6348\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.1167 - auc: 0.7886 - val_loss: 0.1288 - val_auc: 0.7481\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1205 - auc: 0.7758 - val_loss: 0.1242 - val_auc: 0.6079\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1222 - auc: 0.7991 - val_loss: 0.1175 - val_auc: 0.7681\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1194 - auc: 0.7990 - val_loss: 0.1181 - val_auc: 0.7136\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1169 - auc: 0.8265 - val_loss: 0.1155 - val_auc: 0.7775\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1152 - auc: 0.8345 - val_loss: 0.1185 - val_auc: 0.7148\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.1155 - auc: 0.8168 - val_loss: 0.1197 - val_auc: 0.7325\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1136 - auc: 0.8246 - val_loss: 0.1220 - val_auc: 0.7019\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1142 - auc: 0.8157 - val_loss: 0.1232 - val_auc: 0.6898\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1140 - auc: 0.8107 - val_loss: 0.1241 - val_auc: 0.6381\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1139 - auc: 0.8259 - val_loss: 0.1235 - val_auc: 0.6878\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1133 - auc: 0.8502 - val_loss: 0.1219 - val_auc: 0.7050\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.1120 - auc: 0.8586 - val_loss: 0.1167 - val_auc: 0.7827\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1113 - auc: 0.8483 - val_loss: 0.1155 - val_auc: 0.7882\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1122 - auc: 0.8611 - val_loss: 0.1174 - val_auc: 0.7735\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1125 - auc: 0.8551 - val_loss: 0.1132 - val_auc: 0.7840\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1096 - auc: 0.8634 - val_loss: 0.1168 - val_auc: 0.7731\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.1086 - auc: 0.8783 - val_loss: 0.1156 - val_auc: 0.7799\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1100 - auc: 0.8705 - val_loss: 0.1214 - val_auc: 0.7443\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1080 - auc: 0.8678 - val_loss: 0.1233 - val_auc: 0.7777\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1058 - auc: 0.8924 - val_loss: 0.1281 - val_auc: 0.7085\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1052 - auc: 0.8978 - val_loss: 0.1248 - val_auc: 0.7068\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1049 - auc: 0.8827 - val_loss: 0.1279 - val_auc: 0.7099\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1091 - auc: 0.8692 - val_loss: 0.1266 - val_auc: 0.7051\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1060 - auc: 0.8844 - val_loss: 0.1174 - val_auc: 0.7863\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.1045 - auc: 0.8912 - val_loss: 0.1267 - val_auc: 0.7256\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1060 - auc: 0.8839 - val_loss: 0.1230 - val_auc: 0.7856\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1024 - auc: 0.9075 - val_loss: 0.1298 - val_auc: 0.6776\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1026 - auc: 0.8951 - val_loss: 0.1358 - val_auc: 0.6032\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1050 - auc: 0.8877 - val_loss: 0.1238 - val_auc: 0.7309\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1031 - auc: 0.8960 - val_loss: 0.1189 - val_auc: 0.7908\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1065 - auc: 0.8807 - val_loss: 0.1250 - val_auc: 0.7392\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1060 - auc: 0.8726 - val_loss: 0.1258 - val_auc: 0.7348\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1023 - auc: 0.8913 - val_loss: 0.1229 - val_auc: 0.7450\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1011 - auc: 0.8997 - val_loss: 0.1287 - val_auc: 0.6988\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1017 - auc: 0.9079 - val_loss: 0.1157 - val_auc: 0.8028\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1043 - auc: 0.8934 - val_loss: 0.1249 - val_auc: 0.7290\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1021 - auc: 0.8967 - val_loss: 0.1191 - val_auc: 0.7688\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1022 - auc: 0.9008 - val_loss: 0.1254 - val_auc: 0.7556\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0995 - auc: 0.9188 - val_loss: 0.1265 - val_auc: 0.7084\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1030 - auc: 0.8958 - val_loss: 0.1276 - val_auc: 0.6914\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.1004 - auc: 0.9035 - val_loss: 0.1221 - val_auc: 0.7808\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 2s 49ms/step - loss: 0.1012 - auc: 0.8922 - val_loss: 0.1246 - val_auc: 0.7422\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 2s 49ms/step - loss: 0.1020 - auc: 0.9059 - val_loss: 0.1282 - val_auc: 0.7081\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.1019 - auc: 0.8940 - val_loss: 0.1289 - val_auc: 0.6720\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.0996 - auc: 0.9114 - val_loss: 0.1338 - val_auc: 0.6425\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0993 - auc: 0.9120 - val_loss: 0.1344 - val_auc: 0.6225\n",
      "Epoch 63/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1029 - auc: 0.8966 - val_loss: 0.1260 - val_auc: 0.6691\n",
      "Epoch 64/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0998 - auc: 0.9127 - val_loss: 0.1285 - val_auc: 0.6791\n",
      "Epoch 65/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0977 - auc: 0.9137 - val_loss: 0.1310 - val_auc: 0.6396\n",
      "Epoch 66/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.1019 - auc: 0.9035 - val_loss: 0.1166 - val_auc: 0.7969\n",
      "Epoch 67/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0994 - auc: 0.9131 - val_loss: 0.1315 - val_auc: 0.6613\n",
      "Epoch 68/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0961 - auc: 0.9211 - val_loss: 0.1289 - val_auc: 0.6636\n",
      "Epoch 69/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.0982 - auc: 0.9136 - val_loss: 0.1302 - val_auc: 0.6556\n",
      "Epoch 70/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0972 - auc: 0.9137 - val_loss: 0.1311 - val_auc: 0.6345\n",
      "Epoch 71/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0934 - auc: 0.9131Restoring model weights from the end of the best epoch: 51.\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0934 - auc: 0.9131 - val_loss: 0.1290 - val_auc: 0.6738\n",
      "Epoch 71: early stopping\n",
      "\n",
      "Validation AUC: 67.38%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.1\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 9\n",
      "activation_dense: relu\n",
      "learning_rate: 2.6e-04\n",
      "l1_lambda: 1.0e-06\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 5s 51ms/step - loss: 0.6314 - auc: 0.4719 - val_loss: 0.5293 - val_auc: 0.5045\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.4455 - auc: 0.5112 - val_loss: 0.3732 - val_auc: 0.4558\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.3288 - auc: 0.5178 - val_loss: 0.2827 - val_auc: 0.3736\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2517 - auc: 0.4980 - val_loss: 0.2158 - val_auc: 0.4837\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1956 - auc: 0.5315 - val_loss: 0.1702 - val_auc: 0.4253\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1594 - auc: 0.4791 - val_loss: 0.1412 - val_auc: 0.4598\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1364 - auc: 0.4847 - val_loss: 0.1227 - val_auc: 0.4571\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1220 - auc: 0.4719 - val_loss: 0.1109 - val_auc: 0.4728\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1128 - auc: 0.5132 - val_loss: 0.1034 - val_auc: 0.4951\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 38ms/step - loss: 0.1071 - auc: 0.5024 - val_loss: 0.0986 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1035 - auc: 0.4814 - val_loss: 0.0953 - val_auc: 0.4818\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1011 - auc: 0.4919 - val_loss: 0.0932 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0997 - auc: 0.4552 - val_loss: 0.0919 - val_auc: 0.4992\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0988 - auc: 0.5000 - val_loss: 0.0910 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0982 - auc: 0.4975 - val_loss: 0.0906 - val_auc: 0.5190\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0979 - auc: 0.5060 - val_loss: 0.0902 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0976 - auc: 0.4999 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0974 - auc: 0.5000 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0972 - auc: 0.5000 - val_loss: 0.0895 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0971 - auc: 0.5150 - val_loss: 0.0893 - val_auc: 0.5107\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0969 - auc: 0.5055 - val_loss: 0.0892 - val_auc: 0.5190\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0967 - auc: 0.5584 - val_loss: 0.0891 - val_auc: 0.6614\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0966 - auc: 0.7447 - val_loss: 0.0889 - val_auc: 0.6774\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0964 - auc: 0.7313 - val_loss: 0.0888 - val_auc: 0.6804\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0962 - auc: 0.7612 - val_loss: 0.0886 - val_auc: 0.7105\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0960 - auc: 0.7227 - val_loss: 0.0884 - val_auc: 0.7386\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0957 - auc: 0.7777 - val_loss: 0.0883 - val_auc: 0.7216\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0954 - auc: 0.7635 - val_loss: 0.0880 - val_auc: 0.7592\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0951 - auc: 0.7796 - val_loss: 0.0878 - val_auc: 0.7338\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0945 - auc: 0.8023 - val_loss: 0.0874 - val_auc: 0.7317\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0939 - auc: 0.8317 - val_loss: 0.0870 - val_auc: 0.7527\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0934 - auc: 0.8283 - val_loss: 0.0867 - val_auc: 0.7681\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0927 - auc: 0.8304 - val_loss: 0.0863 - val_auc: 0.7640\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0922 - auc: 0.8467 - val_loss: 0.0860 - val_auc: 0.7580\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0914 - auc: 0.8242 - val_loss: 0.0855 - val_auc: 0.7902\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0904 - auc: 0.8528 - val_loss: 0.0848 - val_auc: 0.7760\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0893 - auc: 0.8618 - val_loss: 0.0845 - val_auc: 0.7906\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0883 - auc: 0.8657 - val_loss: 0.0836 - val_auc: 0.8060\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0861 - auc: 0.8728 - val_loss: 0.0824 - val_auc: 0.8102\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0840 - auc: 0.8809 - val_loss: 0.0815 - val_auc: 0.7910\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0823 - auc: 0.8724 - val_loss: 0.0810 - val_auc: 0.7693\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0805 - auc: 0.8815 - val_loss: 0.0800 - val_auc: 0.7736\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0787 - auc: 0.8793 - val_loss: 0.0791 - val_auc: 0.7772\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0763 - auc: 0.8941 - val_loss: 0.0785 - val_auc: 0.8037\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0744 - auc: 0.9050 - val_loss: 0.0779 - val_auc: 0.8216\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0728 - auc: 0.9076 - val_loss: 0.0787 - val_auc: 0.8228\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0707 - auc: 0.9116 - val_loss: 0.0771 - val_auc: 0.8188\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0689 - auc: 0.9105 - val_loss: 0.0768 - val_auc: 0.8126\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0671 - auc: 0.9128 - val_loss: 0.0765 - val_auc: 0.8158\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0655 - auc: 0.9186 - val_loss: 0.0763 - val_auc: 0.8210\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0637 - auc: 0.9107 - val_loss: 0.0762 - val_auc: 0.8142\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0626 - auc: 0.9110 - val_loss: 0.0767 - val_auc: 0.8126\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0607 - auc: 0.9225 - val_loss: 0.0760 - val_auc: 0.8296\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0594 - auc: 0.9247 - val_loss: 0.0763 - val_auc: 0.8353\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0578 - auc: 0.9322 - val_loss: 0.0766 - val_auc: 0.8285\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0565 - auc: 0.9372 - val_loss: 0.0766 - val_auc: 0.8223\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0553 - auc: 0.9374 - val_loss: 0.0772 - val_auc: 0.8238\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0538 - auc: 0.9405 - val_loss: 0.0772 - val_auc: 0.8273\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0525 - auc: 0.9469 - val_loss: 0.0776 - val_auc: 0.8245\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0510 - auc: 0.9478 - val_loss: 0.0782 - val_auc: 0.8326\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0501 - auc: 0.9492 - val_loss: 0.0789 - val_auc: 0.8314\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0508 - auc: 0.9398 - val_loss: 0.0813 - val_auc: 0.8245\n",
      "Epoch 63/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0480 - auc: 0.9486 - val_loss: 0.0801 - val_auc: 0.8399\n",
      "Epoch 64/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0467 - auc: 0.9464 - val_loss: 0.0813 - val_auc: 0.8093\n",
      "Epoch 65/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0454 - auc: 0.9609 - val_loss: 0.0836 - val_auc: 0.7914\n",
      "Epoch 66/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0445 - auc: 0.9498 - val_loss: 0.0823 - val_auc: 0.8355\n",
      "Epoch 67/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0435 - auc: 0.9516 - val_loss: 0.0833 - val_auc: 0.7839\n",
      "Epoch 68/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0423 - auc: 0.9570 - val_loss: 0.0842 - val_auc: 0.7825\n",
      "Epoch 69/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0412 - auc: 0.9590 - val_loss: 0.0852 - val_auc: 0.7817\n",
      "Epoch 70/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0406 - auc: 0.9579 - val_loss: 0.0871 - val_auc: 0.7946\n",
      "Epoch 71/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0399 - auc: 0.9628 - val_loss: 0.0875 - val_auc: 0.7785\n",
      "Epoch 72/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0382 - auc: 0.9590 - val_loss: 0.0886 - val_auc: 0.7741\n",
      "Epoch 73/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0370 - auc: 0.9652 - val_loss: 0.0901 - val_auc: 0.7877\n",
      "Epoch 74/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0361 - auc: 0.9624 - val_loss: 0.0920 - val_auc: 0.7853\n",
      "Epoch 75/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0349 - auc: 0.9664 - val_loss: 0.0929 - val_auc: 0.7233\n",
      "Epoch 76/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0348 - auc: 0.9689 - val_loss: 0.0961 - val_auc: 0.7797\n",
      "Epoch 77/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0337 - auc: 0.9733 - val_loss: 0.0956 - val_auc: 0.7549\n",
      "Epoch 78/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0324 - auc: 0.9732 - val_loss: 0.0955 - val_auc: 0.7035\n",
      "Epoch 79/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0323 - auc: 0.9700 - val_loss: 0.0965 - val_auc: 0.7243\n",
      "Epoch 80/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0311 - auc: 0.9717 - val_loss: 0.0991 - val_auc: 0.7195\n",
      "Epoch 81/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0308 - auc: 0.9797 - val_loss: 0.0999 - val_auc: 0.7222\n",
      "Epoch 82/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0300 - auc: 0.9808 - val_loss: 0.0991 - val_auc: 0.7173\n",
      "Epoch 83/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0285 - auc: 0.9782Restoring model weights from the end of the best epoch: 63.\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0285 - auc: 0.9782 - val_loss: 0.1014 - val_auc: 0.7238\n",
      "Epoch 83: early stopping\n",
      "\n",
      "Validation AUC: 72.38%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.5\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 7\n",
      "activation_dense: relu\n",
      "learning_rate: 3.1e-04\n",
      "l1_lambda: 1.0e-02\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 9s 78ms/step - loss: 1.9251 - auc: 0.5137 - val_loss: 1.6867 - val_auc: 0.3753\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 1.5121 - auc: 0.5221 - val_loss: 1.3328 - val_auc: 0.4053\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 1.2119 - auc: 0.4235 - val_loss: 1.0867 - val_auc: 0.4657\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 1.0037 - auc: 0.4984 - val_loss: 0.9112 - val_auc: 0.4988\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.8475 - auc: 0.5058 - val_loss: 0.7712 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.7192 - auc: 0.4815 - val_loss: 0.6530 - val_auc: 0.4465\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.6102 - auc: 0.4616 - val_loss: 0.5526 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.5176 - auc: 0.5162 - val_loss: 0.4675 - val_auc: 0.4959\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.4391 - auc: 0.4969 - val_loss: 0.3953 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.3726 - auc: 0.4984 - val_loss: 0.3345 - val_auc: 0.4988\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.3173 - auc: 0.4967 - val_loss: 0.2848 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2720 - auc: 0.5000 - val_loss: 0.2432 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.2339 - auc: 0.5000 - val_loss: 0.2088 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.2026 - auc: 0.5000 - val_loss: 0.1806 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1776 - auc: 0.5000 - val_loss: 0.1588 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1583 - auc: 0.4425 - val_loss: 0.1419 - val_auc: 0.4609\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1430 - auc: 0.5042 - val_loss: 0.1284 - val_auc: 0.4843\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 38ms/step - loss: 0.1309 - auc: 0.4761 - val_loss: 0.1175 - val_auc: 0.4460\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1212 - auc: 0.4609 - val_loss: 0.1091 - val_auc: 0.5013\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1140 - auc: 0.5065 - val_loss: 0.1031 - val_auc: 0.4950\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1090 - auc: 0.4849 - val_loss: 0.0991 - val_auc: 0.4728\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1058 - auc: 0.5070 - val_loss: 0.0966 - val_auc: 0.4938\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1037 - auc: 0.4835 - val_loss: 0.0948 - val_auc: 0.4889\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1022 - auc: 0.4868 - val_loss: 0.0936 - val_auc: 0.4691\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1009 - auc: 0.5709 - val_loss: 0.0923 - val_auc: 0.5922\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0989 - auc: 0.6341 - val_loss: 0.0906 - val_auc: 0.6717\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0970 - auc: 0.7612 - val_loss: 0.0893 - val_auc: 0.7333\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0958 - auc: 0.7638 - val_loss: 0.0881 - val_auc: 0.7302\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0941 - auc: 0.7952 - val_loss: 0.0870 - val_auc: 0.7559\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0920 - auc: 0.7934 - val_loss: 0.0857 - val_auc: 0.7928\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0880 - auc: 0.8225 - val_loss: 0.0836 - val_auc: 0.7441\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0856 - auc: 0.8185 - val_loss: 0.0822 - val_auc: 0.7830\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0836 - auc: 0.8287 - val_loss: 0.0811 - val_auc: 0.7620\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0788 - auc: 0.8628 - val_loss: 0.0809 - val_auc: 0.8003\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0800 - auc: 0.8507 - val_loss: 0.0799 - val_auc: 0.7810\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0752 - auc: 0.8724 - val_loss: 0.0794 - val_auc: 0.7933\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0754 - auc: 0.8723 - val_loss: 0.0792 - val_auc: 0.8044\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0743 - auc: 0.8815 - val_loss: 0.0791 - val_auc: 0.7621\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0719 - auc: 0.8594 - val_loss: 0.0803 - val_auc: 0.7481\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0713 - auc: 0.8824 - val_loss: 0.0796 - val_auc: 0.7656\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.0709 - auc: 0.8697 - val_loss: 0.0792 - val_auc: 0.7663\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 2s 38ms/step - loss: 0.0702 - auc: 0.8593 - val_loss: 0.0792 - val_auc: 0.7669\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.0680 - auc: 0.8767 - val_loss: 0.0797 - val_auc: 0.7881\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0672 - auc: 0.8809 - val_loss: 0.0785 - val_auc: 0.8307\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0654 - auc: 0.8873 - val_loss: 0.0799 - val_auc: 0.8407\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0662 - auc: 0.8971 - val_loss: 0.0791 - val_auc: 0.8185\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0636 - auc: 0.9002 - val_loss: 0.0809 - val_auc: 0.7986\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0619 - auc: 0.8936 - val_loss: 0.0843 - val_auc: 0.8184\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0640 - auc: 0.8984 - val_loss: 0.0813 - val_auc: 0.7938\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0622 - auc: 0.8928 - val_loss: 0.0825 - val_auc: 0.7951\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0619 - auc: 0.8950 - val_loss: 0.0826 - val_auc: 0.8058\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0612 - auc: 0.8955 - val_loss: 0.0840 - val_auc: 0.8093\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0607 - auc: 0.8937 - val_loss: 0.0860 - val_auc: 0.7931\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0603 - auc: 0.9109 - val_loss: 0.0837 - val_auc: 0.7951\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0573 - auc: 0.9106 - val_loss: 0.0863 - val_auc: 0.8001\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0572 - auc: 0.9243 - val_loss: 0.0861 - val_auc: 0.7833\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0601 - auc: 0.8887 - val_loss: 0.0833 - val_auc: 0.7970\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0565 - auc: 0.8985 - val_loss: 0.0856 - val_auc: 0.7999\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0536 - auc: 0.9302 - val_loss: 0.0858 - val_auc: 0.8012\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0571 - auc: 0.9191 - val_loss: 0.0834 - val_auc: 0.8010\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0542 - auc: 0.9131 - val_loss: 0.0840 - val_auc: 0.7986\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0536 - auc: 0.9214 - val_loss: 0.0875 - val_auc: 0.7507\n",
      "Epoch 63/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0510 - auc: 0.9259 - val_loss: 0.0861 - val_auc: 0.7885\n",
      "Epoch 64/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0502 - auc: 0.9268 - val_loss: 0.0874 - val_auc: 0.7680\n",
      "Epoch 65/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0518 - auc: 0.9165Restoring model weights from the end of the best epoch: 45.\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0518 - auc: 0.9165 - val_loss: 0.0871 - val_auc: 0.7804\n",
      "Epoch 65: early stopping\n",
      "\n",
      "Validation AUC: 78.04%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 5\n",
      "dropout_rate: 0.3\n",
      "num_dense_layers: 5\n",
      "num_dense_nodes: 26\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 7.3e-03\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-04\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 13s 94ms/step - loss: 1.8720 - auc: 0.5197 - val_loss: 0.6465 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.3178 - auc: 0.5104 - val_loss: 0.1352 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1260 - auc: 0.4749 - val_loss: 0.1089 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1170 - auc: 0.4951 - val_loss: 0.1089 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1161 - auc: 0.4527 - val_loss: 0.1079 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1164 - auc: 0.4877 - val_loss: 0.1070 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1164 - auc: 0.4882 - val_loss: 0.1084 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1163 - auc: 0.4503 - val_loss: 0.1089 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1165 - auc: 0.4934 - val_loss: 0.1070 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1161 - auc: 0.5047 - val_loss: 0.1073 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1166 - auc: 0.4707 - val_loss: 0.1079 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1167 - auc: 0.4689 - val_loss: 0.1088 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1161 - auc: 0.4673 - val_loss: 0.1078 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1167 - auc: 0.4882 - val_loss: 0.1085 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1162 - auc: 0.4830 - val_loss: 0.1081 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1164 - auc: 0.4896 - val_loss: 0.1070 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1162 - auc: 0.4531 - val_loss: 0.1079 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1164 - auc: 0.5033 - val_loss: 0.1085 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1175 - auc: 0.4667 - val_loss: 0.1091 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1164 - auc: 0.4592 - val_loss: 0.1084 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1165 - auc: 0.4872Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1165 - auc: 0.4872 - val_loss: 0.1080 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 4\n",
      "num_dense_nodes: 2\n",
      "activation_dense: relu\n",
      "learning_rate: 1.8e-02\n",
      "l1_lambda: 1.0e-02\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 5s 52ms/step - loss: 0.5552 - auc: 0.4802 - val_loss: 0.4286 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.3527 - auc: 0.5502 - val_loss: 0.2840 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.2466 - auc: 0.4983 - val_loss: 0.2085 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1900 - auc: 0.5351 - val_loss: 0.1666 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.1578 - auc: 0.5079 - val_loss: 0.1418 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1384 - auc: 0.4936 - val_loss: 0.1261 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1259 - auc: 0.5634 - val_loss: 0.1158 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1177 - auc: 0.5392 - val_loss: 0.1087 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1120 - auc: 0.5158 - val_loss: 0.1038 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1081 - auc: 0.5099 - val_loss: 0.1003 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1053 - auc: 0.5000 - val_loss: 0.0977 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1033 - auc: 0.4787 - val_loss: 0.0958 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1018 - auc: 0.5123 - val_loss: 0.0944 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1007 - auc: 0.5000 - val_loss: 0.0933 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0999 - auc: 0.4891 - val_loss: 0.0925 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0994 - auc: 0.5000 - val_loss: 0.0918 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0989 - auc: 0.5000 - val_loss: 0.0914 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0986 - auc: 0.5000 - val_loss: 0.0911 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0983 - auc: 0.5066 - val_loss: 0.0907 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0981 - auc: 0.5000 - val_loss: 0.0904 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0982 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0980 - auc: 0.5000 - val_loss: 0.0904 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.0\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 4\n",
      "activation_dense: relu\n",
      "learning_rate: 1.3e-04\n",
      "l1_lambda: 1.0e-02\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 4s 50ms/step - loss: 0.5638 - auc: 0.4667 - val_loss: 0.4108 - val_auc: 0.3346\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.3376 - auc: 0.4524 - val_loss: 0.2777 - val_auc: 0.3492\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.2530 - auc: 0.4259 - val_loss: 0.2264 - val_auc: 0.3577\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.2159 - auc: 0.3960 - val_loss: 0.1995 - val_auc: 0.3866\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1945 - auc: 0.3653 - val_loss: 0.1822 - val_auc: 0.3644\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1801 - auc: 0.3852 - val_loss: 0.1701 - val_auc: 0.3946\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1695 - auc: 0.4234 - val_loss: 0.1607 - val_auc: 0.3803\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1613 - auc: 0.3836 - val_loss: 0.1531 - val_auc: 0.3888\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1546 - auc: 0.4320 - val_loss: 0.1469 - val_auc: 0.4138\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1488 - auc: 0.4199 - val_loss: 0.1415 - val_auc: 0.4255\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1439 - auc: 0.4195 - val_loss: 0.1368 - val_auc: 0.4089\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1396 - auc: 0.3917 - val_loss: 0.1325 - val_auc: 0.4277\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1357 - auc: 0.4380 - val_loss: 0.1288 - val_auc: 0.4266\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1322 - auc: 0.4462 - val_loss: 0.1254 - val_auc: 0.4010\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1290 - auc: 0.4028 - val_loss: 0.1223 - val_auc: 0.4643\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1261 - auc: 0.4716 - val_loss: 0.1194 - val_auc: 0.4303\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1235 - auc: 0.4627 - val_loss: 0.1167 - val_auc: 0.4020\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1210 - auc: 0.4383 - val_loss: 0.1142 - val_auc: 0.4670\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1187 - auc: 0.4585 - val_loss: 0.1119 - val_auc: 0.4373\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.1165 - auc: 0.4583 - val_loss: 0.1097 - val_auc: 0.4814\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1144 - auc: 0.4414 - val_loss: 0.1075 - val_auc: 0.4690\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1125 - auc: 0.4615 - val_loss: 0.1057 - val_auc: 0.4959\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1108 - auc: 0.4584 - val_loss: 0.1039 - val_auc: 0.4240\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1091 - auc: 0.4774 - val_loss: 0.1022 - val_auc: 0.4979\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1077 - auc: 0.4568 - val_loss: 0.1007 - val_auc: 0.4613\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1063 - auc: 0.4795 - val_loss: 0.0993 - val_auc: 0.4959\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1052 - auc: 0.4728 - val_loss: 0.0981 - val_auc: 0.4492\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1041 - auc: 0.4556 - val_loss: 0.0971 - val_auc: 0.4905\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1032 - auc: 0.4919 - val_loss: 0.0961 - val_auc: 0.4996\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1024 - auc: 0.4781 - val_loss: 0.0953 - val_auc: 0.4499\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1017 - auc: 0.4595 - val_loss: 0.0945 - val_auc: 0.4897\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1011 - auc: 0.4888 - val_loss: 0.0939 - val_auc: 0.4988\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1006 - auc: 0.4996 - val_loss: 0.0933 - val_auc: 0.5000\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.1001 - auc: 0.4955 - val_loss: 0.0929 - val_auc: 0.4644\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0997 - auc: 0.4550 - val_loss: 0.0924 - val_auc: 0.4786\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0994 - auc: 0.4882 - val_loss: 0.0921 - val_auc: 0.4955\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0991 - auc: 0.4989 - val_loss: 0.0918 - val_auc: 0.4996\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0989 - auc: 0.4998 - val_loss: 0.0915 - val_auc: 0.5000\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0987 - auc: 0.5000 - val_loss: 0.0912 - val_auc: 0.5000\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0985 - auc: 0.4952 - val_loss: 0.0910 - val_auc: 0.4814\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0984 - auc: 0.4483 - val_loss: 0.0908 - val_auc: 0.4634\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0983 - auc: 0.4519 - val_loss: 0.0907 - val_auc: 0.4769\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0982 - auc: 0.4847 - val_loss: 0.0906 - val_auc: 0.4876\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0981 - auc: 0.4923 - val_loss: 0.0905 - val_auc: 0.4975\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0980 - auc: 0.4990 - val_loss: 0.0903 - val_auc: 0.4996\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0980 - auc: 0.4996 - val_loss: 0.0903 - val_auc: 0.4996\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0979 - auc: 0.4998 - val_loss: 0.0902 - val_auc: 0.5000\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0979 - auc: 0.4999 - val_loss: 0.0901 - val_auc: 0.5000\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0978 - auc: 0.5000 - val_loss: 0.0901 - val_auc: 0.5000\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0978 - auc: 0.5000 - val_loss: 0.0901 - val_auc: 0.5000\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0978 - auc: 0.5000 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0978 - auc: 0.5000 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 53/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0979 - auc: 0.5000Restoring model weights from the end of the best epoch: 33.\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 53: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.3\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 17\n",
      "activation_dense: relu\n",
      "learning_rate: 2.3e-04\n",
      "l1_lambda: 1.0e-04\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 7s 59ms/step - loss: 0.2542 - auc: 0.5378 - val_loss: 0.1179 - val_auc: 0.4758\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.1114 - auc: 0.5025 - val_loss: 0.0987 - val_auc: 0.4919\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.1025 - auc: 0.4964 - val_loss: 0.0937 - val_auc: 0.4781\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0996 - auc: 0.5380 - val_loss: 0.0918 - val_auc: 0.4947\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0983 - auc: 0.5609 - val_loss: 0.0908 - val_auc: 0.5453\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0975 - auc: 0.5939 - val_loss: 0.0901 - val_auc: 0.5988\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0969 - auc: 0.6106 - val_loss: 0.0896 - val_auc: 0.6154\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0963 - auc: 0.6408 - val_loss: 0.0892 - val_auc: 0.6520\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0956 - auc: 0.6878 - val_loss: 0.0887 - val_auc: 0.6877\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0948 - auc: 0.7282 - val_loss: 0.0881 - val_auc: 0.7382\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0937 - auc: 0.7756 - val_loss: 0.0875 - val_auc: 0.7635\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0927 - auc: 0.7943 - val_loss: 0.0868 - val_auc: 0.7651\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0916 - auc: 0.8064 - val_loss: 0.0862 - val_auc: 0.7642\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0903 - auc: 0.8200 - val_loss: 0.0857 - val_auc: 0.7782\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0891 - auc: 0.8204 - val_loss: 0.0851 - val_auc: 0.7897\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0876 - auc: 0.8252 - val_loss: 0.0847 - val_auc: 0.8004\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0861 - auc: 0.8389 - val_loss: 0.0842 - val_auc: 0.7786\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0845 - auc: 0.8442 - val_loss: 0.0833 - val_auc: 0.7980\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0830 - auc: 0.8599 - val_loss: 0.0825 - val_auc: 0.7853\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0817 - auc: 0.8616 - val_loss: 0.0823 - val_auc: 0.8181\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0806 - auc: 0.8654 - val_loss: 0.0813 - val_auc: 0.8051\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0790 - auc: 0.8761 - val_loss: 0.0807 - val_auc: 0.8090\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0777 - auc: 0.8794 - val_loss: 0.0802 - val_auc: 0.8191\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0764 - auc: 0.8755 - val_loss: 0.0797 - val_auc: 0.8245\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0752 - auc: 0.8848 - val_loss: 0.0794 - val_auc: 0.8212\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0738 - auc: 0.8880 - val_loss: 0.0794 - val_auc: 0.7950\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0726 - auc: 0.8962 - val_loss: 0.0789 - val_auc: 0.8162\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0708 - auc: 0.9018 - val_loss: 0.0786 - val_auc: 0.8208\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0699 - auc: 0.9043 - val_loss: 0.0784 - val_auc: 0.8305\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0681 - auc: 0.9081 - val_loss: 0.0782 - val_auc: 0.8336\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0668 - auc: 0.9068 - val_loss: 0.0782 - val_auc: 0.8423\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0655 - auc: 0.9135 - val_loss: 0.0782 - val_auc: 0.8384\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0637 - auc: 0.9193 - val_loss: 0.0781 - val_auc: 0.8325\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0625 - auc: 0.9214 - val_loss: 0.0783 - val_auc: 0.8429\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0613 - auc: 0.9293 - val_loss: 0.0785 - val_auc: 0.8335\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0595 - auc: 0.9320 - val_loss: 0.0787 - val_auc: 0.8354\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0584 - auc: 0.9307 - val_loss: 0.0787 - val_auc: 0.8396\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0579 - auc: 0.9258 - val_loss: 0.0789 - val_auc: 0.8166\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0559 - auc: 0.9308 - val_loss: 0.0791 - val_auc: 0.8026\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0545 - auc: 0.9326 - val_loss: 0.0795 - val_auc: 0.7992\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0534 - auc: 0.9367 - val_loss: 0.0797 - val_auc: 0.8047\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0520 - auc: 0.9390 - val_loss: 0.0804 - val_auc: 0.8068\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0512 - auc: 0.9395 - val_loss: 0.0804 - val_auc: 0.8019\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0500 - auc: 0.9418 - val_loss: 0.0809 - val_auc: 0.8029\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0488 - auc: 0.9482 - val_loss: 0.0814 - val_auc: 0.8027\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0481 - auc: 0.9498 - val_loss: 0.0819 - val_auc: 0.8016\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0469 - auc: 0.9507 - val_loss: 0.0827 - val_auc: 0.8032\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0462 - auc: 0.9549 - val_loss: 0.0828 - val_auc: 0.8043\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0448 - auc: 0.9602 - val_loss: 0.0835 - val_auc: 0.8082\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0446 - auc: 0.9584 - val_loss: 0.0835 - val_auc: 0.8161\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0433 - auc: 0.9637 - val_loss: 0.0843 - val_auc: 0.8138\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0423 - auc: 0.9661 - val_loss: 0.0839 - val_auc: 0.8019\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.0419 - auc: 0.9684 - val_loss: 0.0850 - val_auc: 0.8000\n",
      "Epoch 54/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0399 - auc: 0.9710Restoring model weights from the end of the best epoch: 34.\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.0404 - auc: 0.9707 - val_loss: 0.0864 - val_auc: 0.7907\n",
      "Epoch 54: early stopping\n",
      "\n",
      "Validation AUC: 79.07%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 5\n",
      "num_dense_nodes: 13\n",
      "activation_dense: relu\n",
      "learning_rate: 3.3e-04\n",
      "l1_lambda: 1.0e-04\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 14s 83ms/step - loss: 0.6895 - auc: 0.4910 - val_loss: 0.6305 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.5721 - auc: 0.5275 - val_loss: 0.4934 - val_auc: 0.3970\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.4046 - auc: 0.5521 - val_loss: 0.3003 - val_auc: 0.5264\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.2231 - auc: 0.5174 - val_loss: 0.1519 - val_auc: 0.4988\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1288 - auc: 0.5235 - val_loss: 0.1054 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.1070 - auc: 0.5014 - val_loss: 0.0967 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 49ms/step - loss: 0.1033 - auc: 0.4744 - val_loss: 0.0945 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.1018 - auc: 0.5000 - val_loss: 0.0935 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.1009 - auc: 0.5546 - val_loss: 0.0926 - val_auc: 0.6101\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.1002 - auc: 0.5329 - val_loss: 0.0920 - val_auc: 0.5017\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0995 - auc: 0.6286 - val_loss: 0.0913 - val_auc: 0.5380\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.0989 - auc: 0.5989 - val_loss: 0.0908 - val_auc: 0.6637\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0984 - auc: 0.5448 - val_loss: 0.0903 - val_auc: 0.6573\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0977 - auc: 0.7020 - val_loss: 0.0897 - val_auc: 0.7303\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.0971 - auc: 0.7307 - val_loss: 0.0893 - val_auc: 0.7468\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0966 - auc: 0.7559 - val_loss: 0.0890 - val_auc: 0.6952\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0964 - auc: 0.7328 - val_loss: 0.0886 - val_auc: 0.7196\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0958 - auc: 0.7511 - val_loss: 0.0882 - val_auc: 0.7439\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0953 - auc: 0.7663 - val_loss: 0.0878 - val_auc: 0.7406\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0945 - auc: 0.7781 - val_loss: 0.0873 - val_auc: 0.7670\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.0939 - auc: 0.8119 - val_loss: 0.0868 - val_auc: 0.7524\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0933 - auc: 0.8260 - val_loss: 0.0863 - val_auc: 0.7559\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0927 - auc: 0.8258 - val_loss: 0.0859 - val_auc: 0.7645\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0919 - auc: 0.8284 - val_loss: 0.0854 - val_auc: 0.7782\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0911 - auc: 0.8470 - val_loss: 0.0850 - val_auc: 0.7623\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0902 - auc: 0.8434 - val_loss: 0.0845 - val_auc: 0.7795\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0899 - auc: 0.8430 - val_loss: 0.0842 - val_auc: 0.8117\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0895 - auc: 0.8379 - val_loss: 0.0838 - val_auc: 0.7811\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0884 - auc: 0.8633 - val_loss: 0.0833 - val_auc: 0.7789\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0874 - auc: 0.8586 - val_loss: 0.0827 - val_auc: 0.7734\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0863 - auc: 0.8621 - val_loss: 0.0821 - val_auc: 0.7894\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0855 - auc: 0.8641 - val_loss: 0.0815 - val_auc: 0.8093\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0845 - auc: 0.8694 - val_loss: 0.0810 - val_auc: 0.8156\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0842 - auc: 0.8713 - val_loss: 0.0808 - val_auc: 0.8150\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.0831 - auc: 0.8776 - val_loss: 0.0803 - val_auc: 0.8127\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.0822 - auc: 0.8734 - val_loss: 0.0799 - val_auc: 0.8095\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0825 - auc: 0.8668 - val_loss: 0.0796 - val_auc: 0.7904\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.0806 - auc: 0.8810 - val_loss: 0.0791 - val_auc: 0.8085\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0802 - auc: 0.8777 - val_loss: 0.0790 - val_auc: 0.8046\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0793 - auc: 0.8809 - val_loss: 0.0789 - val_auc: 0.8071\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0789 - auc: 0.8840 - val_loss: 0.0782 - val_auc: 0.8063\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0777 - auc: 0.8895 - val_loss: 0.0781 - val_auc: 0.8078\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0768 - auc: 0.8825 - val_loss: 0.0785 - val_auc: 0.7952\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0764 - auc: 0.8772 - val_loss: 0.0775 - val_auc: 0.8047\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0742 - auc: 0.9063 - val_loss: 0.0770 - val_auc: 0.8133\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0741 - auc: 0.9013 - val_loss: 0.0768 - val_auc: 0.8030\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0728 - auc: 0.9047 - val_loss: 0.0771 - val_auc: 0.8144\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0727 - auc: 0.9053 - val_loss: 0.0763 - val_auc: 0.8089\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0708 - auc: 0.9101 - val_loss: 0.0761 - val_auc: 0.8171\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0700 - auc: 0.9051 - val_loss: 0.0756 - val_auc: 0.8237\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0682 - auc: 0.9182 - val_loss: 0.0755 - val_auc: 0.8260\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0669 - auc: 0.9149 - val_loss: 0.0756 - val_auc: 0.8330\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0667 - auc: 0.9102 - val_loss: 0.0778 - val_auc: 0.8250\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.0666 - auc: 0.9133 - val_loss: 0.0763 - val_auc: 0.8181\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0654 - auc: 0.9181 - val_loss: 0.0765 - val_auc: 0.8217\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0634 - auc: 0.9280 - val_loss: 0.0766 - val_auc: 0.8204\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0641 - auc: 0.9108 - val_loss: 0.0764 - val_auc: 0.8320\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0626 - auc: 0.9211 - val_loss: 0.0784 - val_auc: 0.8115\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0622 - auc: 0.9251 - val_loss: 0.0786 - val_auc: 0.8141\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0582 - auc: 0.9399 - val_loss: 0.0780 - val_auc: 0.8114\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0621 - auc: 0.9216 - val_loss: 0.0782 - val_auc: 0.8251\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0597 - auc: 0.9350 - val_loss: 0.0786 - val_auc: 0.8266\n",
      "Epoch 63/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0594 - auc: 0.9397 - val_loss: 0.0791 - val_auc: 0.8252\n",
      "Epoch 64/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0569 - auc: 0.9422 - val_loss: 0.0809 - val_auc: 0.8177\n",
      "Epoch 65/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0588 - auc: 0.9326 - val_loss: 0.0824 - val_auc: 0.8051\n",
      "Epoch 66/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0605 - auc: 0.9297 - val_loss: 0.0797 - val_auc: 0.8240\n",
      "Epoch 67/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0574 - auc: 0.9245 - val_loss: 0.0805 - val_auc: 0.8124\n",
      "Epoch 68/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.0576 - auc: 0.9293 - val_loss: 0.0846 - val_auc: 0.8011\n",
      "Epoch 69/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0558 - auc: 0.9357 - val_loss: 0.0850 - val_auc: 0.7999\n",
      "Epoch 70/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0596 - auc: 0.9283 - val_loss: 0.0835 - val_auc: 0.7967\n",
      "Epoch 71/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0559 - auc: 0.9464 - val_loss: 0.0896 - val_auc: 0.7915\n",
      "Epoch 72/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0570 - auc: 0.9388Restoring model weights from the end of the best epoch: 52.\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0567 - auc: 0.9393 - val_loss: 0.0846 - val_auc: 0.7974\n",
      "Epoch 72: early stopping\n",
      "\n",
      "Validation AUC: 79.74%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 30\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 28\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 4.9e-03\n",
      "l1_lambda: 1.0e-03\n",
      "l2_lambda: 1.0e-04\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 25s 161ms/step - loss: 0.4232 - auc: 0.5000 - val_loss: 0.1206 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 0.1100 - auc: 0.5000 - val_loss: 0.0952 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 60ms/step - loss: 0.1024 - auc: 0.5000 - val_loss: 0.0940 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.1021 - auc: 0.5000 - val_loss: 0.0941 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 0.1021 - auc: 0.5000 - val_loss: 0.0940 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 62ms/step - loss: 0.1021 - auc: 0.5000 - val_loss: 0.0942 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.1022 - auc: 0.5000 - val_loss: 0.0940 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 0.1021 - auc: 0.5000 - val_loss: 0.0942 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.1021 - auc: 0.5000 - val_loss: 0.0942 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.1021 - auc: 0.5000 - val_loss: 0.0942 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 0.1022 - auc: 0.5000 - val_loss: 0.0941 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 0.1021 - auc: 0.5000 - val_loss: 0.0940 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 0.1021 - auc: 0.5000 - val_loss: 0.0939 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.1021 - auc: 0.5000 - val_loss: 0.0941 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.1021 - auc: 0.5000 - val_loss: 0.0940 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 0.1021 - auc: 0.5000 - val_loss: 0.0941 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 61ms/step - loss: 0.1022 - auc: 0.5000 - val_loss: 0.0942 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 0.1021 - auc: 0.5000 - val_loss: 0.0941 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 0.1022 - auc: 0.5000 - val_loss: 0.0940 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 56ms/step - loss: 0.1022 - auc: 0.5000 - val_loss: 0.0939 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1020 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 58ms/step - loss: 0.1020 - auc: 0.5000 - val_loss: 0.0939 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 2\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 1\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 4.9e-03\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-04\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 13s 89ms/step - loss: 11.9238 - auc: 0.4817 - val_loss: 2.5455 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 1.1716 - auc: 0.4602 - val_loss: 0.5835 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.4950 - auc: 0.5039 - val_loss: 0.4365 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.4209 - auc: 0.4969 - val_loss: 0.4006 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.3858 - auc: 0.4667 - val_loss: 0.3607 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.3603 - auc: 0.4470 - val_loss: 0.3536 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.3438 - auc: 0.4886 - val_loss: 0.3229 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.3314 - auc: 0.4999 - val_loss: 0.3268 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.3225 - auc: 0.5148 - val_loss: 0.3137 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.3157 - auc: 0.4681 - val_loss: 0.3117 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.3109 - auc: 0.4953 - val_loss: 0.3020 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.3076 - auc: 0.5194 - val_loss: 0.2986 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.3049 - auc: 0.4692 - val_loss: 0.2939 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.3022 - auc: 0.5000 - val_loss: 0.3011 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.3014 - auc: 0.5000 - val_loss: 0.2844 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.2996 - auc: 0.5014 - val_loss: 0.2946 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.2989 - auc: 0.5000 - val_loss: 0.2921 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.2974 - auc: 0.5000 - val_loss: 0.2992 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.2979 - auc: 0.4328 - val_loss: 0.2859 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.2966 - auc: 0.5000 - val_loss: 0.2909 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2967 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.2967 - auc: 0.5000 - val_loss: 0.2826 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 0\n",
      "activation_lstm: sigmoid\n",
      "num_lstm_nodes: 10\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 16\n",
      "activation_dense: relu\n",
      "learning_rate: 2.1e-04\n",
      "l1_lambda: 1.0e-04\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 6s 52ms/step - loss: 0.8766 - auc: 0.4534 - val_loss: 0.6312 - val_auc: 0.3107\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.5533 - auc: 0.4129 - val_loss: 0.4973 - val_auc: 0.3441\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.4738 - auc: 0.4305 - val_loss: 0.4516 - val_auc: 0.3327\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.4402 - auc: 0.3734 - val_loss: 0.4266 - val_auc: 0.3475\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.4194 - auc: 0.4095 - val_loss: 0.4091 - val_auc: 0.3515\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.4037 - auc: 0.3775 - val_loss: 0.3950 - val_auc: 0.3230\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.3909 - auc: 0.3975 - val_loss: 0.3832 - val_auc: 0.4123\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.3800 - auc: 0.3946 - val_loss: 0.3729 - val_auc: 0.3526\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.3702 - auc: 0.3962 - val_loss: 0.3635 - val_auc: 0.4298\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.3612 - auc: 0.4253 - val_loss: 0.3549 - val_auc: 0.4030\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.3529 - auc: 0.3774 - val_loss: 0.3468 - val_auc: 0.3757\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.3451 - auc: 0.4237 - val_loss: 0.3392 - val_auc: 0.4484\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.3377 - auc: 0.4505 - val_loss: 0.3319 - val_auc: 0.4744\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.3307 - auc: 0.4317 - val_loss: 0.3250 - val_auc: 0.4201\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.3240 - auc: 0.4324 - val_loss: 0.3184 - val_auc: 0.4084\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.3175 - auc: 0.4613 - val_loss: 0.3120 - val_auc: 0.4773\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.3113 - auc: 0.4858 - val_loss: 0.3059 - val_auc: 0.4765\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.3054 - auc: 0.4299 - val_loss: 0.3000 - val_auc: 0.4740\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.2996 - auc: 0.4358 - val_loss: 0.2943 - val_auc: 0.4645\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.2940 - auc: 0.4716 - val_loss: 0.2888 - val_auc: 0.4707\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.2887 - auc: 0.4897 - val_loss: 0.2835 - val_auc: 0.4661\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.2835 - auc: 0.4511 - val_loss: 0.2783 - val_auc: 0.4034\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.2785 - auc: 0.4811 - val_loss: 0.2733 - val_auc: 0.4500\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.2736 - auc: 0.4250 - val_loss: 0.2685 - val_auc: 0.4988\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.2688 - auc: 0.4894 - val_loss: 0.2638 - val_auc: 0.4934\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.2643 - auc: 0.4735 - val_loss: 0.2592 - val_auc: 0.4661\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.2598 - auc: 0.4645 - val_loss: 0.2548 - val_auc: 0.3840\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.2555 - auc: 0.5018 - val_loss: 0.2505 - val_auc: 0.5004\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.2513 - auc: 0.4676 - val_loss: 0.2463 - val_auc: 0.4988\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.2473 - auc: 0.4488 - val_loss: 0.2423 - val_auc: 0.4624\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.2433 - auc: 0.4848 - val_loss: 0.2383 - val_auc: 0.4257\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.2395 - auc: 0.4874 - val_loss: 0.2345 - val_auc: 0.4996\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.2358 - auc: 0.4893 - val_loss: 0.2308 - val_auc: 0.4814\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.2321 - auc: 0.4965 - val_loss: 0.2272 - val_auc: 0.4343\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.2286 - auc: 0.4877 - val_loss: 0.2237 - val_auc: 0.4996\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.2252 - auc: 0.4862 - val_loss: 0.2203 - val_auc: 0.4810\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.2219 - auc: 0.4940 - val_loss: 0.2169 - val_auc: 0.5000\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.2186 - auc: 0.4816 - val_loss: 0.2137 - val_auc: 0.4988\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.2155 - auc: 0.4953 - val_loss: 0.2106 - val_auc: 0.4355\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.2124 - auc: 0.4579 - val_loss: 0.2075 - val_auc: 0.4996\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.2095 - auc: 0.4959 - val_loss: 0.2045 - val_auc: 0.4621\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.2066 - auc: 0.4913 - val_loss: 0.2016 - val_auc: 0.4996\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.2037 - auc: 0.4477 - val_loss: 0.1988 - val_auc: 0.4765\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.2010 - auc: 0.4967 - val_loss: 0.1961 - val_auc: 0.5000\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.1983 - auc: 0.4859 - val_loss: 0.1934 - val_auc: 0.4872\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.1957 - auc: 0.4987 - val_loss: 0.1908 - val_auc: 0.5000\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 1s 27ms/step - loss: 0.1932 - auc: 0.4949 - val_loss: 0.1882 - val_auc: 0.4893\n",
      "Epoch 48/1000\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.1903 - auc: 0.4980Restoring model weights from the end of the best epoch: 28.\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.1907 - auc: 0.4980 - val_loss: 0.1857 - val_auc: 0.5000\n",
      "Epoch 48: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 19\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 1.6e-04\n",
      "l1_lambda: 1.0e-04\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 15s 91ms/step - loss: 0.5861 - auc: 0.4978 - val_loss: 0.4438 - val_auc: 0.4162\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.3732 - auc: 0.4768 - val_loss: 0.2983 - val_auc: 0.3765\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.2656 - auc: 0.4662 - val_loss: 0.2229 - val_auc: 0.3288\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.2041 - auc: 0.4483 - val_loss: 0.1755 - val_auc: 0.3275\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1663 - auc: 0.4070 - val_loss: 0.1461 - val_auc: 0.4386\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1427 - auc: 0.5008 - val_loss: 0.1283 - val_auc: 0.4831\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1285 - auc: 0.4267 - val_loss: 0.1168 - val_auc: 0.4666\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1197 - auc: 0.4583 - val_loss: 0.1100 - val_auc: 0.4067\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1144 - auc: 0.4625 - val_loss: 0.1055 - val_auc: 0.4926\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1111 - auc: 0.4705 - val_loss: 0.1027 - val_auc: 0.5033\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1088 - auc: 0.4834 - val_loss: 0.1006 - val_auc: 0.4718\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1071 - auc: 0.4283 - val_loss: 0.0991 - val_auc: 0.4983\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1059 - auc: 0.4949 - val_loss: 0.0980 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1050 - auc: 0.4844 - val_loss: 0.0970 - val_auc: 0.4357\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1042 - auc: 0.4733 - val_loss: 0.0963 - val_auc: 0.4955\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1037 - auc: 0.4911 - val_loss: 0.0956 - val_auc: 0.4996\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1031 - auc: 0.4976 - val_loss: 0.0951 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1026 - auc: 0.5000 - val_loss: 0.0946 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1022 - auc: 0.4999 - val_loss: 0.0942 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1018 - auc: 0.5000 - val_loss: 0.0937 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1014 - auc: 0.4999 - val_loss: 0.0933 - val_auc: 0.5000\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1009 - auc: 0.5053 - val_loss: 0.0929 - val_auc: 0.5111\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1004 - auc: 0.5117 - val_loss: 0.0925 - val_auc: 0.5186\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1001 - auc: 0.5533 - val_loss: 0.0922 - val_auc: 0.5912\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0998 - auc: 0.5780 - val_loss: 0.0918 - val_auc: 0.5973\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0994 - auc: 0.6151 - val_loss: 0.0916 - val_auc: 0.6296\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0992 - auc: 0.5746 - val_loss: 0.0913 - val_auc: 0.6217\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0989 - auc: 0.6544 - val_loss: 0.0910 - val_auc: 0.6741\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0985 - auc: 0.7260 - val_loss: 0.0908 - val_auc: 0.7022\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0981 - auc: 0.7153 - val_loss: 0.0904 - val_auc: 0.7220\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0975 - auc: 0.7091 - val_loss: 0.0900 - val_auc: 0.6657\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0971 - auc: 0.6963 - val_loss: 0.0897 - val_auc: 0.6654\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.0967 - auc: 0.7247 - val_loss: 0.0894 - val_auc: 0.7100\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0964 - auc: 0.7625 - val_loss: 0.0892 - val_auc: 0.7055\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0962 - auc: 0.7682 - val_loss: 0.0889 - val_auc: 0.7457\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0956 - auc: 0.7867 - val_loss: 0.0886 - val_auc: 0.7309\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0951 - auc: 0.8066 - val_loss: 0.0883 - val_auc: 0.7223\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0944 - auc: 0.7993 - val_loss: 0.0879 - val_auc: 0.7304\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0937 - auc: 0.8180 - val_loss: 0.0874 - val_auc: 0.7314\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0930 - auc: 0.8188 - val_loss: 0.0869 - val_auc: 0.7437\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0923 - auc: 0.8303 - val_loss: 0.0864 - val_auc: 0.7653\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0916 - auc: 0.8264 - val_loss: 0.0858 - val_auc: 0.7692\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0910 - auc: 0.8299 - val_loss: 0.0852 - val_auc: 0.7785\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0898 - auc: 0.8358 - val_loss: 0.0846 - val_auc: 0.7766\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0883 - auc: 0.8447 - val_loss: 0.0836 - val_auc: 0.7803\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0855 - auc: 0.8454 - val_loss: 0.0820 - val_auc: 0.7756\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0841 - auc: 0.8357 - val_loss: 0.0809 - val_auc: 0.7870\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0817 - auc: 0.8569 - val_loss: 0.0799 - val_auc: 0.8026\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0792 - auc: 0.8610 - val_loss: 0.0792 - val_auc: 0.8236\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0786 - auc: 0.8674 - val_loss: 0.0786 - val_auc: 0.8176\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0772 - auc: 0.8571 - val_loss: 0.0781 - val_auc: 0.8430\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0761 - auc: 0.8711 - val_loss: 0.0776 - val_auc: 0.8288\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0745 - auc: 0.8750 - val_loss: 0.0774 - val_auc: 0.8157\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0745 - auc: 0.8636 - val_loss: 0.0770 - val_auc: 0.8254\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0719 - auc: 0.8823 - val_loss: 0.0767 - val_auc: 0.8311\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0727 - auc: 0.8805 - val_loss: 0.0764 - val_auc: 0.8453\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 2s 45ms/step - loss: 0.0715 - auc: 0.8820 - val_loss: 0.0769 - val_auc: 0.8136\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0694 - auc: 0.8950 - val_loss: 0.0760 - val_auc: 0.8284\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0709 - auc: 0.8761 - val_loss: 0.0763 - val_auc: 0.8179\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0684 - auc: 0.8961 - val_loss: 0.0760 - val_auc: 0.8248\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0670 - auc: 0.8982 - val_loss: 0.0757 - val_auc: 0.8383\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0634 - auc: 0.9099 - val_loss: 0.0763 - val_auc: 0.8542\n",
      "Epoch 63/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0665 - auc: 0.8971 - val_loss: 0.0761 - val_auc: 0.8423\n",
      "Epoch 64/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0639 - auc: 0.9136 - val_loss: 0.0776 - val_auc: 0.8434\n",
      "Epoch 65/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0638 - auc: 0.9007 - val_loss: 0.0763 - val_auc: 0.8526\n",
      "Epoch 66/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0645 - auc: 0.8989 - val_loss: 0.0775 - val_auc: 0.8260\n",
      "Epoch 67/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0619 - auc: 0.9159 - val_loss: 0.0772 - val_auc: 0.8347\n",
      "Epoch 68/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0615 - auc: 0.9174 - val_loss: 0.0766 - val_auc: 0.8303\n",
      "Epoch 69/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0605 - auc: 0.9163 - val_loss: 0.0767 - val_auc: 0.8284\n",
      "Epoch 70/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0603 - auc: 0.9258 - val_loss: 0.0778 - val_auc: 0.8379\n",
      "Epoch 71/1000\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.0569 - auc: 0.9385 - val_loss: 0.0798 - val_auc: 0.8252\n",
      "Epoch 72/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0593 - auc: 0.9265 - val_loss: 0.0793 - val_auc: 0.8250\n",
      "Epoch 73/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0580 - auc: 0.9247 - val_loss: 0.0780 - val_auc: 0.8409\n",
      "Epoch 74/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0597 - auc: 0.9258 - val_loss: 0.0801 - val_auc: 0.8331\n",
      "Epoch 75/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0597 - auc: 0.9101 - val_loss: 0.0798 - val_auc: 0.8454\n",
      "Epoch 76/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0601 - auc: 0.9211 - val_loss: 0.0806 - val_auc: 0.8512\n",
      "Epoch 77/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0555 - auc: 0.9358 - val_loss: 0.0793 - val_auc: 0.8449\n",
      "Epoch 78/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0560 - auc: 0.9398 - val_loss: 0.0793 - val_auc: 0.8431\n",
      "Epoch 79/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0587 - auc: 0.9320 - val_loss: 0.0793 - val_auc: 0.8401\n",
      "Epoch 80/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0561 - auc: 0.9408 - val_loss: 0.0799 - val_auc: 0.8290\n",
      "Epoch 81/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0530 - auc: 0.9406 - val_loss: 0.0826 - val_auc: 0.8199\n",
      "Epoch 82/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0537 - auc: 0.9330Restoring model weights from the end of the best epoch: 62.\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0537 - auc: 0.9330 - val_loss: 0.0811 - val_auc: 0.8302\n",
      "Epoch 82: early stopping\n",
      "\n",
      "Validation AUC: 83.02%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 26\n",
      "activation_dense: relu\n",
      "learning_rate: 9.4e-02\n",
      "l1_lambda: 1.0e-05\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 17s 112ms/step - loss: 0.1188 - auc: 0.4855 - val_loss: 0.0962 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1006 - auc: 0.4985 - val_loss: 0.0907 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.1000 - auc: 0.4381 - val_loss: 0.0906 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1003 - auc: 0.4365 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0987 - auc: 0.5214 - val_loss: 0.0903 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1008 - auc: 0.4761 - val_loss: 0.0919 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0983 - auc: 0.5590 - val_loss: 0.0981 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1004 - auc: 0.5185 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0991 - auc: 0.4756 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0987 - auc: 0.5126 - val_loss: 0.0903 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0990 - auc: 0.4684 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0987 - auc: 0.5262 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0997 - auc: 0.5054 - val_loss: 0.0911 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0985 - auc: 0.5135 - val_loss: 0.0906 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1008 - auc: 0.4808 - val_loss: 0.0994 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1038 - auc: 0.5015 - val_loss: 0.0928 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0985 - auc: 0.4824 - val_loss: 0.0906 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0988 - auc: 0.4577 - val_loss: 0.0897 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0998 - auc: 0.4762 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1007 - auc: 0.4897 - val_loss: 0.0919 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1001 - auc: 0.4952Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1001 - auc: 0.4952 - val_loss: 0.0912 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 5\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 2.4e-04\n",
      "l1_lambda: 1.0e-03\n",
      "l2_lambda: 1.0e-06\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 16s 112ms/step - loss: 0.5924 - auc: 0.4525 - val_loss: 0.4124 - val_auc: 0.3896\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.3416 - auc: 0.5119 - val_loss: 0.2713 - val_auc: 0.4162\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.2461 - auc: 0.5450 - val_loss: 0.2147 - val_auc: 0.3862\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.2086 - auc: 0.4138 - val_loss: 0.1899 - val_auc: 0.4665\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1894 - auc: 0.4897 - val_loss: 0.1748 - val_auc: 0.4496\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1765 - auc: 0.4630 - val_loss: 0.1630 - val_auc: 0.4626\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1661 - auc: 0.4504 - val_loss: 0.1534 - val_auc: 0.4946\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1571 - auc: 0.4924 - val_loss: 0.1449 - val_auc: 0.5025\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1491 - auc: 0.4955 - val_loss: 0.1374 - val_auc: 0.4826\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1418 - auc: 0.5200 - val_loss: 0.1307 - val_auc: 0.5595\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1344 - auc: 0.6504 - val_loss: 0.1235 - val_auc: 0.6940\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1281 - auc: 0.6984 - val_loss: 0.1177 - val_auc: 0.7391\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1223 - auc: 0.7493 - val_loss: 0.1126 - val_auc: 0.7296\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1176 - auc: 0.7485 - val_loss: 0.1085 - val_auc: 0.6958\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1130 - auc: 0.7653 - val_loss: 0.1042 - val_auc: 0.7591\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1085 - auc: 0.7698 - val_loss: 0.1004 - val_auc: 0.7782\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1027 - auc: 0.8098 - val_loss: 0.0968 - val_auc: 0.7899\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0984 - auc: 0.8262 - val_loss: 0.0929 - val_auc: 0.7895\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0941 - auc: 0.8082 - val_loss: 0.0914 - val_auc: 0.7651\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0896 - auc: 0.8587 - val_loss: 0.0884 - val_auc: 0.7952\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0851 - auc: 0.8562 - val_loss: 0.0875 - val_auc: 0.8020\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0820 - auc: 0.8576 - val_loss: 0.0864 - val_auc: 0.7917\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0791 - auc: 0.8668 - val_loss: 0.0844 - val_auc: 0.7925\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0768 - auc: 0.8657 - val_loss: 0.0845 - val_auc: 0.7766\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0746 - auc: 0.8696 - val_loss: 0.0848 - val_auc: 0.7691\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.0724 - auc: 0.8816 - val_loss: 0.0822 - val_auc: 0.7811\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0734 - auc: 0.8705 - val_loss: 0.0812 - val_auc: 0.7904\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.0695 - auc: 0.8850 - val_loss: 0.0827 - val_auc: 0.8033\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0694 - auc: 0.8964 - val_loss: 0.0812 - val_auc: 0.8316\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0677 - auc: 0.9010 - val_loss: 0.0805 - val_auc: 0.8442\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0666 - auc: 0.9058 - val_loss: 0.0816 - val_auc: 0.7948\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0644 - auc: 0.9157 - val_loss: 0.0812 - val_auc: 0.7960\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0642 - auc: 0.8990 - val_loss: 0.0809 - val_auc: 0.7959\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0632 - auc: 0.9078 - val_loss: 0.0809 - val_auc: 0.8095\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0647 - auc: 0.9098 - val_loss: 0.0814 - val_auc: 0.8000\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0628 - auc: 0.9076 - val_loss: 0.0842 - val_auc: 0.8074\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0609 - auc: 0.9303 - val_loss: 0.0842 - val_auc: 0.8131\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0623 - auc: 0.9242 - val_loss: 0.0813 - val_auc: 0.8267\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0619 - auc: 0.9133 - val_loss: 0.0829 - val_auc: 0.8053\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0600 - auc: 0.9272 - val_loss: 0.0843 - val_auc: 0.8132\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0595 - auc: 0.9269 - val_loss: 0.0852 - val_auc: 0.8152\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0590 - auc: 0.9369 - val_loss: 0.0839 - val_auc: 0.8146\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0571 - auc: 0.9375 - val_loss: 0.0844 - val_auc: 0.8165\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0581 - auc: 0.9321 - val_loss: 0.0840 - val_auc: 0.8144\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0587 - auc: 0.9210 - val_loss: 0.0845 - val_auc: 0.8150\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0574 - auc: 0.9313 - val_loss: 0.0852 - val_auc: 0.8176\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0560 - auc: 0.9335 - val_loss: 0.0854 - val_auc: 0.8118\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0554 - auc: 0.9423 - val_loss: 0.0866 - val_auc: 0.8158\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0545 - auc: 0.9422 - val_loss: 0.0868 - val_auc: 0.8021\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0560 - auc: 0.9529Restoring model weights from the end of the best epoch: 30.\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0560 - auc: 0.9529 - val_loss: 0.0864 - val_auc: 0.8060\n",
      "Epoch 50: early stopping\n",
      "\n",
      "Validation AUC: 80.60%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.5\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 13\n",
      "activation_dense: sigmoid\n",
      "learning_rate: 1.9e-04\n",
      "l1_lambda: 1.0e-03\n",
      "l2_lambda: 1.0e-06\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 16s 110ms/step - loss: 0.7098 - auc: 0.4296 - val_loss: 0.5533 - val_auc: 0.3297\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.4790 - auc: 0.4472 - val_loss: 0.3903 - val_auc: 0.4102\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.3493 - auc: 0.4476 - val_loss: 0.2979 - val_auc: 0.3737\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.2784 - auc: 0.4748 - val_loss: 0.2477 - val_auc: 0.4559\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2377 - auc: 0.4420 - val_loss: 0.2153 - val_auc: 0.4385\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2106 - auc: 0.4881 - val_loss: 0.1935 - val_auc: 0.4180\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1924 - auc: 0.4734 - val_loss: 0.1782 - val_auc: 0.4459\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1792 - auc: 0.4857 - val_loss: 0.1665 - val_auc: 0.4707\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1688 - auc: 0.4784 - val_loss: 0.1570 - val_auc: 0.4678\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1602 - auc: 0.4880 - val_loss: 0.1491 - val_auc: 0.4781\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1529 - auc: 0.4579 - val_loss: 0.1420 - val_auc: 0.4971\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1464 - auc: 0.4951 - val_loss: 0.1359 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1408 - auc: 0.4676 - val_loss: 0.1304 - val_auc: 0.4882\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1357 - auc: 0.4319 - val_loss: 0.1256 - val_auc: 0.4950\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1312 - auc: 0.4824 - val_loss: 0.1213 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1272 - auc: 0.4979 - val_loss: 0.1175 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1236 - auc: 0.4990 - val_loss: 0.1141 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1204 - auc: 0.4996 - val_loss: 0.1110 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1175 - auc: 0.4999 - val_loss: 0.1083 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1150 - auc: 0.5002 - val_loss: 0.1059 - val_auc: 0.5008\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1127 - auc: 0.5000 - val_loss: 0.1037 - val_auc: 0.5004\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.1105 - auc: 0.5051 - val_loss: 0.1017 - val_auc: 0.5264\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1086 - auc: 0.5145 - val_loss: 0.0999 - val_auc: 0.5297\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1069 - auc: 0.5066 - val_loss: 0.0983 - val_auc: 0.5285\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1054 - auc: 0.5685 - val_loss: 0.0968 - val_auc: 0.6250\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1039 - auc: 0.5854 - val_loss: 0.0954 - val_auc: 0.6119\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1024 - auc: 0.5929 - val_loss: 0.0942 - val_auc: 0.6407\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.1012 - auc: 0.6257 - val_loss: 0.0932 - val_auc: 0.6795\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1003 - auc: 0.6619 - val_loss: 0.0924 - val_auc: 0.6808\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0995 - auc: 0.7042 - val_loss: 0.0916 - val_auc: 0.6952\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0988 - auc: 0.7152 - val_loss: 0.0910 - val_auc: 0.7276\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0976 - auc: 0.7037 - val_loss: 0.0896 - val_auc: 0.7123\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0955 - auc: 0.7819 - val_loss: 0.0886 - val_auc: 0.7139\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0940 - auc: 0.8114 - val_loss: 0.0878 - val_auc: 0.7399\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0928 - auc: 0.8168 - val_loss: 0.0867 - val_auc: 0.7471\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0899 - auc: 0.8235 - val_loss: 0.0847 - val_auc: 0.7412\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0872 - auc: 0.8255 - val_loss: 0.0833 - val_auc: 0.7724\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 2s 38ms/step - loss: 0.0850 - auc: 0.8381 - val_loss: 0.0822 - val_auc: 0.7883\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0821 - auc: 0.8443 - val_loss: 0.0817 - val_auc: 0.7626\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0816 - auc: 0.8153 - val_loss: 0.0809 - val_auc: 0.7651\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0803 - auc: 0.8347 - val_loss: 0.0803 - val_auc: 0.7758\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0778 - auc: 0.8588 - val_loss: 0.0798 - val_auc: 0.7913\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0764 - auc: 0.8641 - val_loss: 0.0799 - val_auc: 0.8017\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0753 - auc: 0.8654 - val_loss: 0.0791 - val_auc: 0.8107\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0766 - auc: 0.8482 - val_loss: 0.0792 - val_auc: 0.7943\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0745 - auc: 0.8679 - val_loss: 0.0788 - val_auc: 0.8079\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0731 - auc: 0.8558 - val_loss: 0.0789 - val_auc: 0.7976\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0726 - auc: 0.8657 - val_loss: 0.0794 - val_auc: 0.7922\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0709 - auc: 0.8666 - val_loss: 0.0790 - val_auc: 0.7910\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0708 - auc: 0.8714 - val_loss: 0.0795 - val_auc: 0.8038\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0692 - auc: 0.8661 - val_loss: 0.0794 - val_auc: 0.8035\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0721 - auc: 0.8466 - val_loss: 0.0803 - val_auc: 0.7352\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0685 - auc: 0.8699 - val_loss: 0.0790 - val_auc: 0.8088\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0665 - auc: 0.8946 - val_loss: 0.0800 - val_auc: 0.7743\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0673 - auc: 0.8715 - val_loss: 0.0794 - val_auc: 0.8051\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0664 - auc: 0.8860 - val_loss: 0.0802 - val_auc: 0.7792\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0685 - auc: 0.8693 - val_loss: 0.0803 - val_auc: 0.7678\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0669 - auc: 0.8932 - val_loss: 0.0814 - val_auc: 0.7664\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0670 - auc: 0.8914 - val_loss: 0.0800 - val_auc: 0.7815\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0648 - auc: 0.8819 - val_loss: 0.0801 - val_auc: 0.7756\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0641 - auc: 0.8962 - val_loss: 0.0802 - val_auc: 0.7864\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0647 - auc: 0.8982 - val_loss: 0.0806 - val_auc: 0.7776\n",
      "Epoch 63/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0631 - auc: 0.8817 - val_loss: 0.0807 - val_auc: 0.7853\n",
      "Epoch 64/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0624 - auc: 0.9038Restoring model weights from the end of the best epoch: 44.\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0624 - auc: 0.9038 - val_loss: 0.0813 - val_auc: 0.7939\n",
      "Epoch 64: early stopping\n",
      "\n",
      "Validation AUC: 79.39%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 17\n",
      "activation_dense: relu\n",
      "learning_rate: 2.1e-04\n",
      "l1_lambda: 1.0e-03\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 17s 113ms/step - loss: 0.6438 - auc: 0.5443 - val_loss: 0.4478 - val_auc: 0.3598\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.3718 - auc: 0.4692 - val_loss: 0.2955 - val_auc: 0.3848\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.2695 - auc: 0.4769 - val_loss: 0.2353 - val_auc: 0.4046\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.2263 - auc: 0.4705 - val_loss: 0.2057 - val_auc: 0.4460\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.2034 - auc: 0.4426 - val_loss: 0.1876 - val_auc: 0.4502\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1882 - auc: 0.4285 - val_loss: 0.1744 - val_auc: 0.4847\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1766 - auc: 0.4922 - val_loss: 0.1641 - val_auc: 0.4682\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1672 - auc: 0.4866 - val_loss: 0.1552 - val_auc: 0.4602\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1591 - auc: 0.4707 - val_loss: 0.1476 - val_auc: 0.4847\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1519 - auc: 0.5011 - val_loss: 0.1409 - val_auc: 0.4992\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1450 - auc: 0.5437 - val_loss: 0.1339 - val_auc: 0.6276\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1381 - auc: 0.6977 - val_loss: 0.1280 - val_auc: 0.5894\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1323 - auc: 0.7086 - val_loss: 0.1222 - val_auc: 0.7455\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.1271 - auc: 0.7543 - val_loss: 0.1171 - val_auc: 0.7098\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1222 - auc: 0.7563 - val_loss: 0.1125 - val_auc: 0.7311\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1179 - auc: 0.7472 - val_loss: 0.1084 - val_auc: 0.7321\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1131 - auc: 0.7927 - val_loss: 0.1048 - val_auc: 0.7524\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1093 - auc: 0.7881 - val_loss: 0.1009 - val_auc: 0.7622\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1046 - auc: 0.8141 - val_loss: 0.0972 - val_auc: 0.7838\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0998 - auc: 0.8183 - val_loss: 0.0930 - val_auc: 0.7817\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0957 - auc: 0.8306 - val_loss: 0.0899 - val_auc: 0.8017\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0919 - auc: 0.8342 - val_loss: 0.0876 - val_auc: 0.7860\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0888 - auc: 0.8470 - val_loss: 0.0857 - val_auc: 0.7967\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0855 - auc: 0.8672 - val_loss: 0.0842 - val_auc: 0.7966\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0835 - auc: 0.8596 - val_loss: 0.0821 - val_auc: 0.7969\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0799 - auc: 0.8783 - val_loss: 0.0810 - val_auc: 0.8078\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0778 - auc: 0.8882 - val_loss: 0.0802 - val_auc: 0.8181\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0768 - auc: 0.8649 - val_loss: 0.0798 - val_auc: 0.7964\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0761 - auc: 0.8740 - val_loss: 0.0784 - val_auc: 0.8075\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0737 - auc: 0.8747 - val_loss: 0.0785 - val_auc: 0.8022\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0721 - auc: 0.8701 - val_loss: 0.0783 - val_auc: 0.8019\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0719 - auc: 0.8915 - val_loss: 0.0770 - val_auc: 0.8130\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0691 - auc: 0.8878 - val_loss: 0.0781 - val_auc: 0.8061\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0673 - auc: 0.9103 - val_loss: 0.0773 - val_auc: 0.8453\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0691 - auc: 0.8872 - val_loss: 0.0785 - val_auc: 0.8037\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0683 - auc: 0.8851 - val_loss: 0.0774 - val_auc: 0.8335\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0648 - auc: 0.9181 - val_loss: 0.0769 - val_auc: 0.8370\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0646 - auc: 0.9037 - val_loss: 0.0769 - val_auc: 0.8159\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0645 - auc: 0.9096 - val_loss: 0.0766 - val_auc: 0.8151\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0626 - auc: 0.9158 - val_loss: 0.0779 - val_auc: 0.8069\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0630 - auc: 0.9240 - val_loss: 0.0787 - val_auc: 0.8116\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0617 - auc: 0.9239 - val_loss: 0.0784 - val_auc: 0.8208\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0606 - auc: 0.9027 - val_loss: 0.0786 - val_auc: 0.8217\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0620 - auc: 0.9255 - val_loss: 0.0787 - val_auc: 0.8229\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0620 - auc: 0.9173 - val_loss: 0.0785 - val_auc: 0.8197\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0617 - auc: 0.9135 - val_loss: 0.0806 - val_auc: 0.8204\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0591 - auc: 0.9228 - val_loss: 0.0819 - val_auc: 0.8216\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0616 - auc: 0.9086 - val_loss: 0.0821 - val_auc: 0.8249\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0616 - auc: 0.9169 - val_loss: 0.0818 - val_auc: 0.8257\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0570 - auc: 0.9355 - val_loss: 0.0815 - val_auc: 0.8154\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0595 - auc: 0.9260 - val_loss: 0.0828 - val_auc: 0.8270\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0589 - auc: 0.9251 - val_loss: 0.0816 - val_auc: 0.8153\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0562 - auc: 0.9441 - val_loss: 0.0825 - val_auc: 0.8179\n",
      "Epoch 54/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0552 - auc: 0.9334Restoring model weights from the end of the best epoch: 34.\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0564 - auc: 0.9319 - val_loss: 0.0824 - val_auc: 0.8179\n",
      "Epoch 54: early stopping\n",
      "\n",
      "Validation AUC: 81.79%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 5\n",
      "activation_dense: relu\n",
      "learning_rate: 1.9e-04\n",
      "l1_lambda: 1.0e-06\n",
      "l2_lambda: 1.0e-04\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 15s 105ms/step - loss: 0.5969 - auc: 0.5161 - val_loss: 0.4613 - val_auc: 0.4395\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.3782 - auc: 0.4997 - val_loss: 0.2858 - val_auc: 0.4398\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.2388 - auc: 0.5156 - val_loss: 0.1882 - val_auc: 0.4208\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1688 - auc: 0.4938 - val_loss: 0.1423 - val_auc: 0.4766\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1361 - auc: 0.5017 - val_loss: 0.1202 - val_auc: 0.4389\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1203 - auc: 0.4465 - val_loss: 0.1086 - val_auc: 0.4744\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1118 - auc: 0.5104 - val_loss: 0.1023 - val_auc: 0.4572\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1070 - auc: 0.4691 - val_loss: 0.0984 - val_auc: 0.4909\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1040 - auc: 0.4567 - val_loss: 0.0959 - val_auc: 0.4794\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1022 - auc: 0.4935 - val_loss: 0.0943 - val_auc: 0.4508\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1010 - auc: 0.4567 - val_loss: 0.0933 - val_auc: 0.4983\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1002 - auc: 0.4968 - val_loss: 0.0925 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0997 - auc: 0.4927 - val_loss: 0.0919 - val_auc: 0.4402\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 38ms/step - loss: 0.0993 - auc: 0.4987 - val_loss: 0.0916 - val_auc: 0.4975\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0991 - auc: 0.4964 - val_loss: 0.0913 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0989 - auc: 0.4987 - val_loss: 0.0911 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0988 - auc: 0.4996 - val_loss: 0.0910 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0987 - auc: 0.4999 - val_loss: 0.0908 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0986 - auc: 0.4999 - val_loss: 0.0907 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0985 - auc: 0.4959 - val_loss: 0.0906 - val_auc: 0.5004\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0985 - auc: 0.5006 - val_loss: 0.0906 - val_auc: 0.4793\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0984 - auc: 0.4733 - val_loss: 0.0905 - val_auc: 0.4565\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0983 - auc: 0.5166 - val_loss: 0.0904 - val_auc: 0.5208\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0983 - auc: 0.5289 - val_loss: 0.0904 - val_auc: 0.5138\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0981 - auc: 0.5629 - val_loss: 0.0903 - val_auc: 0.5196\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0980 - auc: 0.5237 - val_loss: 0.0903 - val_auc: 0.5486\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0971 - auc: 0.6380 - val_loss: 0.0891 - val_auc: 0.5971\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0948 - auc: 0.7281 - val_loss: 0.0883 - val_auc: 0.6789\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0927 - auc: 0.7740 - val_loss: 0.0862 - val_auc: 0.7752\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0901 - auc: 0.7549 - val_loss: 0.0846 - val_auc: 0.7755\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0882 - auc: 0.8014 - val_loss: 0.0837 - val_auc: 0.7799\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0857 - auc: 0.8157 - val_loss: 0.0835 - val_auc: 0.7692\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0837 - auc: 0.8123 - val_loss: 0.0826 - val_auc: 0.7648\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0811 - auc: 0.8349 - val_loss: 0.0819 - val_auc: 0.7764\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0798 - auc: 0.8423 - val_loss: 0.0815 - val_auc: 0.7817\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0783 - auc: 0.8471 - val_loss: 0.0812 - val_auc: 0.7717\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0774 - auc: 0.8341 - val_loss: 0.0810 - val_auc: 0.7993\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0742 - auc: 0.8615 - val_loss: 0.0808 - val_auc: 0.8070\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0751 - auc: 0.8355 - val_loss: 0.0807 - val_auc: 0.7892\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0727 - auc: 0.8853 - val_loss: 0.0807 - val_auc: 0.7788\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0687 - auc: 0.8762 - val_loss: 0.0807 - val_auc: 0.7825\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0714 - auc: 0.8623 - val_loss: 0.0824 - val_auc: 0.7451\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0687 - auc: 0.8701 - val_loss: 0.0848 - val_auc: 0.7747\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0688 - auc: 0.8647 - val_loss: 0.0809 - val_auc: 0.8159\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0655 - auc: 0.8831 - val_loss: 0.0819 - val_auc: 0.7575\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0701 - auc: 0.8783 - val_loss: 0.0809 - val_auc: 0.7908\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0687 - auc: 0.8641 - val_loss: 0.0807 - val_auc: 0.8026\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0647 - auc: 0.8984 - val_loss: 0.0815 - val_auc: 0.7748\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0645 - auc: 0.8814 - val_loss: 0.0820 - val_auc: 0.8054\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0644 - auc: 0.8897 - val_loss: 0.0830 - val_auc: 0.7503\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0659 - auc: 0.8865 - val_loss: 0.0828 - val_auc: 0.7523\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0650 - auc: 0.8845 - val_loss: 0.0847 - val_auc: 0.7984\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0634 - auc: 0.8819 - val_loss: 0.0838 - val_auc: 0.7867\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0655 - auc: 0.8682 - val_loss: 0.0836 - val_auc: 0.7858\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0584 - auc: 0.8986 - val_loss: 0.0838 - val_auc: 0.7873\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0609 - auc: 0.9057 - val_loss: 0.0842 - val_auc: 0.7711\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0592 - auc: 0.9106 - val_loss: 0.0849 - val_auc: 0.7695\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0602 - auc: 0.9163 - val_loss: 0.0855 - val_auc: 0.7769\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0591 - auc: 0.9049 - val_loss: 0.0866 - val_auc: 0.7909\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0568 - auc: 0.9032 - val_loss: 0.0876 - val_auc: 0.7796\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0562 - auc: 0.9141 - val_loss: 0.0865 - val_auc: 0.7918\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0588 - auc: 0.9205 - val_loss: 0.0855 - val_auc: 0.7855\n",
      "Epoch 63/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0583 - auc: 0.9143 - val_loss: 0.0857 - val_auc: 0.7914\n",
      "Epoch 64/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0576 - auc: 0.9101Restoring model weights from the end of the best epoch: 44.\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0576 - auc: 0.9101 - val_loss: 0.0863 - val_auc: 0.7997\n",
      "Epoch 64: early stopping\n",
      "\n",
      "Validation AUC: 79.97%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 28\n",
      "activation_dense: relu\n",
      "learning_rate: 2.2e-04\n",
      "l1_lambda: 1.0e-01\n",
      "l2_lambda: 1.0e-05\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 16s 106ms/step - loss: 14.2773 - auc: 0.4811 - val_loss: 13.4370 - val_auc: 0.3572\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 12.7376 - auc: 0.4709 - val_loss: 12.0068 - val_auc: 0.3901\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 11.3995 - auc: 0.4541 - val_loss: 10.7555 - val_auc: 0.4082\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 10.2063 - auc: 0.4519 - val_loss: 9.6208 - val_auc: 0.4503\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 9.1210 - auc: 0.4930 - val_loss: 8.5848 - val_auc: 0.4595\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 8.1265 - auc: 0.5491 - val_loss: 7.6337 - val_auc: 0.4608\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 7.2152 - auc: 0.4512 - val_loss: 6.7643 - val_auc: 0.4950\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 6.3869 - auc: 0.5058 - val_loss: 5.9779 - val_auc: 0.4967\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 5.6370 - auc: 0.4430 - val_loss: 5.2655 - val_auc: 0.4839\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 4.9554 - auc: 0.4875 - val_loss: 4.6148 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 4.3353 - auc: 0.4908 - val_loss: 4.0298 - val_auc: 0.4499\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 3.7817 - auc: 0.4450 - val_loss: 3.5097 - val_auc: 0.4909\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 3.2921 - auc: 0.4880 - val_loss: 3.0501 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 2.8556 - auc: 0.4983 - val_loss: 2.6380 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 2.4646 - auc: 0.4999 - val_loss: 2.2696 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 2.1192 - auc: 0.5000 - val_loss: 1.9488 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 1.8189 - auc: 0.5001 - val_loss: 1.6680 - val_auc: 0.4773\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 1.5560 - auc: 0.4958 - val_loss: 1.4244 - val_auc: 0.4190\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 1.3293 - auc: 0.4488 - val_loss: 1.2154 - val_auc: 0.4759\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 1.1347 - auc: 0.4471 - val_loss: 1.0356 - val_auc: 0.4883\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.9675 - auc: 0.4848 - val_loss: 0.8815 - val_auc: 0.5032\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.8240 - auc: 0.5043 - val_loss: 0.7486 - val_auc: 0.4564\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.6995 - auc: 0.5285 - val_loss: 0.6335 - val_auc: 0.5382\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.5925 - auc: 0.5464 - val_loss: 0.5349 - val_auc: 0.5286\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.5025 - auc: 0.5859 - val_loss: 0.4525 - val_auc: 0.5691\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.4258 - auc: 0.5892 - val_loss: 0.3837 - val_auc: 0.6092\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.3638 - auc: 0.6386 - val_loss: 0.3276 - val_auc: 0.5961\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.3106 - auc: 0.6755 - val_loss: 0.2791 - val_auc: 0.7349\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.2657 - auc: 0.7350 - val_loss: 0.2390 - val_auc: 0.7201\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.2298 - auc: 0.8187 - val_loss: 0.2078 - val_auc: 0.7491\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.2023 - auc: 0.8176 - val_loss: 0.1851 - val_auc: 0.7638\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1817 - auc: 0.8069 - val_loss: 0.1682 - val_auc: 0.7530\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1646 - auc: 0.8398 - val_loss: 0.1521 - val_auc: 0.7868\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1481 - auc: 0.8362 - val_loss: 0.1384 - val_auc: 0.7804\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1353 - auc: 0.8471 - val_loss: 0.1268 - val_auc: 0.7844\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1234 - auc: 0.8445 - val_loss: 0.1175 - val_auc: 0.7803\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1121 - auc: 0.8511 - val_loss: 0.1082 - val_auc: 0.7671\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1029 - auc: 0.8586 - val_loss: 0.1016 - val_auc: 0.7846\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0951 - auc: 0.8595 - val_loss: 0.0951 - val_auc: 0.8065\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0889 - auc: 0.8754 - val_loss: 0.0908 - val_auc: 0.7968\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0845 - auc: 0.8814 - val_loss: 0.0876 - val_auc: 0.7804\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0817 - auc: 0.8751 - val_loss: 0.0862 - val_auc: 0.7952\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0769 - auc: 0.8925 - val_loss: 0.0863 - val_auc: 0.7866\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0753 - auc: 0.8930 - val_loss: 0.0834 - val_auc: 0.7945\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0743 - auc: 0.8805 - val_loss: 0.0827 - val_auc: 0.8018\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0730 - auc: 0.8756 - val_loss: 0.0832 - val_auc: 0.7917\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0706 - auc: 0.8981 - val_loss: 0.0827 - val_auc: 0.8029\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0718 - auc: 0.8784 - val_loss: 0.0829 - val_auc: 0.7986\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0694 - auc: 0.8744 - val_loss: 0.0841 - val_auc: 0.8032\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0717 - auc: 0.8769 - val_loss: 0.0845 - val_auc: 0.7634\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0692 - auc: 0.9056 - val_loss: 0.0856 - val_auc: 0.7658\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0693 - auc: 0.8875 - val_loss: 0.0843 - val_auc: 0.7855\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0666 - auc: 0.9023 - val_loss: 0.0848 - val_auc: 0.7790\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0660 - auc: 0.9165 - val_loss: 0.0846 - val_auc: 0.7787\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0641 - auc: 0.9167 - val_loss: 0.0864 - val_auc: 0.7954\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0669 - auc: 0.9135 - val_loss: 0.0875 - val_auc: 0.8179\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0654 - auc: 0.9093 - val_loss: 0.0879 - val_auc: 0.8239\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0654 - auc: 0.9165 - val_loss: 0.0861 - val_auc: 0.8053\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0614 - auc: 0.9330 - val_loss: 0.0864 - val_auc: 0.8013\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0650 - auc: 0.9050 - val_loss: 0.0872 - val_auc: 0.7998\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0605 - auc: 0.9143 - val_loss: 0.0873 - val_auc: 0.8108\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0605 - auc: 0.9172 - val_loss: 0.0874 - val_auc: 0.8233\n",
      "Epoch 63/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0619 - auc: 0.9181 - val_loss: 0.0881 - val_auc: 0.7942\n",
      "Epoch 64/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0592 - auc: 0.9303 - val_loss: 0.0892 - val_auc: 0.7958\n",
      "Epoch 65/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0601 - auc: 0.9289 - val_loss: 0.0894 - val_auc: 0.8134\n",
      "Epoch 66/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0553 - auc: 0.9349 - val_loss: 0.0914 - val_auc: 0.7291\n",
      "Epoch 67/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0579 - auc: 0.9337 - val_loss: 0.0901 - val_auc: 0.8100\n",
      "Epoch 68/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0569 - auc: 0.9126 - val_loss: 0.0912 - val_auc: 0.7938\n",
      "Epoch 69/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0556 - auc: 0.9248 - val_loss: 0.0922 - val_auc: 0.8080\n",
      "Epoch 70/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0584 - auc: 0.9184 - val_loss: 0.0933 - val_auc: 0.7314\n",
      "Epoch 71/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0549 - auc: 0.9442 - val_loss: 0.0930 - val_auc: 0.7481\n",
      "Epoch 72/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0553 - auc: 0.9243 - val_loss: 0.0938 - val_auc: 0.7809\n",
      "Epoch 73/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0564 - auc: 0.9239 - val_loss: 0.0938 - val_auc: 0.7910\n",
      "Epoch 74/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0532 - auc: 0.9269 - val_loss: 0.0947 - val_auc: 0.7799\n",
      "Epoch 75/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0549 - auc: 0.9343 - val_loss: 0.0958 - val_auc: 0.7330\n",
      "Epoch 76/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0516 - auc: 0.9455 - val_loss: 0.0967 - val_auc: 0.7170\n",
      "Epoch 77/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0502 - auc: 0.9386Restoring model weights from the end of the best epoch: 57.\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0505 - auc: 0.9403 - val_loss: 0.0954 - val_auc: 0.7440\n",
      "Epoch 77: early stopping\n",
      "\n",
      "Validation AUC: 74.40%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 1\n",
      "activation_dense: relu\n",
      "learning_rate: 2.0e-04\n",
      "l1_lambda: 1.0e-05\n",
      "l2_lambda: 1.0e-06\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 16s 111ms/step - loss: 0.5616 - auc: 0.5312 - val_loss: 0.4009 - val_auc: 0.4443\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.3091 - auc: 0.5233 - val_loss: 0.2153 - val_auc: 0.4132\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1806 - auc: 0.4596 - val_loss: 0.1425 - val_auc: 0.4023\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1335 - auc: 0.4864 - val_loss: 0.1146 - val_auc: 0.3889\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1146 - auc: 0.3783 - val_loss: 0.1015 - val_auc: 0.4360\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1057 - auc: 0.4268 - val_loss: 0.0960 - val_auc: 0.4294\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1017 - auc: 0.4765 - val_loss: 0.0932 - val_auc: 0.4893\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1000 - auc: 0.4770 - val_loss: 0.0920 - val_auc: 0.4497\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0993 - auc: 0.5033 - val_loss: 0.0914 - val_auc: 0.4975\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0989 - auc: 0.4930 - val_loss: 0.0911 - val_auc: 0.5004\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0986 - auc: 0.5045 - val_loss: 0.0909 - val_auc: 0.5239\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0984 - auc: 0.5044 - val_loss: 0.0907 - val_auc: 0.5619\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0981 - auc: 0.6081 - val_loss: 0.0904 - val_auc: 0.6291\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0974 - auc: 0.6010 - val_loss: 0.0898 - val_auc: 0.5945\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0964 - auc: 0.7054 - val_loss: 0.0889 - val_auc: 0.6698\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0954 - auc: 0.7466 - val_loss: 0.0882 - val_auc: 0.7214\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0943 - auc: 0.7763 - val_loss: 0.0875 - val_auc: 0.7151\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0937 - auc: 0.7844 - val_loss: 0.0873 - val_auc: 0.7263\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0923 - auc: 0.8010 - val_loss: 0.0864 - val_auc: 0.7203\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0908 - auc: 0.8272 - val_loss: 0.0852 - val_auc: 0.7729\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0890 - auc: 0.8382 - val_loss: 0.0845 - val_auc: 0.7740\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0876 - auc: 0.8248 - val_loss: 0.0832 - val_auc: 0.7868\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0871 - auc: 0.8359 - val_loss: 0.0822 - val_auc: 0.7697\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0832 - auc: 0.8570 - val_loss: 0.0811 - val_auc: 0.7835\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0811 - auc: 0.8639 - val_loss: 0.0813 - val_auc: 0.8000\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0798 - auc: 0.8643 - val_loss: 0.0795 - val_auc: 0.8094\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0792 - auc: 0.8523 - val_loss: 0.0792 - val_auc: 0.7717\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0754 - auc: 0.8795 - val_loss: 0.0790 - val_auc: 0.7779\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0749 - auc: 0.8749 - val_loss: 0.0788 - val_auc: 0.7867\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0725 - auc: 0.8686 - val_loss: 0.0805 - val_auc: 0.7734\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0724 - auc: 0.8637 - val_loss: 0.0783 - val_auc: 0.8093\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 2s 38ms/step - loss: 0.0695 - auc: 0.8929 - val_loss: 0.0804 - val_auc: 0.7852\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0700 - auc: 0.8804 - val_loss: 0.0783 - val_auc: 0.8067\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0695 - auc: 0.8873 - val_loss: 0.0794 - val_auc: 0.8019\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0677 - auc: 0.9062 - val_loss: 0.0791 - val_auc: 0.8294\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0679 - auc: 0.8902 - val_loss: 0.0808 - val_auc: 0.7789\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0662 - auc: 0.8980 - val_loss: 0.0790 - val_auc: 0.8214\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0666 - auc: 0.8969 - val_loss: 0.0799 - val_auc: 0.7984\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0666 - auc: 0.8998 - val_loss: 0.0791 - val_auc: 0.7933\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 2s 38ms/step - loss: 0.0645 - auc: 0.9124 - val_loss: 0.0797 - val_auc: 0.8012\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0660 - auc: 0.8933 - val_loss: 0.0811 - val_auc: 0.7905\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0638 - auc: 0.9127 - val_loss: 0.0810 - val_auc: 0.8018\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0610 - auc: 0.9140 - val_loss: 0.0814 - val_auc: 0.8068\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0618 - auc: 0.9131 - val_loss: 0.0821 - val_auc: 0.8155\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0597 - auc: 0.9259 - val_loss: 0.0821 - val_auc: 0.8185\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0609 - auc: 0.9146 - val_loss: 0.0810 - val_auc: 0.8037\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0580 - auc: 0.9152 - val_loss: 0.0829 - val_auc: 0.8175\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0586 - auc: 0.9204 - val_loss: 0.0836 - val_auc: 0.8241\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0591 - auc: 0.9182 - val_loss: 0.0825 - val_auc: 0.8083\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0616 - auc: 0.9080 - val_loss: 0.0841 - val_auc: 0.8163\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0564 - auc: 0.9378 - val_loss: 0.0838 - val_auc: 0.8174\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0596 - auc: 0.9135 - val_loss: 0.0873 - val_auc: 0.7531\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0605 - auc: 0.9154 - val_loss: 0.0862 - val_auc: 0.8050\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0568 - auc: 0.9258 - val_loss: 0.0870 - val_auc: 0.7551\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0558 - auc: 0.9170Restoring model weights from the end of the best epoch: 35.\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0558 - auc: 0.9170 - val_loss: 0.0862 - val_auc: 0.7860\n",
      "Epoch 55: early stopping\n",
      "\n",
      "Validation AUC: 78.60%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.2\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 13\n",
      "activation_dense: relu\n",
      "learning_rate: 1.9e-04\n",
      "l1_lambda: 0.0e+00\n",
      "l2_lambda: 1.0e-04\n",
      "act_last: softmax\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 17s 113ms/step - loss: 0.6165 - auc: 0.5000 - val_loss: 0.5024 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.4127 - auc: 0.5000 - val_loss: 0.3156 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2614 - auc: 0.5000 - val_loss: 0.2094 - val_auc: 0.5000\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.1872 - auc: 0.5000 - val_loss: 0.1598 - val_auc: 0.5000\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1504 - auc: 0.5000 - val_loss: 0.1327 - val_auc: 0.5000\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1299 - auc: 0.5000 - val_loss: 0.1169 - val_auc: 0.5000\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1176 - auc: 0.5000 - val_loss: 0.1071 - val_auc: 0.5000\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1101 - auc: 0.5000 - val_loss: 0.1011 - val_auc: 0.5000\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1056 - auc: 0.5000 - val_loss: 0.0972 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1029 - auc: 0.5000 - val_loss: 0.0948 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1012 - auc: 0.5000 - val_loss: 0.0934 - val_auc: 0.5000\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1003 - auc: 0.5000 - val_loss: 0.0925 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0996 - auc: 0.5000 - val_loss: 0.0919 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0992 - auc: 0.5000 - val_loss: 0.0913 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0989 - auc: 0.5000 - val_loss: 0.0911 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0986 - auc: 0.5000 - val_loss: 0.0908 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0984 - auc: 0.5000 - val_loss: 0.0906 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0983 - auc: 0.5000 - val_loss: 0.0904 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0981 - auc: 0.5000 - val_loss: 0.0902 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0979 - auc: 0.5000 - val_loss: 0.0901 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0987 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 21: early stopping\n",
      "\n",
      "Validation AUC: 50.00%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.5\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 16\n",
      "activation_dense: relu\n",
      "learning_rate: 1.8e-04\n",
      "l1_lambda: 0.0e+00\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 18s 110ms/step - loss: 0.5921 - auc: 0.4922 - val_loss: 0.4658 - val_auc: 0.3695\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.3836 - auc: 0.4666 - val_loss: 0.2892 - val_auc: 0.4159\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.2438 - auc: 0.4999 - val_loss: 0.1887 - val_auc: 0.4260\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1681 - auc: 0.4375 - val_loss: 0.1387 - val_auc: 0.4013\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1326 - auc: 0.4436 - val_loss: 0.1160 - val_auc: 0.4464\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1164 - auc: 0.4061 - val_loss: 0.1045 - val_auc: 0.4189\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1081 - auc: 0.4412 - val_loss: 0.0985 - val_auc: 0.4232\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1037 - auc: 0.4713 - val_loss: 0.0952 - val_auc: 0.4523\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1012 - auc: 0.4515 - val_loss: 0.0931 - val_auc: 0.4802\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0998 - auc: 0.4567 - val_loss: 0.0919 - val_auc: 0.4992\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0989 - auc: 0.5005 - val_loss: 0.0911 - val_auc: 0.4798\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0984 - auc: 0.4327 - val_loss: 0.0906 - val_auc: 0.4785\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0982 - auc: 0.4894 - val_loss: 0.0902 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0980 - auc: 0.4950 - val_loss: 0.0901 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0978 - auc: 0.4992 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0978 - auc: 0.4997 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0977 - auc: 0.4952 - val_loss: 0.0898 - val_auc: 0.4731\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0977 - auc: 0.4857 - val_loss: 0.0898 - val_auc: 0.4669\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0976 - auc: 0.5156 - val_loss: 0.0897 - val_auc: 0.5239\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0974 - auc: 0.5328 - val_loss: 0.0896 - val_auc: 0.5874\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0973 - auc: 0.5951 - val_loss: 0.0895 - val_auc: 0.5625\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0973 - auc: 0.5897 - val_loss: 0.0895 - val_auc: 0.6134\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0972 - auc: 0.6073 - val_loss: 0.0894 - val_auc: 0.6278\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0971 - auc: 0.6426 - val_loss: 0.0894 - val_auc: 0.6220\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0970 - auc: 0.6560 - val_loss: 0.0893 - val_auc: 0.6717\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0966 - auc: 0.6246 - val_loss: 0.0890 - val_auc: 0.6692\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0963 - auc: 0.6461 - val_loss: 0.0888 - val_auc: 0.6717\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0960 - auc: 0.6700 - val_loss: 0.0886 - val_auc: 0.7010\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0957 - auc: 0.7119 - val_loss: 0.0884 - val_auc: 0.7160\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0955 - auc: 0.7467 - val_loss: 0.0881 - val_auc: 0.7241\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0949 - auc: 0.7423 - val_loss: 0.0874 - val_auc: 0.7321\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0935 - auc: 0.7841 - val_loss: 0.0862 - val_auc: 0.7357\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0910 - auc: 0.8161 - val_loss: 0.0852 - val_auc: 0.7546\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0901 - auc: 0.8085 - val_loss: 0.0836 - val_auc: 0.7858\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0886 - auc: 0.8104 - val_loss: 0.0827 - val_auc: 0.7911\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0867 - auc: 0.8224 - val_loss: 0.0820 - val_auc: 0.8211\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0864 - auc: 0.8147 - val_loss: 0.0815 - val_auc: 0.7923\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0842 - auc: 0.8315 - val_loss: 0.0812 - val_auc: 0.7682\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0831 - auc: 0.8335 - val_loss: 0.0809 - val_auc: 0.7746\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0816 - auc: 0.8385 - val_loss: 0.0799 - val_auc: 0.7784\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0796 - auc: 0.8499 - val_loss: 0.0793 - val_auc: 0.7869\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0788 - auc: 0.8447 - val_loss: 0.0790 - val_auc: 0.8104\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0769 - auc: 0.8611 - val_loss: 0.0786 - val_auc: 0.8227\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0756 - auc: 0.8609 - val_loss: 0.0783 - val_auc: 0.8287\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0755 - auc: 0.8552 - val_loss: 0.0786 - val_auc: 0.7860\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0741 - auc: 0.8635 - val_loss: 0.0786 - val_auc: 0.7950\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0746 - auc: 0.8599 - val_loss: 0.0787 - val_auc: 0.7968\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0745 - auc: 0.8627 - val_loss: 0.0788 - val_auc: 0.8029\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0702 - auc: 0.8802 - val_loss: 0.0782 - val_auc: 0.8018\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0723 - auc: 0.8772 - val_loss: 0.0783 - val_auc: 0.7865\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0726 - auc: 0.8689 - val_loss: 0.0775 - val_auc: 0.8022\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0703 - auc: 0.8720 - val_loss: 0.0785 - val_auc: 0.7899\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0696 - auc: 0.8744 - val_loss: 0.0791 - val_auc: 0.8009\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0687 - auc: 0.8838 - val_loss: 0.0786 - val_auc: 0.8153\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0695 - auc: 0.8755 - val_loss: 0.0779 - val_auc: 0.8133\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0671 - auc: 0.8866 - val_loss: 0.0787 - val_auc: 0.8184\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0663 - auc: 0.8971 - val_loss: 0.0793 - val_auc: 0.8375\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0656 - auc: 0.8890 - val_loss: 0.0803 - val_auc: 0.8066\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0663 - auc: 0.9000 - val_loss: 0.0783 - val_auc: 0.8187\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 2s 38ms/step - loss: 0.0665 - auc: 0.8891 - val_loss: 0.0793 - val_auc: 0.8311\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0640 - auc: 0.9099 - val_loss: 0.0814 - val_auc: 0.8042\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0669 - auc: 0.8966 - val_loss: 0.0797 - val_auc: 0.8201\n",
      "Epoch 63/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0654 - auc: 0.8921 - val_loss: 0.0784 - val_auc: 0.8276\n",
      "Epoch 64/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0651 - auc: 0.9044 - val_loss: 0.0810 - val_auc: 0.8124\n",
      "Epoch 65/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0653 - auc: 0.8801 - val_loss: 0.0804 - val_auc: 0.8219\n",
      "Epoch 66/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0651 - auc: 0.9065 - val_loss: 0.0797 - val_auc: 0.8157\n",
      "Epoch 67/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0617 - auc: 0.9142 - val_loss: 0.0799 - val_auc: 0.8231\n",
      "Epoch 68/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0625 - auc: 0.8983 - val_loss: 0.0802 - val_auc: 0.8248\n",
      "Epoch 69/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0645 - auc: 0.9070 - val_loss: 0.0819 - val_auc: 0.8035\n",
      "Epoch 70/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0605 - auc: 0.9081 - val_loss: 0.0817 - val_auc: 0.8008\n",
      "Epoch 71/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0606 - auc: 0.9312 - val_loss: 0.0826 - val_auc: 0.8108\n",
      "Epoch 72/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0602 - auc: 0.9276 - val_loss: 0.0841 - val_auc: 0.8217\n",
      "Epoch 73/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0583 - auc: 0.9236 - val_loss: 0.0831 - val_auc: 0.8074\n",
      "Epoch 74/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0600 - auc: 0.9090 - val_loss: 0.0845 - val_auc: 0.8235\n",
      "Epoch 75/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0586 - auc: 0.9168 - val_loss: 0.0849 - val_auc: 0.8238\n",
      "Epoch 76/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0554 - auc: 0.9338 - val_loss: 0.0859 - val_auc: 0.8303\n",
      "Epoch 77/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0601 - auc: 0.9280Restoring model weights from the end of the best epoch: 57.\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0597 - auc: 0.9277 - val_loss: 0.0844 - val_auc: 0.8133\n",
      "Epoch 77: early stopping\n",
      "\n",
      "Validation AUC: 81.33%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 1\n",
      "activation_dense: relu\n",
      "learning_rate: 1.8e-04\n",
      "l1_lambda: 0.0e+00\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 16s 108ms/step - loss: 0.5625 - auc: 0.4564 - val_loss: 0.4268 - val_auc: 0.4228\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.3550 - auc: 0.4493 - val_loss: 0.2714 - val_auc: 0.4042\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.2343 - auc: 0.5199 - val_loss: 0.1906 - val_auc: 0.4077\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1737 - auc: 0.4639 - val_loss: 0.1479 - val_auc: 0.4156\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.1413 - auc: 0.4783 - val_loss: 0.1243 - val_auc: 0.3696\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1231 - auc: 0.4688 - val_loss: 0.1106 - val_auc: 0.4699\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1128 - auc: 0.4468 - val_loss: 0.1026 - val_auc: 0.4913\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1068 - auc: 0.4805 - val_loss: 0.0980 - val_auc: 0.4432\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1034 - auc: 0.4652 - val_loss: 0.0952 - val_auc: 0.4379\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1013 - auc: 0.5015 - val_loss: 0.0935 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1001 - auc: 0.4336 - val_loss: 0.0922 - val_auc: 0.4872\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0992 - auc: 0.4827 - val_loss: 0.0915 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0987 - auc: 0.5011 - val_loss: 0.0910 - val_auc: 0.4429\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0983 - auc: 0.4796 - val_loss: 0.0907 - val_auc: 0.4872\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0981 - auc: 0.4847 - val_loss: 0.0904 - val_auc: 0.4996\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0979 - auc: 0.4970 - val_loss: 0.0902 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0978 - auc: 0.4993 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0977 - auc: 0.4999 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0976 - auc: 0.4999 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0976 - auc: 0.5000 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0975 - auc: 0.4959 - val_loss: 0.0897 - val_auc: 0.5087\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0973 - auc: 0.5070 - val_loss: 0.0895 - val_auc: 0.5524\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0971 - auc: 0.5129 - val_loss: 0.0894 - val_auc: 0.5462\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0968 - auc: 0.5724 - val_loss: 0.0892 - val_auc: 0.6378\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0963 - auc: 0.5969 - val_loss: 0.0888 - val_auc: 0.6158\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0953 - auc: 0.7306 - val_loss: 0.0883 - val_auc: 0.6595\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0948 - auc: 0.7355 - val_loss: 0.0879 - val_auc: 0.7269\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0943 - auc: 0.7694 - val_loss: 0.0874 - val_auc: 0.7456\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0936 - auc: 0.7712 - val_loss: 0.0867 - val_auc: 0.7574\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0928 - auc: 0.7699 - val_loss: 0.0861 - val_auc: 0.7599\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0918 - auc: 0.8114 - val_loss: 0.0856 - val_auc: 0.7817\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0904 - auc: 0.8290 - val_loss: 0.0850 - val_auc: 0.7783\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0898 - auc: 0.8176 - val_loss: 0.0844 - val_auc: 0.7847\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0879 - auc: 0.8500 - val_loss: 0.0838 - val_auc: 0.7956\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0868 - auc: 0.8317 - val_loss: 0.0829 - val_auc: 0.8103\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0847 - auc: 0.8561 - val_loss: 0.0817 - val_auc: 0.7979\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0838 - auc: 0.8361 - val_loss: 0.0808 - val_auc: 0.8036\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0808 - auc: 0.8737 - val_loss: 0.0808 - val_auc: 0.7893\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0809 - auc: 0.8525 - val_loss: 0.0797 - val_auc: 0.7885\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0779 - auc: 0.8727 - val_loss: 0.0796 - val_auc: 0.7851\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0781 - auc: 0.8644 - val_loss: 0.0796 - val_auc: 0.8057\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0761 - auc: 0.8783 - val_loss: 0.0796 - val_auc: 0.8060\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0755 - auc: 0.8587 - val_loss: 0.0784 - val_auc: 0.8279\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0745 - auc: 0.8787 - val_loss: 0.0797 - val_auc: 0.8222\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0735 - auc: 0.8994 - val_loss: 0.0785 - val_auc: 0.8074\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0705 - auc: 0.8998 - val_loss: 0.0785 - val_auc: 0.7934\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0703 - auc: 0.8999 - val_loss: 0.0796 - val_auc: 0.7893\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0706 - auc: 0.8954 - val_loss: 0.0786 - val_auc: 0.7880\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0699 - auc: 0.8960 - val_loss: 0.0784 - val_auc: 0.7887\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0700 - auc: 0.8807 - val_loss: 0.0789 - val_auc: 0.7931\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0683 - auc: 0.8993 - val_loss: 0.0788 - val_auc: 0.7953\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0648 - auc: 0.9224 - val_loss: 0.0802 - val_auc: 0.8028\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0646 - auc: 0.9023 - val_loss: 0.0802 - val_auc: 0.8107\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0649 - auc: 0.9032 - val_loss: 0.0799 - val_auc: 0.8166\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0642 - auc: 0.9228 - val_loss: 0.0816 - val_auc: 0.8160\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0663 - auc: 0.9055 - val_loss: 0.0800 - val_auc: 0.8157\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0669 - auc: 0.8925 - val_loss: 0.0806 - val_auc: 0.8150\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0646 - auc: 0.9069 - val_loss: 0.0816 - val_auc: 0.8210\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0647 - auc: 0.9124 - val_loss: 0.0814 - val_auc: 0.8249\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0623 - auc: 0.9067 - val_loss: 0.0810 - val_auc: 0.8161\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0633 - auc: 0.9027 - val_loss: 0.0816 - val_auc: 0.8067\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0611 - auc: 0.9212 - val_loss: 0.0819 - val_auc: 0.8063\n",
      "Epoch 63/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0610 - auc: 0.9244Restoring model weights from the end of the best epoch: 43.\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0604 - auc: 0.9245 - val_loss: 0.0827 - val_auc: 0.8077\n",
      "Epoch 63: early stopping\n",
      "\n",
      "Validation AUC: 80.77%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 13\n",
      "activation_dense: relu\n",
      "learning_rate: 1.8e-04\n",
      "l1_lambda: 0.0e+00\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 16s 107ms/step - loss: 0.5709 - auc: 0.4920 - val_loss: 0.4291 - val_auc: 0.4003\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.3438 - auc: 0.4865 - val_loss: 0.2573 - val_auc: 0.3840\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.2251 - auc: 0.4742 - val_loss: 0.1872 - val_auc: 0.3893\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1736 - auc: 0.5107 - val_loss: 0.1503 - val_auc: 0.4288\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1435 - auc: 0.4539 - val_loss: 0.1260 - val_auc: 0.4361\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1253 - auc: 0.4449 - val_loss: 0.1131 - val_auc: 0.4664\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1154 - auc: 0.5034 - val_loss: 0.1057 - val_auc: 0.4718\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1096 - auc: 0.4642 - val_loss: 0.1008 - val_auc: 0.4568\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1058 - auc: 0.4708 - val_loss: 0.0977 - val_auc: 0.4435\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1034 - auc: 0.4800 - val_loss: 0.0953 - val_auc: 0.4727\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1016 - auc: 0.4703 - val_loss: 0.0937 - val_auc: 0.4996\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1004 - auc: 0.4921 - val_loss: 0.0927 - val_auc: 0.4440\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0996 - auc: 0.4437 - val_loss: 0.0919 - val_auc: 0.4988\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0990 - auc: 0.4960 - val_loss: 0.0913 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0986 - auc: 0.4855 - val_loss: 0.0909 - val_auc: 0.4842\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0983 - auc: 0.4772 - val_loss: 0.0906 - val_auc: 0.4789\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0981 - auc: 0.4891 - val_loss: 0.0904 - val_auc: 0.4996\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0980 - auc: 0.5025 - val_loss: 0.0902 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0978 - auc: 0.4995 - val_loss: 0.0901 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0978 - auc: 0.4999 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0978 - auc: 0.4999 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0976 - auc: 0.4926 - val_loss: 0.0898 - val_auc: 0.4884\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0976 - auc: 0.4988 - val_loss: 0.0897 - val_auc: 0.4999\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0976 - auc: 0.4797 - val_loss: 0.0897 - val_auc: 0.4747\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0976 - auc: 0.4968 - val_loss: 0.0897 - val_auc: 0.5065\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0975 - auc: 0.5080 - val_loss: 0.0897 - val_auc: 0.5158\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0976 - auc: 0.5226 - val_loss: 0.0896 - val_auc: 0.5584\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0975 - auc: 0.5502 - val_loss: 0.0896 - val_auc: 0.5873\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0974 - auc: 0.5752 - val_loss: 0.0896 - val_auc: 0.5699\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0974 - auc: 0.5379 - val_loss: 0.0896 - val_auc: 0.5449\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0971 - auc: 0.5928 - val_loss: 0.0894 - val_auc: 0.5895\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0967 - auc: 0.6082 - val_loss: 0.0891 - val_auc: 0.5727\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0960 - auc: 0.6274 - val_loss: 0.0885 - val_auc: 0.6235\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0945 - auc: 0.7534 - val_loss: 0.0877 - val_auc: 0.7578\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0935 - auc: 0.7843 - val_loss: 0.0865 - val_auc: 0.7710\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0922 - auc: 0.7928 - val_loss: 0.0854 - val_auc: 0.7859\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0904 - auc: 0.8200 - val_loss: 0.0845 - val_auc: 0.7717\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0888 - auc: 0.8212 - val_loss: 0.0836 - val_auc: 0.7793\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0875 - auc: 0.8413 - val_loss: 0.0825 - val_auc: 0.8079\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0858 - auc: 0.8326 - val_loss: 0.0817 - val_auc: 0.7794\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0846 - auc: 0.8532 - val_loss: 0.0807 - val_auc: 0.7897\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0823 - auc: 0.8396 - val_loss: 0.0801 - val_auc: 0.7955\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0796 - auc: 0.8641 - val_loss: 0.0792 - val_auc: 0.7963\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0775 - auc: 0.8687 - val_loss: 0.0788 - val_auc: 0.8060\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0773 - auc: 0.8580 - val_loss: 0.0783 - val_auc: 0.8206\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0748 - auc: 0.8769 - val_loss: 0.0777 - val_auc: 0.8089\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0750 - auc: 0.8671 - val_loss: 0.0777 - val_auc: 0.8161\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0723 - auc: 0.8925 - val_loss: 0.0776 - val_auc: 0.7984\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0717 - auc: 0.8713 - val_loss: 0.0776 - val_auc: 0.8045\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0713 - auc: 0.8846 - val_loss: 0.0776 - val_auc: 0.8110\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0696 - auc: 0.8909 - val_loss: 0.0779 - val_auc: 0.7854\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0702 - auc: 0.8824 - val_loss: 0.0783 - val_auc: 0.7853\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0697 - auc: 0.8814 - val_loss: 0.0786 - val_auc: 0.7966\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0674 - auc: 0.8927 - val_loss: 0.0785 - val_auc: 0.7842\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0677 - auc: 0.8988 - val_loss: 0.0790 - val_auc: 0.7946\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0663 - auc: 0.8984 - val_loss: 0.0806 - val_auc: 0.7710\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0670 - auc: 0.8846 - val_loss: 0.0799 - val_auc: 0.8004\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0643 - auc: 0.9045 - val_loss: 0.0804 - val_auc: 0.7888\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0656 - auc: 0.9093 - val_loss: 0.0800 - val_auc: 0.8007\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0620 - auc: 0.8989 - val_loss: 0.0807 - val_auc: 0.7991\n",
      "Epoch 63/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0605 - auc: 0.9217 - val_loss: 0.0813 - val_auc: 0.7965\n",
      "Epoch 64/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0627 - auc: 0.9150 - val_loss: 0.0821 - val_auc: 0.7771\n",
      "Epoch 65/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0606 - auc: 0.9161 - val_loss: 0.0822 - val_auc: 0.7723\n",
      "Epoch 66/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0614 - auc: 0.9222 - val_loss: 0.0822 - val_auc: 0.7903\n",
      "Epoch 67/1000\n",
      "37/39 [===========================>..] - ETA: 0s - loss: 0.0612 - auc: 0.9270Restoring model weights from the end of the best epoch: 47.\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0604 - auc: 0.9283 - val_loss: 0.0824 - val_auc: 0.7913\n",
      "Epoch 67: early stopping\n",
      "\n",
      "Validation AUC: 79.13%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.3\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 3\n",
      "activation_dense: relu\n",
      "learning_rate: 1.8e-04\n",
      "l1_lambda: 0.0e+00\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 16s 110ms/step - loss: 0.5662 - auc: 0.5102 - val_loss: 0.4038 - val_auc: 0.4033\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.3270 - auc: 0.4786 - val_loss: 0.2541 - val_auc: 0.4233\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.2221 - auc: 0.4930 - val_loss: 0.1837 - val_auc: 0.3790\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1682 - auc: 0.4766 - val_loss: 0.1446 - val_auc: 0.3913\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1393 - auc: 0.4520 - val_loss: 0.1241 - val_auc: 0.4703\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1240 - auc: 0.4641 - val_loss: 0.1128 - val_auc: 0.4360\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1152 - auc: 0.4661 - val_loss: 0.1057 - val_auc: 0.4710\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1097 - auc: 0.4520 - val_loss: 0.1011 - val_auc: 0.5003\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1060 - auc: 0.4632 - val_loss: 0.0980 - val_auc: 0.4941\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1037 - auc: 0.4528 - val_loss: 0.0957 - val_auc: 0.4777\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.1018 - auc: 0.5118 - val_loss: 0.0943 - val_auc: 0.4913\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1007 - auc: 0.5015 - val_loss: 0.0930 - val_auc: 0.4417\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0998 - auc: 0.5008 - val_loss: 0.0923 - val_auc: 0.4810\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0992 - auc: 0.4933 - val_loss: 0.0916 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0987 - auc: 0.4998 - val_loss: 0.0912 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0984 - auc: 0.4514 - val_loss: 0.0908 - val_auc: 0.4245\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0982 - auc: 0.4819 - val_loss: 0.0905 - val_auc: 0.4946\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0980 - auc: 0.4996 - val_loss: 0.0903 - val_auc: 0.5000\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0979 - auc: 0.4992 - val_loss: 0.0902 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0978 - auc: 0.5000 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0898 - val_auc: 0.5000\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 2s 38ms/step - loss: 0.0976 - auc: 0.5012 - val_loss: 0.0898 - val_auc: 0.4810\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0976 - auc: 0.4829 - val_loss: 0.0897 - val_auc: 0.4632\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0976 - auc: 0.5019 - val_loss: 0.0897 - val_auc: 0.4888\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0976 - auc: 0.5318 - val_loss: 0.0897 - val_auc: 0.5217\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0975 - auc: 0.5255 - val_loss: 0.0897 - val_auc: 0.5506\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0975 - auc: 0.5304 - val_loss: 0.0897 - val_auc: 0.5349\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0975 - auc: 0.5595 - val_loss: 0.0896 - val_auc: 0.5299\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0975 - auc: 0.5489 - val_loss: 0.0896 - val_auc: 0.5439\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0975 - auc: 0.6079 - val_loss: 0.0896 - val_auc: 0.5088\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0974 - auc: 0.5839 - val_loss: 0.0896 - val_auc: 0.5811\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0974 - auc: 0.5578 - val_loss: 0.0896 - val_auc: 0.5540\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0973 - auc: 0.6218 - val_loss: 0.0895 - val_auc: 0.6217\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0971 - auc: 0.6386 - val_loss: 0.0893 - val_auc: 0.6725\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0964 - auc: 0.6312 - val_loss: 0.0886 - val_auc: 0.6304\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0942 - auc: 0.7146 - val_loss: 0.0869 - val_auc: 0.7517\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0918 - auc: 0.7908 - val_loss: 0.0856 - val_auc: 0.7718\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0900 - auc: 0.7912 - val_loss: 0.0841 - val_auc: 0.7746\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0877 - auc: 0.8047 - val_loss: 0.0830 - val_auc: 0.7848\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0860 - auc: 0.8193 - val_loss: 0.0824 - val_auc: 0.7893\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0834 - auc: 0.8368 - val_loss: 0.0810 - val_auc: 0.7807\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0811 - auc: 0.8423 - val_loss: 0.0800 - val_auc: 0.8060\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0780 - auc: 0.8505 - val_loss: 0.0795 - val_auc: 0.7704\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0767 - auc: 0.8575 - val_loss: 0.0783 - val_auc: 0.7798\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0755 - auc: 0.8501 - val_loss: 0.0791 - val_auc: 0.7636\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0708 - auc: 0.8835 - val_loss: 0.0780 - val_auc: 0.7994\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0736 - auc: 0.8497 - val_loss: 0.0781 - val_auc: 0.8043\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0709 - auc: 0.8654 - val_loss: 0.0783 - val_auc: 0.8277\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0704 - auc: 0.8687 - val_loss: 0.0779 - val_auc: 0.8029\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0688 - auc: 0.8804 - val_loss: 0.0778 - val_auc: 0.8093\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0669 - auc: 0.8820 - val_loss: 0.0777 - val_auc: 0.8275\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0660 - auc: 0.9063 - val_loss: 0.0786 - val_auc: 0.8431\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0654 - auc: 0.8944 - val_loss: 0.0785 - val_auc: 0.8303\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0637 - auc: 0.8955 - val_loss: 0.0789 - val_auc: 0.8225\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0619 - auc: 0.9074 - val_loss: 0.0792 - val_auc: 0.8127\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0617 - auc: 0.8828 - val_loss: 0.0802 - val_auc: 0.7756\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0619 - auc: 0.8890 - val_loss: 0.0803 - val_auc: 0.7763\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0608 - auc: 0.9022 - val_loss: 0.0819 - val_auc: 0.8160\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0588 - auc: 0.9031 - val_loss: 0.0810 - val_auc: 0.8265\n",
      "Epoch 63/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0612 - auc: 0.9018 - val_loss: 0.0809 - val_auc: 0.8005\n",
      "Epoch 64/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0584 - auc: 0.8878 - val_loss: 0.0825 - val_auc: 0.8054\n",
      "Epoch 65/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0591 - auc: 0.9070 - val_loss: 0.0832 - val_auc: 0.7916\n",
      "Epoch 66/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0583 - auc: 0.9149 - val_loss: 0.0830 - val_auc: 0.8058\n",
      "Epoch 67/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0567 - auc: 0.9030 - val_loss: 0.0833 - val_auc: 0.7859\n",
      "Epoch 68/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0562 - auc: 0.9174 - val_loss: 0.0833 - val_auc: 0.7921\n",
      "Epoch 69/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0543 - auc: 0.9104 - val_loss: 0.0847 - val_auc: 0.7932\n",
      "Epoch 70/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0538 - auc: 0.9027 - val_loss: 0.0850 - val_auc: 0.7809\n",
      "Epoch 71/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0548 - auc: 0.9128 - val_loss: 0.0854 - val_auc: 0.7812\n",
      "Epoch 72/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0517 - auc: 0.9143 - val_loss: 0.0858 - val_auc: 0.7903\n",
      "Epoch 73/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0526 - auc: 0.9152 - val_loss: 0.0864 - val_auc: 0.7889\n",
      "Epoch 74/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0530 - auc: 0.9148 - val_loss: 0.0867 - val_auc: 0.8018\n",
      "Epoch 75/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0529 - auc: 0.9247Restoring model weights from the end of the best epoch: 55.\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0525 - auc: 0.9256 - val_loss: 0.0864 - val_auc: 0.8008\n",
      "Epoch 75: early stopping\n",
      "\n",
      "Validation AUC: 80.08%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 12\n",
      "activation_dense: relu\n",
      "learning_rate: 1.9e-04\n",
      "l1_lambda: 0.0e+00\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 15s 104ms/step - loss: 0.5434 - auc: 0.4653 - val_loss: 0.3881 - val_auc: 0.4112\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.3241 - auc: 0.4796 - val_loss: 0.2536 - val_auc: 0.3683\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.2221 - auc: 0.4415 - val_loss: 0.1823 - val_auc: 0.3568\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1671 - auc: 0.4904 - val_loss: 0.1429 - val_auc: 0.4031\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1371 - auc: 0.4629 - val_loss: 0.1208 - val_auc: 0.4143\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.1205 - auc: 0.4747 - val_loss: 0.1087 - val_auc: 0.4105\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1113 - auc: 0.4589 - val_loss: 0.1016 - val_auc: 0.4171\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1060 - auc: 0.4771 - val_loss: 0.0973 - val_auc: 0.4682\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1028 - auc: 0.4614 - val_loss: 0.0946 - val_auc: 0.4827\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1009 - auc: 0.4877 - val_loss: 0.0929 - val_auc: 0.4387\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0996 - auc: 0.4946 - val_loss: 0.0918 - val_auc: 0.4988\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0989 - auc: 0.4996 - val_loss: 0.0912 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0985 - auc: 0.4907 - val_loss: 0.0906 - val_auc: 0.4868\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0981 - auc: 0.4714 - val_loss: 0.0903 - val_auc: 0.4992\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0979 - auc: 0.4954 - val_loss: 0.0901 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0978 - auc: 0.4989 - val_loss: 0.0900 - val_auc: 0.5000\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0976 - auc: 0.4997 - val_loss: 0.0899 - val_auc: 0.5000\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0977 - auc: 0.5000 - val_loss: 0.0898 - val_auc: 0.5012\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0975 - auc: 0.4984 - val_loss: 0.0898 - val_auc: 0.4545\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0974 - auc: 0.5236 - val_loss: 0.0897 - val_auc: 0.5172\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0974 - auc: 0.5506 - val_loss: 0.0896 - val_auc: 0.5283\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0972 - auc: 0.5928 - val_loss: 0.0895 - val_auc: 0.5716\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0968 - auc: 0.6102 - val_loss: 0.0893 - val_auc: 0.5733\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0966 - auc: 0.6482 - val_loss: 0.0891 - val_auc: 0.6383\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0962 - auc: 0.6580 - val_loss: 0.0888 - val_auc: 0.6514\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0959 - auc: 0.6890 - val_loss: 0.0885 - val_auc: 0.6591\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0956 - auc: 0.7106 - val_loss: 0.0883 - val_auc: 0.6500\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0948 - auc: 0.7672 - val_loss: 0.0880 - val_auc: 0.6880\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0943 - auc: 0.7986 - val_loss: 0.0877 - val_auc: 0.6964\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0942 - auc: 0.7818 - val_loss: 0.0873 - val_auc: 0.7429\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0938 - auc: 0.7585 - val_loss: 0.0873 - val_auc: 0.6998\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0924 - auc: 0.8299 - val_loss: 0.0868 - val_auc: 0.6863\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0921 - auc: 0.8194 - val_loss: 0.0865 - val_auc: 0.6840\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0917 - auc: 0.8199 - val_loss: 0.0862 - val_auc: 0.6871\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0910 - auc: 0.8182 - val_loss: 0.0859 - val_auc: 0.6930\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0900 - auc: 0.8341 - val_loss: 0.0856 - val_auc: 0.6920\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0889 - auc: 0.8466 - val_loss: 0.0850 - val_auc: 0.6944\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0878 - auc: 0.8345 - val_loss: 0.0840 - val_auc: 0.7315\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0847 - auc: 0.8614 - val_loss: 0.0833 - val_auc: 0.7518\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0832 - auc: 0.8649 - val_loss: 0.0823 - val_auc: 0.7751\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0815 - auc: 0.8648 - val_loss: 0.0816 - val_auc: 0.7661\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0792 - auc: 0.8771 - val_loss: 0.0811 - val_auc: 0.7843\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0776 - auc: 0.8695 - val_loss: 0.0810 - val_auc: 0.7615\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0758 - auc: 0.8747 - val_loss: 0.0813 - val_auc: 0.7864\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0742 - auc: 0.8706 - val_loss: 0.0815 - val_auc: 0.7840\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0729 - auc: 0.8856 - val_loss: 0.0799 - val_auc: 0.8064\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0710 - auc: 0.8853 - val_loss: 0.0794 - val_auc: 0.8133\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0704 - auc: 0.8897 - val_loss: 0.0796 - val_auc: 0.7755\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0688 - auc: 0.8998 - val_loss: 0.0793 - val_auc: 0.7915\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0673 - auc: 0.9081 - val_loss: 0.0806 - val_auc: 0.7687\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0680 - auc: 0.8955 - val_loss: 0.0797 - val_auc: 0.7965\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 2s 38ms/step - loss: 0.0677 - auc: 0.8797 - val_loss: 0.0803 - val_auc: 0.7882\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0652 - auc: 0.8788 - val_loss: 0.0803 - val_auc: 0.7885\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0641 - auc: 0.8920 - val_loss: 0.0809 - val_auc: 0.7771\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0635 - auc: 0.8817 - val_loss: 0.0806 - val_auc: 0.8016\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0641 - auc: 0.8912 - val_loss: 0.0808 - val_auc: 0.8131\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 2s 43ms/step - loss: 0.0611 - auc: 0.9148 - val_loss: 0.0825 - val_auc: 0.7913\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0596 - auc: 0.9252 - val_loss: 0.0820 - val_auc: 0.8200\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0595 - auc: 0.8989 - val_loss: 0.0833 - val_auc: 0.7782\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0603 - auc: 0.9131 - val_loss: 0.0841 - val_auc: 0.7793\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0610 - auc: 0.9243 - val_loss: 0.0824 - val_auc: 0.7946\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0585 - auc: 0.9214 - val_loss: 0.0831 - val_auc: 0.7926\n",
      "Epoch 63/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0589 - auc: 0.9180 - val_loss: 0.0839 - val_auc: 0.7914\n",
      "Epoch 64/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0605 - auc: 0.9243 - val_loss: 0.0844 - val_auc: 0.8000\n",
      "Epoch 65/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0566 - auc: 0.9300 - val_loss: 0.0837 - val_auc: 0.7891\n",
      "Epoch 66/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0576 - auc: 0.9147 - val_loss: 0.0849 - val_auc: 0.7973\n",
      "Epoch 67/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0603 - auc: 0.9243 - val_loss: 0.0848 - val_auc: 0.8051\n",
      "Epoch 68/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0579 - auc: 0.9256 - val_loss: 0.0845 - val_auc: 0.8010\n",
      "Epoch 69/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0554 - auc: 0.9365 - val_loss: 0.0861 - val_auc: 0.7996\n",
      "Epoch 70/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0558 - auc: 0.9273 - val_loss: 0.0859 - val_auc: 0.7860\n",
      "Epoch 71/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0563 - auc: 0.9240 - val_loss: 0.0867 - val_auc: 0.7894\n",
      "Epoch 72/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0570 - auc: 0.9289 - val_loss: 0.0869 - val_auc: 0.7709\n",
      "Epoch 73/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0548 - auc: 0.9319 - val_loss: 0.0870 - val_auc: 0.7762\n",
      "Epoch 74/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0555 - auc: 0.9296 - val_loss: 0.0857 - val_auc: 0.7846\n",
      "Epoch 75/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0554 - auc: 0.9248 - val_loss: 0.0884 - val_auc: 0.7792\n",
      "Epoch 76/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0538 - auc: 0.9360 - val_loss: 0.0882 - val_auc: 0.7861\n",
      "Epoch 77/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0534 - auc: 0.9324 - val_loss: 0.0889 - val_auc: 0.7703\n",
      "Epoch 78/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0525 - auc: 0.9346Restoring model weights from the end of the best epoch: 58.\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0525 - auc: 0.9346 - val_loss: 0.0890 - val_auc: 0.7646\n",
      "Epoch 78: early stopping\n",
      "\n",
      "Validation AUC: 76.46%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.1\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 7\n",
      "activation_dense: relu\n",
      "learning_rate: 1.6e-04\n",
      "l1_lambda: 1.0e-05\n",
      "l2_lambda: 0.0e+00\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 16s 105ms/step - loss: 0.5715 - auc: 0.5099 - val_loss: 0.4609 - val_auc: 0.4257\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.3925 - auc: 0.4723 - val_loss: 0.3253 - val_auc: 0.3930\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2823 - auc: 0.5252 - val_loss: 0.2367 - val_auc: 0.3841\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.2115 - auc: 0.5226 - val_loss: 0.1818 - val_auc: 0.4374\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1685 - auc: 0.5012 - val_loss: 0.1485 - val_auc: 0.4314\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1427 - auc: 0.4385 - val_loss: 0.1280 - val_auc: 0.4083\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1268 - auc: 0.4680 - val_loss: 0.1153 - val_auc: 0.4736\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1169 - auc: 0.5061 - val_loss: 0.1074 - val_auc: 0.4884\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1107 - auc: 0.4655 - val_loss: 0.1022 - val_auc: 0.4667\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.1067 - auc: 0.4963 - val_loss: 0.0988 - val_auc: 0.4063\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1041 - auc: 0.4910 - val_loss: 0.0965 - val_auc: 0.4425\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1024 - auc: 0.4713 - val_loss: 0.0948 - val_auc: 0.4996\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1013 - auc: 0.4660 - val_loss: 0.0937 - val_auc: 0.3927\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1004 - auc: 0.4864 - val_loss: 0.0930 - val_auc: 0.4992\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0999 - auc: 0.4999 - val_loss: 0.0923 - val_auc: 0.5000\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0995 - auc: 0.4720 - val_loss: 0.0919 - val_auc: 0.4783\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0992 - auc: 0.4820 - val_loss: 0.0916 - val_auc: 0.5013\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0990 - auc: 0.4946 - val_loss: 0.0914 - val_auc: 0.4992\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 42ms/step - loss: 0.0989 - auc: 0.4990 - val_loss: 0.0912 - val_auc: 0.5000\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0987 - auc: 0.4998 - val_loss: 0.0910 - val_auc: 0.5000\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0987 - auc: 0.5000 - val_loss: 0.0909 - val_auc: 0.5000\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0986 - auc: 0.5000 - val_loss: 0.0908 - val_auc: 0.5000\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0985 - auc: 0.5000 - val_loss: 0.0907 - val_auc: 0.5000\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0984 - auc: 0.5000 - val_loss: 0.0907 - val_auc: 0.5000\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0983 - auc: 0.5000 - val_loss: 0.0906 - val_auc: 0.5000\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0983 - auc: 0.5120 - val_loss: 0.0905 - val_auc: 0.5202\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0982 - auc: 0.5196 - val_loss: 0.0904 - val_auc: 0.5417\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0981 - auc: 0.5633 - val_loss: 0.0903 - val_auc: 0.5921\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0979 - auc: 0.5258 - val_loss: 0.0902 - val_auc: 0.5091\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0970 - auc: 0.6190 - val_loss: 0.0891 - val_auc: 0.6737\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0955 - auc: 0.7688 - val_loss: 0.0886 - val_auc: 0.7665\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0948 - auc: 0.8088 - val_loss: 0.0882 - val_auc: 0.7407\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0942 - auc: 0.8290 - val_loss: 0.0877 - val_auc: 0.7554\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0932 - auc: 0.8164 - val_loss: 0.0869 - val_auc: 0.7598\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0917 - auc: 0.8451 - val_loss: 0.0864 - val_auc: 0.7746\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0906 - auc: 0.8548 - val_loss: 0.0855 - val_auc: 0.7766\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0889 - auc: 0.8592 - val_loss: 0.0840 - val_auc: 0.8054\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0869 - auc: 0.8539 - val_loss: 0.0829 - val_auc: 0.8042\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0849 - auc: 0.8702 - val_loss: 0.0823 - val_auc: 0.8109\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0829 - auc: 0.8775 - val_loss: 0.0814 - val_auc: 0.8289\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0817 - auc: 0.8814 - val_loss: 0.0807 - val_auc: 0.8215\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0799 - auc: 0.8874 - val_loss: 0.0806 - val_auc: 0.8074\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0784 - auc: 0.8870 - val_loss: 0.0795 - val_auc: 0.8329\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0762 - auc: 0.8951 - val_loss: 0.0791 - val_auc: 0.7984\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0738 - auc: 0.8887 - val_loss: 0.0785 - val_auc: 0.8000\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0721 - auc: 0.8961 - val_loss: 0.0786 - val_auc: 0.8345\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0709 - auc: 0.9046 - val_loss: 0.0775 - val_auc: 0.8384\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0685 - auc: 0.9175 - val_loss: 0.0776 - val_auc: 0.8433\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0677 - auc: 0.9054 - val_loss: 0.0776 - val_auc: 0.8285\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0652 - auc: 0.9205 - val_loss: 0.0778 - val_auc: 0.8263\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0630 - auc: 0.9259 - val_loss: 0.0790 - val_auc: 0.8222\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0627 - auc: 0.9260 - val_loss: 0.0782 - val_auc: 0.8424\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0621 - auc: 0.9186 - val_loss: 0.0788 - val_auc: 0.8413\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0609 - auc: 0.9246 - val_loss: 0.0792 - val_auc: 0.8254\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0582 - auc: 0.9315 - val_loss: 0.0800 - val_auc: 0.8304\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0568 - auc: 0.9294 - val_loss: 0.0807 - val_auc: 0.8313\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0569 - auc: 0.9292 - val_loss: 0.0818 - val_auc: 0.8094\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0552 - auc: 0.9332 - val_loss: 0.0816 - val_auc: 0.8323\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0555 - auc: 0.9384 - val_loss: 0.0821 - val_auc: 0.8437\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0542 - auc: 0.9411 - val_loss: 0.0827 - val_auc: 0.8122\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0531 - auc: 0.9454 - val_loss: 0.0844 - val_auc: 0.7952\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0507 - auc: 0.9385 - val_loss: 0.0844 - val_auc: 0.7985\n",
      "Epoch 63/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0499 - auc: 0.9437 - val_loss: 0.0855 - val_auc: 0.7987\n",
      "Epoch 64/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0470 - auc: 0.9539 - val_loss: 0.0866 - val_auc: 0.8003\n",
      "Epoch 65/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0496 - auc: 0.9517 - val_loss: 0.0874 - val_auc: 0.7847\n",
      "Epoch 66/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0467 - auc: 0.9538 - val_loss: 0.0907 - val_auc: 0.7901\n",
      "Epoch 67/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0472 - auc: 0.9555 - val_loss: 0.0892 - val_auc: 0.7893\n",
      "Epoch 68/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0448 - auc: 0.9536 - val_loss: 0.0905 - val_auc: 0.7863\n",
      "Epoch 69/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0447 - auc: 0.9671 - val_loss: 0.0908 - val_auc: 0.8055\n",
      "Epoch 70/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0438 - auc: 0.9657 - val_loss: 0.0913 - val_auc: 0.7581\n",
      "Epoch 71/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0418 - auc: 0.9602 - val_loss: 0.0920 - val_auc: 0.7602\n",
      "Epoch 72/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0402 - auc: 0.9700 - val_loss: 0.0937 - val_auc: 0.7516\n",
      "Epoch 73/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0387 - auc: 0.9710 - val_loss: 0.0938 - val_auc: 0.7050\n",
      "Epoch 74/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0397 - auc: 0.9715 - val_loss: 0.0951 - val_auc: 0.7410\n",
      "Epoch 75/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0378 - auc: 0.9704 - val_loss: 0.0957 - val_auc: 0.7124\n",
      "Epoch 76/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0356 - auc: 0.9742 - val_loss: 0.0977 - val_auc: 0.7106\n",
      "Epoch 77/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0355 - auc: 0.9750 - val_loss: 0.0983 - val_auc: 0.7150\n",
      "Epoch 78/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0353 - auc: 0.9755 - val_loss: 0.0983 - val_auc: 0.6902\n",
      "Epoch 79/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0336 - auc: 0.9776Restoring model weights from the end of the best epoch: 59.\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0340 - auc: 0.9775 - val_loss: 0.0999 - val_auc: 0.6866\n",
      "Epoch 79: early stopping\n",
      "\n",
      "Validation AUC: 68.66%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.5\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 1\n",
      "activation_dense: relu\n",
      "learning_rate: 1.7e-04\n",
      "l1_lambda: 1.0e-04\n",
      "l2_lambda: 1.0e-06\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 15s 103ms/step - loss: 0.5721 - auc: 0.5259 - val_loss: 0.4535 - val_auc: 0.3525\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.3996 - auc: 0.4770 - val_loss: 0.3296 - val_auc: 0.3573\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.2955 - auc: 0.4558 - val_loss: 0.2482 - val_auc: 0.3706\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.2261 - auc: 0.4887 - val_loss: 0.1932 - val_auc: 0.4129\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1802 - auc: 0.4313 - val_loss: 0.1560 - val_auc: 0.4694\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1503 - auc: 0.4700 - val_loss: 0.1333 - val_auc: 0.4358\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1328 - auc: 0.4716 - val_loss: 0.1201 - val_auc: 0.4666\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1225 - auc: 0.4857 - val_loss: 0.1123 - val_auc: 0.4682\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1163 - auc: 0.4854 - val_loss: 0.1073 - val_auc: 0.4996\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1123 - auc: 0.4825 - val_loss: 0.1038 - val_auc: 0.5000\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1094 - auc: 0.4991 - val_loss: 0.1011 - val_auc: 0.4988\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1074 - auc: 0.4865 - val_loss: 0.0994 - val_auc: 0.4432\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1060 - auc: 0.4781 - val_loss: 0.0981 - val_auc: 0.4992\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1047 - auc: 0.5008 - val_loss: 0.0969 - val_auc: 0.5000\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1039 - auc: 0.4778 - val_loss: 0.0960 - val_auc: 0.4683\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1032 - auc: 0.5348 - val_loss: 0.0953 - val_auc: 0.4983\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.1023 - auc: 0.5476 - val_loss: 0.0940 - val_auc: 0.6420\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1011 - auc: 0.6277 - val_loss: 0.0930 - val_auc: 0.6646\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1004 - auc: 0.6265 - val_loss: 0.0922 - val_auc: 0.6675\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0997 - auc: 0.6542 - val_loss: 0.0915 - val_auc: 0.7322\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0991 - auc: 0.6868 - val_loss: 0.0909 - val_auc: 0.7020\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0982 - auc: 0.7229 - val_loss: 0.0904 - val_auc: 0.7229\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 2s 38ms/step - loss: 0.0975 - auc: 0.7431 - val_loss: 0.0898 - val_auc: 0.7594\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0974 - auc: 0.7226 - val_loss: 0.0893 - val_auc: 0.7521\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0963 - auc: 0.7645 - val_loss: 0.0887 - val_auc: 0.7805\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0945 - auc: 0.7882 - val_loss: 0.0874 - val_auc: 0.7449\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0933 - auc: 0.7794 - val_loss: 0.0862 - val_auc: 0.7459\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0928 - auc: 0.7685 - val_loss: 0.0852 - val_auc: 0.7722\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0910 - auc: 0.8018 - val_loss: 0.0844 - val_auc: 0.7647\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0894 - auc: 0.8047 - val_loss: 0.0837 - val_auc: 0.7629\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0892 - auc: 0.7852 - val_loss: 0.0831 - val_auc: 0.7722\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0878 - auc: 0.8001 - val_loss: 0.0829 - val_auc: 0.7801\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0867 - auc: 0.8318 - val_loss: 0.0818 - val_auc: 0.7755\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0851 - auc: 0.8306 - val_loss: 0.0814 - val_auc: 0.8104\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0846 - auc: 0.8258 - val_loss: 0.0815 - val_auc: 0.7698\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0839 - auc: 0.8213 - val_loss: 0.0805 - val_auc: 0.7908\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0827 - auc: 0.7988 - val_loss: 0.0803 - val_auc: 0.7671\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0814 - auc: 0.8214 - val_loss: 0.0797 - val_auc: 0.7996\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0811 - auc: 0.8304 - val_loss: 0.0797 - val_auc: 0.7736\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0803 - auc: 0.8247 - val_loss: 0.0794 - val_auc: 0.8056\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0776 - auc: 0.8565 - val_loss: 0.0793 - val_auc: 0.7727\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0769 - auc: 0.8593 - val_loss: 0.0791 - val_auc: 0.7745\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0734 - auc: 0.8754 - val_loss: 0.0792 - val_auc: 0.7954\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0756 - auc: 0.8769 - val_loss: 0.0790 - val_auc: 0.8057\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0747 - auc: 0.8617 - val_loss: 0.0788 - val_auc: 0.8003\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0748 - auc: 0.8491 - val_loss: 0.0794 - val_auc: 0.7867\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0730 - auc: 0.8738 - val_loss: 0.0795 - val_auc: 0.7892\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0712 - auc: 0.8499 - val_loss: 0.0796 - val_auc: 0.7713\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0724 - auc: 0.8510 - val_loss: 0.0789 - val_auc: 0.8045\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0709 - auc: 0.8813 - val_loss: 0.0792 - val_auc: 0.7747\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0701 - auc: 0.8789 - val_loss: 0.0810 - val_auc: 0.7518\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0722 - auc: 0.8561 - val_loss: 0.0794 - val_auc: 0.7764\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0702 - auc: 0.8610 - val_loss: 0.0802 - val_auc: 0.7817\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0695 - auc: 0.8705Restoring model weights from the end of the best epoch: 34.\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.0695 - auc: 0.8705 - val_loss: 0.0813 - val_auc: 0.7688\n",
      "Epoch 54: early stopping\n",
      "\n",
      "Validation AUC: 76.88%\n",
      "\n",
      "num_lstm_layers: 1\n",
      "activation_lstm: tanh\n",
      "num_lstm_nodes: 20\n",
      "dropout_rate: 0.4\n",
      "num_dense_layers: 0\n",
      "num_dense_nodes: 7\n",
      "activation_dense: relu\n",
      "learning_rate: 1.8e-04\n",
      "l1_lambda: 0.0e+00\n",
      "l2_lambda: 1.0e-01\n",
      "act_last: sigmoid\n",
      "\n",
      "Epoch 1/1000\n",
      "39/39 [==============================] - 16s 102ms/step - loss: 2.4635 - auc: 0.4648 - val_loss: 2.2166 - val_auc: 0.3477\n",
      "Epoch 2/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 2.0538 - auc: 0.4749 - val_loss: 1.8820 - val_auc: 0.3993\n",
      "Epoch 3/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 1.7673 - auc: 0.4244 - val_loss: 1.6426 - val_auc: 0.3795\n",
      "Epoch 4/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 1.5592 - auc: 0.4869 - val_loss: 1.4641 - val_auc: 0.4129\n",
      "Epoch 5/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 1.3985 - auc: 0.4364 - val_loss: 1.3193 - val_auc: 0.4353\n",
      "Epoch 6/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 1.2649 - auc: 0.4868 - val_loss: 1.1957 - val_auc: 0.4244\n",
      "Epoch 7/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 1.1495 - auc: 0.4364 - val_loss: 1.0876 - val_auc: 0.4597\n",
      "Epoch 8/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 1.0479 - auc: 0.4779 - val_loss: 0.9921 - val_auc: 0.4903\n",
      "Epoch 9/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.9575 - auc: 0.5180 - val_loss: 0.9066 - val_auc: 0.4996\n",
      "Epoch 10/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.8764 - auc: 0.4807 - val_loss: 0.8297 - val_auc: 0.4724\n",
      "Epoch 11/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.8034 - auc: 0.4798 - val_loss: 0.7604 - val_auc: 0.4983\n",
      "Epoch 12/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.7374 - auc: 0.4939 - val_loss: 0.6977 - val_auc: 0.5000\n",
      "Epoch 13/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.6776 - auc: 0.4989 - val_loss: 0.6409 - val_auc: 0.5000\n",
      "Epoch 14/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.6233 - auc: 0.4995 - val_loss: 0.5893 - val_auc: 0.5326\n",
      "Epoch 15/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.5741 - auc: 0.5250 - val_loss: 0.5426 - val_auc: 0.5545\n",
      "Epoch 16/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.5296 - auc: 0.5613 - val_loss: 0.5003 - val_auc: 0.6023\n",
      "Epoch 17/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.4892 - auc: 0.6052 - val_loss: 0.4620 - val_auc: 0.6395\n",
      "Epoch 18/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.4524 - auc: 0.6756 - val_loss: 0.4270 - val_auc: 0.6945\n",
      "Epoch 19/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.4182 - auc: 0.7125 - val_loss: 0.3944 - val_auc: 0.7276\n",
      "Epoch 20/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.3874 - auc: 0.7486 - val_loss: 0.3654 - val_auc: 0.7622\n",
      "Epoch 21/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.3596 - auc: 0.7632 - val_loss: 0.3391 - val_auc: 0.7348\n",
      "Epoch 22/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.3341 - auc: 0.7898 - val_loss: 0.3151 - val_auc: 0.7416\n",
      "Epoch 23/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.3113 - auc: 0.7917 - val_loss: 0.2933 - val_auc: 0.7564\n",
      "Epoch 24/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.2901 - auc: 0.7878 - val_loss: 0.2734 - val_auc: 0.7445\n",
      "Epoch 25/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.2709 - auc: 0.7961 - val_loss: 0.2553 - val_auc: 0.7590\n",
      "Epoch 26/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.2532 - auc: 0.7956 - val_loss: 0.2384 - val_auc: 0.7762\n",
      "Epoch 27/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.2361 - auc: 0.8215 - val_loss: 0.2231 - val_auc: 0.7865\n",
      "Epoch 28/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.2197 - auc: 0.8404 - val_loss: 0.2089 - val_auc: 0.7786\n",
      "Epoch 29/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.2048 - auc: 0.8536 - val_loss: 0.1960 - val_auc: 0.7727\n",
      "Epoch 30/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1926 - auc: 0.8422 - val_loss: 0.1842 - val_auc: 0.7806\n",
      "Epoch 31/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.1813 - auc: 0.8203 - val_loss: 0.1735 - val_auc: 0.7806\n",
      "Epoch 32/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1698 - auc: 0.8439 - val_loss: 0.1641 - val_auc: 0.7947\n",
      "Epoch 33/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1593 - auc: 0.8528 - val_loss: 0.1552 - val_auc: 0.8025\n",
      "Epoch 34/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.1502 - auc: 0.8601 - val_loss: 0.1477 - val_auc: 0.7937\n",
      "Epoch 35/1000\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1421 - auc: 0.8606 - val_loss: 0.1404 - val_auc: 0.8004\n",
      "Epoch 36/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1351 - auc: 0.8689 - val_loss: 0.1354 - val_auc: 0.7717\n",
      "Epoch 37/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.1281 - auc: 0.8660 - val_loss: 0.1289 - val_auc: 0.7846\n",
      "Epoch 38/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1208 - auc: 0.8812 - val_loss: 0.1228 - val_auc: 0.7920\n",
      "Epoch 39/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1150 - auc: 0.8711 - val_loss: 0.1183 - val_auc: 0.8060\n",
      "Epoch 40/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.1123 - auc: 0.8718 - val_loss: 0.1143 - val_auc: 0.8178\n",
      "Epoch 41/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.1057 - auc: 0.8728 - val_loss: 0.1116 - val_auc: 0.7953\n",
      "Epoch 42/1000\n",
      "39/39 [==============================] - 2s 41ms/step - loss: 0.1003 - auc: 0.9003 - val_loss: 0.1067 - val_auc: 0.8034\n",
      "Epoch 43/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0958 - auc: 0.9044 - val_loss: 0.1044 - val_auc: 0.8064\n",
      "Epoch 44/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0930 - auc: 0.9030 - val_loss: 0.1021 - val_auc: 0.8236\n",
      "Epoch 45/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0891 - auc: 0.8977 - val_loss: 0.1001 - val_auc: 0.8237\n",
      "Epoch 46/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0860 - auc: 0.9034 - val_loss: 0.0968 - val_auc: 0.8292\n",
      "Epoch 47/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0818 - auc: 0.9097 - val_loss: 0.0954 - val_auc: 0.8016\n",
      "Epoch 48/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0826 - auc: 0.9101 - val_loss: 0.0939 - val_auc: 0.7944\n",
      "Epoch 49/1000\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.0790 - auc: 0.9160 - val_loss: 0.0932 - val_auc: 0.8047\n",
      "Epoch 50/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0756 - auc: 0.9223 - val_loss: 0.0920 - val_auc: 0.7992\n",
      "Epoch 51/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0737 - auc: 0.9228 - val_loss: 0.0906 - val_auc: 0.8080\n",
      "Epoch 52/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0705 - auc: 0.9310 - val_loss: 0.0891 - val_auc: 0.8115\n",
      "Epoch 53/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0710 - auc: 0.9234 - val_loss: 0.0880 - val_auc: 0.8147\n",
      "Epoch 54/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0691 - auc: 0.9223 - val_loss: 0.0883 - val_auc: 0.8188\n",
      "Epoch 55/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0672 - auc: 0.9195 - val_loss: 0.0889 - val_auc: 0.8272\n",
      "Epoch 56/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0668 - auc: 0.9282 - val_loss: 0.0883 - val_auc: 0.8292\n",
      "Epoch 57/1000\n",
      "39/39 [==============================] - 2s 38ms/step - loss: 0.0643 - auc: 0.9369 - val_loss: 0.0878 - val_auc: 0.8369\n",
      "Epoch 58/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0608 - auc: 0.9369 - val_loss: 0.0864 - val_auc: 0.8283\n",
      "Epoch 59/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0647 - auc: 0.9102 - val_loss: 0.0868 - val_auc: 0.8166\n",
      "Epoch 60/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0619 - auc: 0.9299 - val_loss: 0.0865 - val_auc: 0.8345\n",
      "Epoch 61/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0620 - auc: 0.9319 - val_loss: 0.0888 - val_auc: 0.8010\n",
      "Epoch 62/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0609 - auc: 0.9255 - val_loss: 0.0920 - val_auc: 0.7392\n",
      "Epoch 63/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0609 - auc: 0.9289 - val_loss: 0.0861 - val_auc: 0.8117\n",
      "Epoch 64/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0596 - auc: 0.9287 - val_loss: 0.0859 - val_auc: 0.7938\n",
      "Epoch 65/1000\n",
      "39/39 [==============================] - 1s 38ms/step - loss: 0.0600 - auc: 0.9327 - val_loss: 0.0856 - val_auc: 0.7977\n",
      "Epoch 66/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0558 - auc: 0.9404 - val_loss: 0.0873 - val_auc: 0.8025\n",
      "Epoch 67/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0557 - auc: 0.9332 - val_loss: 0.0891 - val_auc: 0.7673\n",
      "Epoch 68/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0533 - auc: 0.9411 - val_loss: 0.0882 - val_auc: 0.7638\n",
      "Epoch 69/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0537 - auc: 0.9459 - val_loss: 0.0873 - val_auc: 0.7734\n",
      "Epoch 70/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0549 - auc: 0.9355 - val_loss: 0.0877 - val_auc: 0.7791\n",
      "Epoch 71/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0500 - auc: 0.9536 - val_loss: 0.0891 - val_auc: 0.7420\n",
      "Epoch 72/1000\n",
      "39/39 [==============================] - 1s 39ms/step - loss: 0.0509 - auc: 0.9581 - val_loss: 0.0905 - val_auc: 0.7462\n",
      "Epoch 73/1000\n",
      "39/39 [==============================] - 2s 40ms/step - loss: 0.0513 - auc: 0.9400 - val_loss: 0.0895 - val_auc: 0.7576\n",
      "Epoch 74/1000\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0502 - auc: 0.9523 - val_loss: 0.0895 - val_auc: 0.7446\n",
      "Epoch 75/1000\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.0489 - auc: 0.9481 - val_loss: 0.0925 - val_auc: 0.7546\n",
      "Epoch 76/1000\n",
      "39/39 [==============================] - 1s 37ms/step - loss: 0.0495 - auc: 0.9589 - val_loss: 0.0928 - val_auc: 0.7375\n",
      "Epoch 77/1000\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0497 - auc: 0.9662Restoring model weights from the end of the best epoch: 57.\n",
      "39/39 [==============================] - 2s 44ms/step - loss: 0.0499 - auc: 0.9663 - val_loss: 0.0907 - val_auc: 0.7541\n",
      "Epoch 77: early stopping\n",
      "\n",
      "Validation AUC: 75.41%\n",
      "\n",
      "Train loss: 0.0638134703040123\n",
      "Train AUC: 0.9035066366195679\n",
      "Test loss: 0.09638296812772751\n",
      "Test AUC: 0.7368457317352295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(-0.8302116394042969,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.4,\n",
       "   0,\n",
       "   19,\n",
       "   'sigmoid',\n",
       "   0.0001641215952170319,\n",
       "   0.0001,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.8290481567382812,\n",
       "  [0,\n",
       "   'sigmoid',\n",
       "   30,\n",
       "   0.4,\n",
       "   0,\n",
       "   5,\n",
       "   'relu',\n",
       "   0.00021155608067402554,\n",
       "   0.1,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.8178815245628357,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.4,\n",
       "   0,\n",
       "   17,\n",
       "   'relu',\n",
       "   0.00020647421250237624,\n",
       "   0.001,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.813321053981781,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.5,\n",
       "   0,\n",
       "   16,\n",
       "   'relu',\n",
       "   0.00018212677299990075,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.8131333589553833,\n",
       "  [2,\n",
       "   'tanh',\n",
       "   30,\n",
       "   0.4,\n",
       "   0,\n",
       "   5,\n",
       "   'sigmoid',\n",
       "   0.005472679913214501,\n",
       "   0.1,\n",
       "   0.0001,\n",
       "   'sigmoid']),\n",
       " (-0.8077471256256104,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.4,\n",
       "   0,\n",
       "   1,\n",
       "   'relu',\n",
       "   0.00017584221319521715,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.8059830665588379,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.4,\n",
       "   0,\n",
       "   5,\n",
       "   'sigmoid',\n",
       "   0.00023813745390309496,\n",
       "   0.001,\n",
       "   1e-06,\n",
       "   'sigmoid']),\n",
       " (-0.8008220195770264,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.3,\n",
       "   0,\n",
       "   3,\n",
       "   'relu',\n",
       "   0.00017968741776647613,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.7997148036956787,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.4,\n",
       "   0,\n",
       "   5,\n",
       "   'relu',\n",
       "   0.0001945519936820306,\n",
       "   1e-06,\n",
       "   0.0001,\n",
       "   'sigmoid']),\n",
       " (-0.7973687648773193,\n",
       "  [1,\n",
       "   'sigmoid',\n",
       "   20,\n",
       "   0.4,\n",
       "   5,\n",
       "   13,\n",
       "   'relu',\n",
       "   0.0003324529709732297,\n",
       "   0.0001,\n",
       "   1e-05,\n",
       "   'sigmoid']),\n",
       " (-0.7939155697822571,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.5,\n",
       "   0,\n",
       "   13,\n",
       "   'sigmoid',\n",
       "   0.0001886738302404937,\n",
       "   0.001,\n",
       "   1e-06,\n",
       "   'sigmoid']),\n",
       " (-0.7930899262428284,\n",
       "  [1,\n",
       "   'sigmoid',\n",
       "   30,\n",
       "   0.4,\n",
       "   5,\n",
       "   5,\n",
       "   'relu',\n",
       "   0.0004241667396612241,\n",
       "   0.1,\n",
       "   1e-05,\n",
       "   'sigmoid']),\n",
       " (-0.7912507057189941,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.4,\n",
       "   0,\n",
       "   13,\n",
       "   'relu',\n",
       "   0.00017762882575169027,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.7907439470291138,\n",
       "  [0,\n",
       "   'tanh',\n",
       "   10,\n",
       "   0.3,\n",
       "   0,\n",
       "   17,\n",
       "   'relu',\n",
       "   0.00022858859507018416,\n",
       "   0.0001,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.7860144972801208,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.4,\n",
       "   0,\n",
       "   1,\n",
       "   'relu',\n",
       "   0.00020205239830711392,\n",
       "   1e-05,\n",
       "   1e-06,\n",
       "   'sigmoid']),\n",
       " (-0.7827866077423096,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   10,\n",
       "   0.4,\n",
       "   1,\n",
       "   19,\n",
       "   'relu',\n",
       "   0.00022423137518400263,\n",
       "   0.1,\n",
       "   1e-05,\n",
       "   'sigmoid']),\n",
       " (-0.7803842425346375,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.5,\n",
       "   1,\n",
       "   7,\n",
       "   'relu',\n",
       "   0.0003141343448817407,\n",
       "   0.01,\n",
       "   1e-05,\n",
       "   'sigmoid']),\n",
       " (-0.7688424587249756,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.5,\n",
       "   0,\n",
       "   1,\n",
       "   'relu',\n",
       "   0.00016863187471275113,\n",
       "   0.0001,\n",
       "   1e-06,\n",
       "   'sigmoid']),\n",
       " (-0.7645821571350098,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.4,\n",
       "   0,\n",
       "   12,\n",
       "   'relu',\n",
       "   0.00018563758010779063,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.7540913224220276,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.4,\n",
       "   0,\n",
       "   7,\n",
       "   'relu',\n",
       "   0.00018180561369688825,\n",
       "   0.0,\n",
       "   0.1,\n",
       "   'sigmoid']),\n",
       " (-0.7440882921218872,\n",
       "  [2,\n",
       "   'sigmoid',\n",
       "   20,\n",
       "   0.4,\n",
       "   1,\n",
       "   19,\n",
       "   'sigmoid',\n",
       "   0.002126469511297577,\n",
       "   0.1,\n",
       "   1e-06,\n",
       "   'sigmoid']),\n",
       " (-0.7439944744110107,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.4,\n",
       "   0,\n",
       "   28,\n",
       "   'relu',\n",
       "   0.0002206748447524539,\n",
       "   0.1,\n",
       "   1e-05,\n",
       "   'sigmoid']),\n",
       " (-0.7331844568252563,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.3,\n",
       "   3,\n",
       "   30,\n",
       "   'tanh',\n",
       "   0.00021431295406317276,\n",
       "   1e-05,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.723800778388977,\n",
       "  [0,\n",
       "   'sigmoid',\n",
       "   30,\n",
       "   0.1,\n",
       "   3,\n",
       "   9,\n",
       "   'relu',\n",
       "   0.0002602876143733726,\n",
       "   1e-06,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.7121086120605469,\n",
       "  [0, 'tanh', 10, 0.0, 0, 1, 'sigmoid', 0.001, 0.0, 0.0, 'sigmoid']),\n",
       " (-0.6866414546966553,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.1,\n",
       "   0,\n",
       "   7,\n",
       "   'relu',\n",
       "   0.00016222633220459853,\n",
       "   1e-05,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.673804521560669,\n",
       "  [2,\n",
       "   'tanh',\n",
       "   10,\n",
       "   0.2,\n",
       "   3,\n",
       "   22,\n",
       "   'sigmoid',\n",
       "   0.0037556317192635673,\n",
       "   0.1,\n",
       "   0.0001,\n",
       "   'sigmoid']),\n",
       " (-0.6616620421409607,\n",
       "  [1,\n",
       "   'sigmoid',\n",
       "   10,\n",
       "   0.0,\n",
       "   1,\n",
       "   26,\n",
       "   'tanh',\n",
       "   0.0010374545769920491,\n",
       "   0.001,\n",
       "   1e-06,\n",
       "   'sigmoid']),\n",
       " (-0.6584528088569641,\n",
       "  [3,\n",
       "   'tanh',\n",
       "   30,\n",
       "   0.2,\n",
       "   0,\n",
       "   8,\n",
       "   'sigmoid',\n",
       "   0.0013047270759966917,\n",
       "   0.1,\n",
       "   1e-05,\n",
       "   'sigmoid']),\n",
       " (-0.6559567451477051,\n",
       "  [2,\n",
       "   'tanh',\n",
       "   30,\n",
       "   0.4,\n",
       "   1,\n",
       "   18,\n",
       "   'sigmoid',\n",
       "   0.0016180058545582447,\n",
       "   0.001,\n",
       "   1e-06,\n",
       "   'sigmoid']),\n",
       " (-0.6556565761566162,\n",
       "  [0,\n",
       "   'sigmoid',\n",
       "   30,\n",
       "   0.1,\n",
       "   1,\n",
       "   5,\n",
       "   'relu',\n",
       "   0.002133310285684126,\n",
       "   0.1,\n",
       "   1e-06,\n",
       "   'sigmoid']),\n",
       " (-0.6294947862625122,\n",
       "  [0,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.3,\n",
       "   3,\n",
       "   26,\n",
       "   'tanh',\n",
       "   0.00022847090611077944,\n",
       "   0.0001,\n",
       "   1e-06,\n",
       "   'sigmoid']),\n",
       " (-0.6243524551391602,\n",
       "  [0,\n",
       "   'sigmoid',\n",
       "   10,\n",
       "   0.5,\n",
       "   4,\n",
       "   12,\n",
       "   'relu',\n",
       "   0.0010885171188644161,\n",
       "   1e-05,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.6125290393829346,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.4,\n",
       "   1,\n",
       "   19,\n",
       "   'relu',\n",
       "   0.0005686110549199072,\n",
       "   0.001,\n",
       "   0.0001,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [0,\n",
       "   'sigmoid',\n",
       "   5,\n",
       "   0.0,\n",
       "   2,\n",
       "   5,\n",
       "   'relu',\n",
       "   0.0022600000008634307,\n",
       "   0.1,\n",
       "   0.0,\n",
       "   'softmax']),\n",
       " (-0.5, [0, 'sigmoid', 10, 0.1, 2, 1, 'relu', 0.1, 0.0, 0.0, 'sigmoid']),\n",
       " (-0.5,\n",
       "  [0,\n",
       "   'sigmoid',\n",
       "   10,\n",
       "   0.4,\n",
       "   0,\n",
       "   16,\n",
       "   'relu',\n",
       "   0.00020809398721562987,\n",
       "   0.0001,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [0,\n",
       "   'sigmoid',\n",
       "   10,\n",
       "   0.5,\n",
       "   1,\n",
       "   5,\n",
       "   'relu',\n",
       "   0.006939046663228557,\n",
       "   0.1,\n",
       "   1e-05,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [0,\n",
       "   'sigmoid',\n",
       "   30,\n",
       "   0.0,\n",
       "   1,\n",
       "   4,\n",
       "   'relu',\n",
       "   0.00012811430543010682,\n",
       "   0.01,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [0,\n",
       "   'sigmoid',\n",
       "   30,\n",
       "   0.1,\n",
       "   1,\n",
       "   10,\n",
       "   'softmax',\n",
       "   0.000295934664173338,\n",
       "   0.001,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [0,\n",
       "   'sigmoid',\n",
       "   30,\n",
       "   0.4,\n",
       "   0,\n",
       "   22,\n",
       "   'relu',\n",
       "   0.0028784549933077255,\n",
       "   0.0001,\n",
       "   0.0,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [0,\n",
       "   'sigmoid',\n",
       "   30,\n",
       "   0.4,\n",
       "   1,\n",
       "   23,\n",
       "   'softmax',\n",
       "   0.0007960574795425511,\n",
       "   0.1,\n",
       "   1e-05,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [0,\n",
       "   'sigmoid',\n",
       "   30,\n",
       "   0.4,\n",
       "   4,\n",
       "   2,\n",
       "   'relu',\n",
       "   0.017696861159360155,\n",
       "   0.01,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [0,\n",
       "   'sigmoid',\n",
       "   30,\n",
       "   0.5,\n",
       "   2,\n",
       "   6,\n",
       "   'relu',\n",
       "   0.001378797600008766,\n",
       "   0.1,\n",
       "   1e-06,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [0,\n",
       "   'tanh',\n",
       "   10,\n",
       "   0.3,\n",
       "   4,\n",
       "   23,\n",
       "   'tanh',\n",
       "   3.742441161781103e-05,\n",
       "   0.1,\n",
       "   0.001,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [0,\n",
       "   'tanh',\n",
       "   10,\n",
       "   0.5,\n",
       "   1,\n",
       "   19,\n",
       "   'softmax',\n",
       "   0.0002485383445670644,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [0,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.2,\n",
       "   2,\n",
       "   2,\n",
       "   'sigmoid',\n",
       "   1.784930925155351e-05,\n",
       "   1e-06,\n",
       "   0.01,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [0,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.2,\n",
       "   4,\n",
       "   28,\n",
       "   'sigmoid',\n",
       "   0.00011039033572578915,\n",
       "   0.01,\n",
       "   0.01,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [0,\n",
       "   'tanh',\n",
       "   30,\n",
       "   0.2,\n",
       "   5,\n",
       "   1,\n",
       "   'tanh',\n",
       "   0.01666175845497451,\n",
       "   1e-06,\n",
       "   0.0,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [1,\n",
       "   'sigmoid',\n",
       "   5,\n",
       "   0.2,\n",
       "   0,\n",
       "   27,\n",
       "   'tanh',\n",
       "   5.2766018580571085e-05,\n",
       "   0.0,\n",
       "   1e-06,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [1,\n",
       "   'sigmoid',\n",
       "   20,\n",
       "   0.3,\n",
       "   1,\n",
       "   2,\n",
       "   'relu',\n",
       "   0.0001974637303536988,\n",
       "   0.1,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [1,\n",
       "   'sigmoid',\n",
       "   20,\n",
       "   0.4,\n",
       "   0,\n",
       "   12,\n",
       "   'relu',\n",
       "   0.00041910564494525185,\n",
       "   0.1,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [1,\n",
       "   'sigmoid',\n",
       "   20,\n",
       "   0.5,\n",
       "   4,\n",
       "   17,\n",
       "   'sigmoid',\n",
       "   0.058566716470754905,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   10,\n",
       "   0.4,\n",
       "   5,\n",
       "   1,\n",
       "   'relu',\n",
       "   0.023323820376380155,\n",
       "   0.1,\n",
       "   1e-05,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.0,\n",
       "   4,\n",
       "   18,\n",
       "   'softmax',\n",
       "   0.011163947677670304,\n",
       "   0.0,\n",
       "   0.0001,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.2,\n",
       "   0,\n",
       "   13,\n",
       "   'relu',\n",
       "   0.00019448921012515407,\n",
       "   0.0,\n",
       "   0.0001,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.3,\n",
       "   2,\n",
       "   28,\n",
       "   'sigmoid',\n",
       "   0.0005567149782127144,\n",
       "   1e-05,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.3,\n",
       "   3,\n",
       "   27,\n",
       "   'softmax',\n",
       "   5.347434496632001e-05,\n",
       "   1e-05,\n",
       "   1e-05,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.4,\n",
       "   0,\n",
       "   26,\n",
       "   'relu',\n",
       "   0.09393264945770335,\n",
       "   1e-05,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.4,\n",
       "   4,\n",
       "   5,\n",
       "   'sigmoid',\n",
       "   0.0006508931569261795,\n",
       "   0.1,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   30,\n",
       "   0.1,\n",
       "   2,\n",
       "   14,\n",
       "   'softmax',\n",
       "   0.07644322898758832,\n",
       "   0.0001,\n",
       "   0.001,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   30,\n",
       "   0.3,\n",
       "   3,\n",
       "   29,\n",
       "   'softmax',\n",
       "   6.471202706014147e-05,\n",
       "   0.0001,\n",
       "   0.001,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   30,\n",
       "   0.4,\n",
       "   3,\n",
       "   28,\n",
       "   'sigmoid',\n",
       "   0.00022505379183958635,\n",
       "   1e-05,\n",
       "   0.001,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'sigmoid',\n",
       "   5,\n",
       "   0.4,\n",
       "   4,\n",
       "   26,\n",
       "   'sigmoid',\n",
       "   0.08061924293119554,\n",
       "   1e-06,\n",
       "   0.0001,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'sigmoid',\n",
       "   10,\n",
       "   0.0,\n",
       "   1,\n",
       "   25,\n",
       "   'softmax',\n",
       "   0.002509357021135951,\n",
       "   0.001,\n",
       "   1e-06,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'sigmoid',\n",
       "   10,\n",
       "   0.0,\n",
       "   2,\n",
       "   1,\n",
       "   'sigmoid',\n",
       "   0.00013140578738947295,\n",
       "   1e-06,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'sigmoid',\n",
       "   10,\n",
       "   0.3,\n",
       "   4,\n",
       "   23,\n",
       "   'relu',\n",
       "   0.028930747707706518,\n",
       "   0.0001,\n",
       "   0.01,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'sigmoid',\n",
       "   10,\n",
       "   0.4,\n",
       "   2,\n",
       "   26,\n",
       "   'relu',\n",
       "   0.06172153966438575,\n",
       "   0.01,\n",
       "   0.1,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'sigmoid',\n",
       "   20,\n",
       "   0.0,\n",
       "   1,\n",
       "   5,\n",
       "   'relu',\n",
       "   0.00011274421375947315,\n",
       "   0.1,\n",
       "   1e-05,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'sigmoid',\n",
       "   20,\n",
       "   0.4,\n",
       "   0,\n",
       "   5,\n",
       "   'relu',\n",
       "   0.00012594198033815708,\n",
       "   0.1,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'sigmoid',\n",
       "   20,\n",
       "   0.4,\n",
       "   2,\n",
       "   1,\n",
       "   'sigmoid',\n",
       "   0.0049277412312649215,\n",
       "   0.1,\n",
       "   0.0001,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'sigmoid',\n",
       "   20,\n",
       "   0.4,\n",
       "   2,\n",
       "   14,\n",
       "   'sigmoid',\n",
       "   9.822727131863163e-05,\n",
       "   0.1,\n",
       "   1e-05,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'tanh',\n",
       "   5,\n",
       "   0.3,\n",
       "   5,\n",
       "   26,\n",
       "   'sigmoid',\n",
       "   0.007297342071308462,\n",
       "   0.1,\n",
       "   0.0001,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'tanh',\n",
       "   5,\n",
       "   0.4,\n",
       "   4,\n",
       "   5,\n",
       "   'softmax',\n",
       "   0.0074524073359545535,\n",
       "   0.1,\n",
       "   0.0001,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'tanh',\n",
       "   10,\n",
       "   0.1,\n",
       "   1,\n",
       "   20,\n",
       "   'sigmoid',\n",
       "   0.009348729607491687,\n",
       "   0.0001,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'tanh',\n",
       "   10,\n",
       "   0.2,\n",
       "   2,\n",
       "   27,\n",
       "   'sigmoid',\n",
       "   4.866534211668913e-05,\n",
       "   0.001,\n",
       "   1e-06,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'tanh',\n",
       "   10,\n",
       "   0.2,\n",
       "   5,\n",
       "   22,\n",
       "   'tanh',\n",
       "   3.381019033840573e-05,\n",
       "   1e-05,\n",
       "   0.0001,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'tanh',\n",
       "   10,\n",
       "   0.3,\n",
       "   4,\n",
       "   29,\n",
       "   'tanh',\n",
       "   0.00436967941802264,\n",
       "   1e-06,\n",
       "   0.001,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.0,\n",
       "   2,\n",
       "   17,\n",
       "   'relu',\n",
       "   0.03670270036037837,\n",
       "   0.0,\n",
       "   1e-05,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.1,\n",
       "   1,\n",
       "   14,\n",
       "   'tanh',\n",
       "   0.06769282063187196,\n",
       "   0.0,\n",
       "   0.1,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.2,\n",
       "   1,\n",
       "   2,\n",
       "   'sigmoid',\n",
       "   0.002834107313280681,\n",
       "   1e-05,\n",
       "   0.0001,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'tanh',\n",
       "   30,\n",
       "   0.3,\n",
       "   1,\n",
       "   5,\n",
       "   'sigmoid',\n",
       "   0.01100550493849977,\n",
       "   0.1,\n",
       "   1e-05,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'tanh',\n",
       "   30,\n",
       "   0.4,\n",
       "   0,\n",
       "   5,\n",
       "   'sigmoid',\n",
       "   5.790369270025627e-05,\n",
       "   0.0001,\n",
       "   0.0001,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'tanh',\n",
       "   30,\n",
       "   0.4,\n",
       "   2,\n",
       "   28,\n",
       "   'sigmoid',\n",
       "   0.004922235707050135,\n",
       "   0.001,\n",
       "   0.0001,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [2,\n",
       "   'tanh',\n",
       "   30,\n",
       "   0.4,\n",
       "   3,\n",
       "   11,\n",
       "   'sigmoid',\n",
       "   0.005442990544426111,\n",
       "   0.1,\n",
       "   1e-05,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [3,\n",
       "   'sigmoid',\n",
       "   5,\n",
       "   0.1,\n",
       "   5,\n",
       "   10,\n",
       "   'relu',\n",
       "   0.01739989524959703,\n",
       "   1e-06,\n",
       "   0.1,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [3,\n",
       "   'sigmoid',\n",
       "   10,\n",
       "   0.4,\n",
       "   0,\n",
       "   8,\n",
       "   'sigmoid',\n",
       "   6.788182251243355e-05,\n",
       "   0.01,\n",
       "   0.01,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [3,\n",
       "   'sigmoid',\n",
       "   20,\n",
       "   0.0,\n",
       "   3,\n",
       "   6,\n",
       "   'relu',\n",
       "   0.0012526576568917974,\n",
       "   0.1,\n",
       "   1e-05,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [3,\n",
       "   'sigmoid',\n",
       "   20,\n",
       "   0.0,\n",
       "   3,\n",
       "   23,\n",
       "   'tanh',\n",
       "   0.04969734573346957,\n",
       "   0.0001,\n",
       "   0.0001,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [3,\n",
       "   'sigmoid',\n",
       "   30,\n",
       "   0.1,\n",
       "   3,\n",
       "   5,\n",
       "   'relu',\n",
       "   0.04222759767108878,\n",
       "   0.0,\n",
       "   1e-06,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [3,\n",
       "   'sigmoid',\n",
       "   30,\n",
       "   0.3,\n",
       "   1,\n",
       "   5,\n",
       "   'relu',\n",
       "   8.262085891234464e-05,\n",
       "   1e-06,\n",
       "   0.01,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [3,\n",
       "   'sigmoid',\n",
       "   30,\n",
       "   0.4,\n",
       "   4,\n",
       "   14,\n",
       "   'relu',\n",
       "   0.00016562900784579106,\n",
       "   0.1,\n",
       "   1e-05,\n",
       "   'sigmoid']),\n",
       " (-0.5,\n",
       "  [3,\n",
       "   'tanh',\n",
       "   5,\n",
       "   0.4,\n",
       "   4,\n",
       "   19,\n",
       "   'softmax',\n",
       "   0.0007474772084595489,\n",
       "   0.001,\n",
       "   0.1,\n",
       "   'softmax']),\n",
       " (-0.5,\n",
       "  [3,\n",
       "   'tanh',\n",
       "   10,\n",
       "   0.4,\n",
       "   1,\n",
       "   19,\n",
       "   'relu',\n",
       "   7.714168520549007e-05,\n",
       "   0.1,\n",
       "   1e-05,\n",
       "   'sigmoid']),\n",
       " (-0.5, [3, 'tanh', 10, 0.5, 0, 30, 'tanh', 1e-05, 0.1, 0.1, 'softmax']),\n",
       " (-0.5,\n",
       "  [3,\n",
       "   'tanh',\n",
       "   30,\n",
       "   0.4,\n",
       "   4,\n",
       "   5,\n",
       "   'sigmoid',\n",
       "   0.0004075208932464668,\n",
       "   0.1,\n",
       "   0.001,\n",
       "   'sigmoid']),\n",
       " (-0.49958711862564087,\n",
       "  [0,\n",
       "   'sigmoid',\n",
       "   10,\n",
       "   0.1,\n",
       "   0,\n",
       "   30,\n",
       "   'softmax',\n",
       "   0.00021093573203949007,\n",
       "   0.001,\n",
       "   0.1,\n",
       "   'sigmoid']),\n",
       " (-0.4810074269771576,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.0,\n",
       "   2,\n",
       "   16,\n",
       "   'sigmoid',\n",
       "   1.2917497413368875e-05,\n",
       "   1e-06,\n",
       "   0.0,\n",
       "   'sigmoid']),\n",
       " (-0.47800466418266296,\n",
       "  [1,\n",
       "   'tanh',\n",
       "   30,\n",
       "   0.0,\n",
       "   1,\n",
       "   20,\n",
       "   'tanh',\n",
       "   5.541201214647854e-05,\n",
       "   0.001,\n",
       "   0.001,\n",
       "   'sigmoid']),\n",
       " (-0.39569103717803955,\n",
       "  [0,\n",
       "   'tanh',\n",
       "   20,\n",
       "   0.2,\n",
       "   4,\n",
       "   5,\n",
       "   'relu',\n",
       "   2.4260429384163596e-05,\n",
       "   0.1,\n",
       "   0.01,\n",
       "   'sigmoid'])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAHICAYAAABTb96uAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABScElEQVR4nO3de1xU1d4/8M/McBNhBAREEgXCBMUrJqGYFoqmqVknLy/ynto5WhpmaWmGHsVLelLzJ9HR0o6UqcdOWZl4v3GAUPBGeMlLRyBUREQUB2b9/vBhxzQow57NAMPn/Xrxepg1a+9Z8/V54vOsvfbaKiGEABERERE9krq2B0BERERUHzA0EREREZmAoYmIiIjIBAxNRERERCZgaCIiIiIyAUMTERERkQkYmoiIiIhMwNBEREREZAKGJiIiIiITMDQRETVQly5dgkqlwueff17bQyGqFxiaiMjAhQsXMHnyZPj7+8PBwQFarRY9evTAypUrcffu3doeHtURR48exQcffICCgoLaHgqRxdjU9gCIqO74/vvv8fLLL8Pe3h6jR49GcHAw7t+/j8OHD2PmzJk4ffo04uPja3uYVAccPXoUMTExGDt2LFxcXGp7OEQWwdBERACAixcvYsSIEWjVqhX27t2L5s2bS+9NmTIF58+fx/fff1+LIzTfvXv3YGdnB7Wak+xEVH38LwcRAQCWLl2KoqIirFu3ziAwlQsICMC0adOk16WlpViwYAEef/xx2Nvbw9fXF++++y5KSkoMjvP19cXzzz+Pw4cPo1u3bnBwcIC/vz82btwo9fn555+hUqmwYcMGo8/96aefoFKpsGPHDqnt6tWrGD9+PJo1awZ7e3u0a9cO69evNzhu//79UKlU+OqrrzBnzhw89thjcHR0RGFhIQBgy5YtaNu2LRwcHBAcHIzt27dj7Nix8PX1NTiPXq/HRx99hHbt2sHBwQHNmjXD5MmTcfPmzWp/z3IFBQV488034evrC3t7e7Ro0QKjR4/G9evXpT4lJSWYN28eAgICYG9vDx8fH7z99ttG9a1M7969ERwcjLS0NHTv3h2NGjWCn58f4uLiqjwWAPbu3YuePXuicePGcHFxwZAhQ5CZmSm9/8EHH2DmzJkAAD8/P6hUKqhUKly6dMmk8xPVW4KISAjx2GOPCX9/f5P7jxkzRgAQf/nLX8SaNWvE6NGjBQDxwgsvGPRr1aqVaNOmjWjWrJl49913xccffyy6dOkiVCqVOHXqlNTP399fDBgwwOhzxo0bJ1xdXcX9+/eFEELk5uaKFi1aCB8fHzF//nyxdu1aMXjwYAFA/OMf/5CO27dvnwAg2rZtKzp16iRWrFghYmNjxZ07d8SOHTuESqUSHTp0ECtWrBBz584Vrq6uIjg4WLRq1crg81999VVhY2MjJk6cKOLi4sQ777wjGjduLJ588klpTNX5nrdv3xbBwcFCo9GIiRMnirVr14oFCxaIJ598Uhw/flwIIURZWZmIjIwUjo6OYvr06eKTTz4RU6dOFTY2NmLIkCFV/tv06tVLeHt7C09PTzF16lSxatUqER4eLgCIdevWSf0uXrwoAIjPPvtMaktMTBQ2NjbiiSeeEEuXLhUxMTHC3d1duLq6iosXLwohhMjIyBAjR46Uav7FF1+IL774QhQVFVU5NqL6jKGJiMStW7cEAJP+IAshRHp6ugAgXn31VYP2t956SwAQe/fuldpatWolAIiDBw9KbXl5ecLe3l7MmDFDaps9e7awtbUV+fn5UltJSYlwcXER48ePl9omTJggmjdvLq5fv27w2SNGjBBNmjQRxcXFQog/QpO/v7/UVq59+/aiRYsW4vbt21Lb/v37BQCD0HTo0CEBQGzatMng+J07dxq1m/o933//fQFA/Pvf/xZ/ptfrhRBCfPHFF0KtVotDhw4ZvB8XFycAiCNHjhgdW1GvXr0EALF8+XKpraSkRHTq1El4enpKYa+y0FTe58aNG1JbRkaGUKvVYvTo0VLbsmXLBAApSBE1BLw8R0TSJStnZ2eT+v/www8AgOjoaIP2GTNmAIDR2qe2bduiZ8+e0msPDw+0adMGv/76q9Q2fPhw6HQ6/Pvf/5badu3ahYKCAgwfPhwAIITAtm3bMGjQIAghcP36demnX79+uHXrFo4dO2bw2WPGjEGjRo2k19nZ2Th58iRGjx4NJycnqb1Xr15o3769wbFbtmxBkyZN0LdvX4PPCgkJgZOTE/bt21ft77lt2zZ07NgRQ4cONaqrSqWSPjcoKAiBgYEGn/vss88CgNHnVsbGxgaTJ0+WXtvZ2WHy5MnIy8tDWlpapcfk5OQgPT0dY8eOhZubm9TeoUMH9O3bV/p3J2qoGJqICFqtFgBw+/Ztk/pfvnwZarUaAQEBBu1eXl5wcXHB5cuXDdpbtmxpdA5XV1eDdUEdO3ZEYGAgNm/eLLVt3rwZ7u7uUli4du0aCgoKEB8fDw8PD4OfcePGAQDy8vIMPsfPz89o7ACMxl5Z27lz53Dr1i14enoafV5RUZHRZ5nyPS9cuIDg4GCjfn/+3NOnTxt95hNPPFHpd6yMt7c3GjdubNBWfvzD1h6V16ZNmzZG7wUFBeH69eu4c+dOlZ9NZK149xwRQavVwtvbG6dOnarWceUzI1XRaDSVtgshDF4PHz4cCxcuxPXr1+Hs7Ixvv/0WI0eOhI3Ng/9U6fV6AMArr7yCMWPGVHrODh06GLyuOMtUXXq9Hp6enti0aVOl73t4eBi8NvV7mvK57du3x4oVKyp938fHp1rnIyJlMDQREQDg+eefR3x8PJKSkhAWFvbIvq1atYJer8e5c+cQFBQktf/+++8oKChAq1atZI1h+PDhiImJwbZt29CsWTMUFhZixIgR0vseHh5wdnZGWVkZ+vTpI+szysd2/vx5o/f+3Pb4449j9+7d6NGjh1nh68/nrCqcPv7448jIyEBERITJwfTPsrOzcefOHYPZprNnzwKA0R2C5cprk5WVZfTeL7/8And3d+l8csdFVJ/x8hwRAQDefvttNG7cGK+++ip+//13o/cvXLiAlStXAgAGDBgAAPjoo48M+pTPjAwcOFDWGIKCgtC+fXts3rwZmzdvRvPmzfH0009L72s0Grz00kvYtm1bpcHj2rVrVX6Gt7c3goODsXHjRhQVFUntBw4cwMmTJw36Dhs2DGVlZViwYIHReUpLS2Xthv3SSy8hIyMD27dvN3qvfEZq2LBhuHr1Kj799FOjPnfv3jXpEllpaSk++eQT6fX9+/fxySefwMPDAyEhIZUe07x5c3Tq1AkbNmww+G6nTp3Crl27pH93AFJ44o7g1JBwpomIADyY3UhISMDw4cMRFBRksCP40aNHsWXLFowdOxbAg/VHY8aMQXx8PAoKCtCrVy+kpKRgw4YNeOGFF/DMM8/IHsfw4cPx/vvvw8HBARMmTDDaiHLx4sXYt28fQkNDMXHiRLRt2xb5+fk4duwYdu/ejfz8/Co/Y9GiRRgyZAh69OiBcePG4ebNm/j4448RHBxsEKR69eqFyZMnIzY2Funp6YiMjIStrS3OnTuHLVu2YOXKlfjLX/5Sre83c+ZMbN26FS+//DLGjx+PkJAQ5Ofn49tvv0VcXBw6duyIUaNG4euvv8Zrr72Gffv2oUePHigrK8Mvv/yCr7/+Gj/99BO6du36yM/x9vbGkiVLcOnSJTzxxBPYvHkz0tPTER8fD1tb24cet2zZMjz33HMICwvDhAkTcPfuXaxevRpNmjTBBx98IPUrD17vvfceRowYAVtbWwwaNMhoHRWRVanVe/eIqM45e/asmDhxovD19RV2dnbC2dlZ9OjRQ6xevVrcu3dP6qfT6URMTIzw8/MTtra2wsfHR8yePdugjxAPbsUfOHCg0ef06tVL9OrVy6j93LlzAoAAIA4fPlzpGH///XcxZcoU4ePjI2xtbYWXl5eIiIgQ8fHxUp/yLQe2bNlS6Tm++uorERgYKOzt7UVwcLD49ttvxUsvvSQCAwON+sbHx4uQkBDRqFEj4ezsLNq3by/efvttkZ2dLet73rhxQ0ydOlU89thjws7OTrRo0UKMGTPGYBuF+/fviyVLloh27doJe3t74erqKkJCQkRMTIy4detWpd+p4me2a9dO/PzzzyIsLEw4ODiIVq1aiY8//tigX2VbDgghxO7du0WPHj1Eo0aNhFarFYMGDRJnzpwx+pwFCxaIxx57TKjVam4/QA2CSohqrlAkIrJSnTp1goeHBxITE2t7KGbp3bs3rl+/Xu2F/UT0aFzTREQNjk6nQ2lpqUHb/v37kZGRgd69e9fOoIiozuOaJiJqcK5evYo+ffrglVdegbe3N3755RfExcXBy8sLr732Wm0Pj4jqKIYmImpwXF1dERISgn/+85+4du0aGjdujIEDB2Lx4sVo2rRpbQ+PiOoormkiIiIiMgHXNBERERGZgKGJiIiIyARc06QgvV6P7OxsODs78xEDRERE9YQQArdv34a3t7fRhroVMTQpKDs7mw/SJCIiqqd+++03tGjR4qHvMzQpyNnZGcCDomu1Wtnn0el02LVrl/TIBqo5rLXlsNaWw1pbDmttOTVZ68LCQvj4+Eh/xx+GoUlB5ZfktFqt2aHJ0dERWq2W/0dYw1hry2GtLYe1thzW2nIsUeuqltZwITgRERGRCRiaiIiIiEzA0ERERERkAoYmIiIiIhMwNBERERGZgKGJiIiIyAQMTUREREQmYGgiIiIiMgFDExEREZEJuCN4HVdWpkdG5v9w4+YdNHVtjI5BD56JU7EtuI03TmVlm92nJs9d22PsGNQCGg3/fwQiIpKPoakOO5hyHh9vOIBrN4qkNq2TAwCgsOie1KZWq6DXC7P71OS5a3uMHk2dMH38s+j11BMgIiKSg6Gpjjp7uQj/2fi9UXvFIFCuYmAwp09Nnru2x3jtRhHeW/YtFs4czOBERESy8HpFHVSm12NP6rXaHoZVWrl+H8rK9LU9DCIiqocYmuqgk5nZKCoure1hWKW8G7eRkfm/2h4GERHVQwxNddCNgju1PQSrduMm60tERNXH0FQHNXVpXNtDsGpNXVlfIiKqPoamOqh9kDecHLlGvyZ4NnWWtiQgIiKqDoamOkijViPiSY/aHoZVmjb+Ge7XREREsvCvRx31RCsnxEQPhEdTJ4N2rZODtA9RObVapUifmjy3pcdo86dg5NnUmdsNEBGRWXgNqA57ulsAej/VhjuCyxjje0v/g8M/XwAALJw5GOFPBnCGiYiIzMLQVMdpNGp0CW5p1P7nNqX61OS5LTnGigGpbevmDExERGQ2/iUhq1QxJJX9acdwIiIiORiayCpVXOfEHcCJiEgJDE1klSrONP352XRERERyMDSRVdJUnGnSc6aJiIjMx9BEVslwpomhiYiIzMfQRFZJra6wELyMl+eIiMh8DE1klXh5joiIlMbQRFaJM01ERKQ0hiayShrNHzNNXNNERERKYGgiq6QxmGliaCIiIvMxNJFV4j5NRESktHoXmvLz8xEVFQWtVgsXFxdMmDABRUVFD+1/6dIlqFSqSn+2bNkCAMjIyMDIkSPh4+ODRo0aISgoCCtXrrTUV6IaoOZCcCIiUli9e2BvVFQUcnJykJiYCJ1Oh3HjxmHSpElISEiotL+Pjw9ycnIM2uLj47Fs2TI899xzAIC0tDR4enriX//6F3x8fHD06FFMmjQJGo0GU6dOrfHvRMrjs+eIiEhp9So0ZWZmYufOnUhNTUXXrl0BAKtXr8aAAQPw4Ycfwtvb2+gYjUYDLy8vg7bt27dj2LBhcHJyAgCMHz/e4H1/f38kJSXh3//+N0NTPcVnzxERkdLqVWhKSkqCi4uLFJgAoE+fPlCr1UhOTsbQoUOrPEdaWhrS09OxZs2aR/a7desW3NzcHtmnpKQEJSUl0uvCwkIAgE6ng06nq3IsD1N+rDnnaOhU+GN26f4j/j1Ya8thrS2HtbYc1tpyarLWpp6zXoWm3NxceHp6GrTZ2NjAzc0Nubm5Jp1j3bp1CAoKQvfu3R/a5+jRo9i8eTO+//77R54rNjYWMTExRu27du2Co6OjSeN5lMTERLPP0VCdzbop/Z6a+jMK87Ie2Z+1thzW2nJYa8thrS2nJmpdXFxsUr86EZpmzZqFJUuWPLJPZmam2Z9z9+5dJCQkYO7cuQ/tc+rUKQwZMgTz5s1DZGTkI883e/ZsREdHS68LCwvh4+ODyMhIaLVa2ePU6XRITExE3759YWtrK/s8DdkdcQwHjh0CAHTq1BnPhD1RaT/W2nJYa8thrS2Htbacmqx1+ZWiqtSJ0DRjxgyMHTv2kX38/f3h5eWFvLw8g/bS0lLk5+cbrVuqzNatW1FcXIzRo0dX+v6ZM2cQERGBSZMmYc6cOVWez97eHvb29kbttra2ivyDKnWehsjOrkLdVOoq68haWw5rbTmsteWw1pZTE7U29Xx1IjR5eHjAw8Ojyn5hYWEoKChAWloaQkJCAAB79+6FXq9HaGholcevW7cOgwcPrvSzTp8+jWeffRZjxozBwoULq/8lqE6puBBcz4XgRESkgHq1T1NQUBD69++PiRMnIiUlBUeOHMHUqVMxYsQI6c65q1evIjAwECkpKQbHnj9/HgcPHsSrr75qdN5Tp07hmWeeQWRkJKKjo5Gbm4vc3Fxcu3bNIt+LlGewIzj3aSIiIgXUq9AEAJs2bUJgYCAiIiIwYMAAhIeHIz4+Xnpfp9MhKyvLaFHX+vXr0aJFi0rXKW3duhXXrl3Dv/71LzRv3lz6efLJJ2v8+1DNqBiauCM4EREpoU5cnqsONze3h25kCQC+vr4QwviP5KJFi7Bo0aJKj/nggw/wwQcfKDVEqgMqPrCX+zQREZES6t1ME5Ep1GruCE5ERMpiaCKrZPAYFc40ERGRAhiayCoZ3D3HmSYiIlIAQxNZJcMH9nKmiYiIzMfQRFZJw5kmIiJSGEMTWSWDfZq4pomIiBTA0ERWqeKaJl6eIyIiJTA0kVUyvHuOl+eIiMh8DE1klQzvnuNMExERmY+hiawS92kiIiKlMTSRVdJwR3AiIlIYQxNZpYrPnuPlOSIiUgJDE1mlis+eK+XlOSIiUgBDE1mlipfnuLklEREpgaGJrJLBPk2caSIiIgUwNJFVstFwpomIiJTF0ERWiTuCExGR0hiayCpxR3AiIlIaQxNZJc40ERGR0hiayCppuKaJiIgUxtBEVknDu+eIiEhhDE1klQz3aWJoIiIi8zE0kVWquCM4F4ITEZESGJrIKlV89hwXghMRkRIYmsgqGcw0MTQREZECGJrIKvHuOSIiUhpDE1kl3j1HRERKY2giq6RSqaQNLjnTRERESmBoIqtVHpo400REREpgaCKrVb5XUxlnmoiISAEMTWS1ONNERERKYmgiq1V+Bx3XNBERkRIYmshq/XF5jjNNRERkPoYmslrlu4Lz2XNERKQEhiayWuW7gvPZc0REpASGJrJa5Rtc8vIcEREpgaGJrNYfM00MTUREZD6GJrJavHuOiIiUxNBEVouX54iISEkMTWS1ONNERERKYmgiq8UdwYmISEkMTWS1ymea+Ow5IiJSAkMTWS3ONBERkZIYmshq2aj/+F9vrmsiIiJzMTSR1VJXCE2cbSIiInMxNJHVKn/2HMDnzxERkfnqXWjKz89HVFQUtFotXFxcMGHCBBQVFT20/6VLl6BSqSr92bJli1H/GzduoEWLFlCpVCgoKKjBb0I1zWCmiZfniIjITPUuNEVFReH06dNITEzEjh07cPDgQUyaNOmh/X18fJCTk2PwExMTAycnJzz33HNG/SdMmIAOHTrU5FcgC6k408QNLomIyFz1KjRlZmZi586d+Oc//4nQ0FCEh4dj9erV+Oqrr5CdnV3pMRqNBl5eXgY/27dvx7Bhw+Dk5GTQd+3atSgoKMBbb71lia9DNUzDNU1ERKQgm9oeQHUkJSXBxcUFXbt2ldr69OkDtVqN5ORkDB06tMpzpKWlIT09HWvWrDFoP3PmDObPn4/k5GT8+uuvJo2npKQEJSUl0uvCwkIAgE6ng06nM+kclSk/1pxzEKD6Y6IJJSX3odPZGvVhrS2HtbYc1tpyWGvLqclam3rOehWacnNz4enpadBmY2MDNzc35ObmmnSOdevWISgoCN27d5faSkpKMHLkSCxbtgwtW7Y0OTTFxsYiJibGqH3Xrl1wdHQ06RyPkpiYaPY5GrK8vN+l3xN374Gz48P/1521thzW2nJYa8thrS2nJmpdXFxsUr86EZpmzZqFJUuWPLJPZmam2Z9z9+5dJCQkYO7cuQbts2fPRlBQEF555ZVqnW/27NmIjo6WXhcWFsLHxweRkZHQarWyx6nT6ZCYmIi+ffvC1tZ4doRMk3r2B5y7cg4A0Lt3bzRzN/43Ya0th7W2HNbaclhry6nJWpdfKapKnQhNM2bMwNixYx/Zx9/fH15eXsjLyzNoLy0tRX5+Pry8vKr8nK1bt6K4uBijR482aN+7dy9OnjyJrVu3AgCEeHCnlbu7O957771KZ5MAwN7eHvb29kbttra2ivyDKnWehsrGRiP9rlJpHllL1tpyWGvLYa0th7W2nJqotannqxOhycPDAx4eHlX2CwsLQ0FBAdLS0hASEgLgQeDR6/UIDQ2t8vh169Zh8ODBRp+1bds23L17V3qdmpqK8ePH49ChQ3j88cer+W2orih/9hzAHcGJiMh8dSI0mSooKAj9+/fHxIkTERcXB51Oh6lTp2LEiBHw9vYGAFy9ehURERHYuHEjunXrJh17/vx5HDx4ED/88IPRef8cjK5fvy59nouLS819IapRGjW3HCAiIuXUqy0HAGDTpk0IDAxEREQEBgwYgPDwcMTHx0vv63Q6ZGVlGS3qWr9+PVq0aIHIyEhLD5lqieFME0MTERGZp17NNAGAm5sbEhISHvq+r6+vtCapokWLFmHRokUmfUbv3r0rPQfVL4bPnuO/JxERmafezTQRmYqX54iISEkMTWS1ONNERERKYmgiq1Xx2XNc00REROZiaCKrxWfPERGRkhiayGpxnyYiIlISQxNZLTUXghMRkYIYmshqVZxpKuNMExERmYmhiayWwUwT1zQREZGZGJrIalVcCM41TUREZC6GJrJaGs40ERGRghiayGoZrmliaCIiIvMwNJHVMtgRnJfniIjITAxNZLUMdgTn5TkiIjITQxNZLcOZJoYmIiIyD0MTWS0b3j1HREQKkh2a7t69i+LiYun15cuX8dFHH2HXrl2KDIzIXNyniYiIlCQ7NA0ZMgQbN24EABQUFCA0NBTLly/HkCFDsHbtWsUGSCQXdwQnIiIlyQ5Nx44dQ8+ePQEAW7duRbNmzXD58mVs3LgRq1atUmyARHJxpomIiJQkOzQVFxfD2dkZALBr1y68+OKLUKvVeOqpp3D58mXFBkgkV8WZJq5pIiIic8kOTQEBAfjmm2/w22+/4aeffkJkZCQAIC8vD1qtVrEBEsllsCM4754jIiIzyQ5N77//Pt566y34+voiNDQUYWFhAB7MOnXu3FmxARLJxZkmIiJSko3cA//yl78gPDwcOTk56Nixo9QeERGBoUOHKjI4InNwTRMRESlJdmgCAC8vL3h5eRm0devWzawBESlFw80tiYhIQdUKTdHR0Sb3XbFiRbUHQ6Qkgx3By3h5joiIzFOt0HT8+HGT+qlUqqo7EdUwg2fPcaaJiIjMVK3QtG/fvpoaB5HiDC7PcU0TERGZic+eI6vFu+eIiEhJXNNEVkvNfZqIiEhBXNNEVovPniMiIiVxTRNZLe7TRERESuKaJrJaFReCc00TERGZy6zNLQHgzJkzuHLlCu7fv2/QPnjwYHNPTWQWDWeaiIhIQbJD06+//oqhQ4fi5MmTUKlUEOLB/ydfvp6prKxMmRESycS754iISEmyL89NmzYNfn5+yMvLg6OjI06fPo2DBw+ia9eu2L9/v4JDJJKn4pqmUs40ERGRmWTPNCUlJWHv3r1wd3eHWq2GWq1GeHg4YmNj8cYbb5h8px1RTTGcaWJoIiIi88ieaSorK4OzszMAwN3dHdnZ2QCAVq1aISsrS5nREZnB8IG9vDxHRETmkT3TFBwcjIyMDPj5+SE0NBRLly6FnZ0d4uPj4e/vr+QYiWThs+eIiEhJskPTnDlzcOfOHQDA/Pnz8fzzz6Nnz55o2rQpNm/erNgAieRS89lzRESkINmhqV+/ftLvAQEB+OWXX5Cfnw9XV1fuCE51Ai/PERGRkmSvaYqNjcX69esN2tzc3PDZZ59hyZIlZg+MyFzcEZyIiJQkOzR98sknCAwMNGpv164d4uLizBoUkRK4TxMRESlJdmjKzc1F8+bNjdo9PDyQk5Nj1qCIlGCwIzgXghMRkZlkhyYfHx8cOXLEqP3IkSPw9vY2a1BESlDz2XNERKQg2QvBJ06ciOnTp0On0+HZZ58FAOzZswdvv/02ZsyYodgAieRSq1VQqQAhuKaJiIjMJzs0zZw5Ezdu3MDf/vY36WG9Dg4OeOeddzB79mzFBkhkDo1GjdJSPe+eIyIis8kOTSqVCkuWLMHcuXORmZmJRo0aoXXr1rC3t1dyfERmeXCJTs+ZJiIiMpvsNU3lnJyc8OSTTyI4ONgigSk/Px9RUVHQarVwcXHBhAkTUFRU9ND+ly5dgkqlqvRny5YtBn0///xzdOjQAQ4ODvD09MSUKVNq+utQDStfDM4dwYmIyFyyZ5pqS1RUFHJycpCYmAidTodx48Zh0qRJSEhIqLS/j4+P0d188fHxWLZsGZ577jmpbcWKFVi+fDmWLVuG0NBQ3LlzB5cuXarJr0IWUL7BZVkZL88REZF56lVoyszMxM6dO5GamoquXbsCAFavXo0BAwbgww8/rPSuPY1GAy8vL4O27du3Y9iwYXBycgIA3Lx5E3PmzMF3332HiIgIqV+HDh1q8NuQJZTv1cSZJiIiMle9Ck1JSUlwcXGRAhMA9OnTB2q1GsnJyRg6dGiV50hLS0N6ejrWrFkjtSUmJkKv1+Pq1asICgrC7du30b17dyxfvhw+Pj4PPVdJSQlKSkqk14WFhQAAnU4HnU4n5ytKx1f8nyRf+a4DpWX6SuvJWlsOa205rLXlsNaWU5O1NvWc9So05ebmwtPT06DNxsYGbm5uyM3NNekc69atQ1BQELp37y61/frrr9Dr9Vi0aBFWrlyJJk2aYM6cOejbty9OnDgBOzu7Ss8VGxuLmJgYo/Zdu3bB0dGxGt+scomJiWafo6HT/d+dnXfuFOOHH354aD/W2nJYa8thrS2Htbacmqh1cXGxSf3qRGiaNWtWlc+ry8zMNPtz7t69i4SEBMydO9egXa9/MAuxatUqREZGAgC+/PJLeHl5Yd++fQYPJ65o9uzZiI6Oll4XFhbCx8cHkZGR0Gq1ssep0+mQmJiIvn37wtbWVvZ5CNjw/ToU3S2Cnb09BgwYYPQ+a205rLXlsNaWw1pbTk3WuvxKUVVkh6bU1FTMmjUL165dQ0BAADp16iT9tGzZslrnmjFjBsaOHfvIPv7+/vDy8kJeXp5Be2lpKfLz843WLVVm69atKC4uxujRow3ayx8H07ZtW6nNw8MD7u7uuHLlykPPZ29vX+kdg7a2tor8gyp1noZMo9EAeLAj+KNqyVpbDmttOay15bDWllMTtTb1fLJD06hRo9CyZUtMmjQJFy9exIEDB7By5UrcvHkTrq6uuHHjhsnn8vDwgIeHR5X9wsLCUFBQgLS0NISEhAAA9u7dC71ej9DQ0CqPX7duHQYPHmz0WT169AAAZGVloUWLFgAebG1w/fp1tGrVyuTvQXVP+ZYD3KeJiIjMJTs0/fbbb/j+++/x+OOPG7RfvnwZ6enp5o6rUkFBQejfvz8mTpyIuLg46HQ6TJ06FSNGjJDunLt69SoiIiKwceNGdOvWTTr2/PnzOHjwYKXrWp544gkMGTIE06ZNQ3x8PLRaLWbPno3AwEA888wzNfJdyDL+uHuOWw4QEZF5ZG9uGRYWhqtXrxq1t2rVCkOGDDFrUI+yadMmBAYGIiIiAgMGDEB4eDji4+Ol93U6HbKysowWda1fvx4tWrSQ1iz92caNGxEaGoqBAweiV69esLW1xc6dOzndWs+py2eauOUAERGZSfZM05tvvon58+fj66+/hpubm5JjeiQ3N7eHbmQJAL6+vhDCeFZh0aJFWLRo0UOP02q1WLduHdatW6fIOKluKJ9p4rPniIjIXLJD06BBg6BSqaRLW2FhYejcuTPat2//0Fv0iSxNCk1c00RERGaSHZrOnz+PjIwM6WfRokW4dOkSbG1t0aZNG5w4cULJcRLJouaz54iISCGyQ5O/vz/8/f0NduEuLCxERkYGAxPVGTb/tyW4EA8Wg5eHKCIioupSdHNLrVaLnj17omfPnkqelkg2tfqPex30ej3Uak0tjoaIiOoz2XfPEdUHGs0fM0tc10REROZgaCKrVnGmiXfQERGRORiayKoZzDRxMTgREZlBVmjS6XSIiIjAuXPnlB4PkaI0BmuaONNERETyyQpNtra2vEOO6oWKoYlrmoiIyByyL8+98sor3D2b6ryKWwxwpomIiMwhe8uB0tJSrF+/Hrt370ZISAgaN25s8P6KFSvMHhyRucp3BAeAUs40ERGRGWSHplOnTqFLly4AgLNnzxq8p1JxA0GqGwxnmhiaiIhIPtmhad++fUqOg6hGVJxpKivj5TkiIpLPrC0HDh06hFdeeQXdu3fH1atXAQBffPEFDh8+rMjgiMyl4UwTEREpRHZo2rZtG/r164dGjRrh2LFjKCkpAQDcunULixYtUmyAROYwmGliaCIiIjPIDk1///vfERcXh08//RS2trZSe48ePXDs2DFFBkdkLu4ITkRESpEdmrKysvD0008btTdp0gQFBQXmjIlIMRUvz3GfJiIiMofs0OTl5YXz588btR8+fBj+/v5mDYpIKQYzTVwITkREZpAdmiZOnIhp06YhOTkZKpUK2dnZ2LRpE9566y389a9/VXKMRLJVfPYcF4ITEZE5ZG85MGvWLOj1ekRERKC4uBhPP/007O3t8dZbb+H1119XcoxEsvExKkREpBTZoUmlUuG9997DzJkzcf78eRQVFaFt27ZwcnJScnxEZql49xwfo0JEROaQHZquXLkCHx8f2NnZoW3btkbvtWzZ0uzBEZmr4o7g3HKAiIjMIXtNk5+fH65du2bUfuPGDfj5+Zk1KCKlGO7TxJkmIiKST3ZoEkJU+oy5oqIiODg4mDUoIqWoueUAEREppNqX56KjowE8WNM0d+5cODo6Su+VlZUhOTkZnTp1UmyAROaouBCca5qIiMgc1Q5Nx48fB/BgpunkyZOws7OT3rOzs0PHjh3x1ltvKTdCIjNwc0siIlJKtULTiRMnsHv3bmg0GowbNw6rVq2Cs7NzTY2NyGx89hwRESmlWmuaOnfujPz8fADAgQMHcP/+/RoZFJFS+Ow5IiJSSrVCk4uLC3799VcAwKVLl7jDMtV5BjuC8/IcERGZoVqX51566SX06tULzZs3h0qlQteuXaHRaCrtWx6uiGqT4UwTQxMREclXrdAUHx+PF198EefPn8cbb7yBiRMnck0T1Wk2vHuOiIgUUu275/r37w8ASEtLw7Rp0xiaqE7jPk1ERKQU2Y9R+eyzzwAAZ86cwZUrV4wWhQ8ePNi8kREpgDuCExGRUmSHposXL+KFF17AyZMnoVKpIMSDP0jlu4SXlZUpM0IiM3CmiYiIlCL7MSpvvPEG/Pz8kJeXB0dHR5w+fRoHDx5E165dsX//fgWHSCRfxZkmrmkiIiJzyJ5pSkpKwt69e+Hu7g61Wg21Wo3w8HDExsbijTfekHYOJ6pNBjuC8+45IiIyg+yZprKyMmkRuLu7O7KzswEArVq1QlZWljKjIzITZ5qIiEgpsmeagoODkZGRAT8/P4SGhmLp0qWws7NDfHw8/P39lRwjkWxc00REREqRHZrmzJmDO3fuAADmz5+P559/Hj179kTTpk2xefNmxQZIZA4NN7ckIiKFyA5N/fr1k34PCAjAL7/8gvz8fLi6ukp30BHVNoMdwct4eY6IiOSTHZoq4+bmpuTpiMxm8Ow5zjQREZEZZC8EJ6oPDC7PcU0TERGZgaGJrBrvniMiIqUwNJFVU3OfJiIiUghDE1k1PnuOiIiUYtZCcJ1Oh9zcXBQXF8PDw4MLwanO4T5NRESklGrPNN2+fRtr165Fr169oNVq4evri6CgIHh4eKBVq1aYOHEiUlNTa2KsAID8/HxERUVBq9XCxcUFEyZMQFFR0UP7X7p0CSqVqtKfLVu2SP1SU1MREREBFxcXuLq6ol+/fsjIyKix70GWUXEhONc0ERGROaoVmlasWAFfX1989tln6NOnD7755hukp6fj7NmzSEpKwrx581BaWorIyEj0798f586dU3zAUVFROH36NBITE7Fjxw4cPHgQkyZNemh/Hx8f5OTkGPzExMTAyckJzz33HACgqKgI/fv3R8uWLZGcnIzDhw/D2dkZ/fr1g06nU/w7kOVoONNEREQKqdbludTUVBw8eBDt2rWr9P1u3bph/PjxiIuLw2effYZDhw6hdevWigwUADIzM7Fz506kpqaia9euAIDVq1djwIAB+PDDD+Ht7W10jEajgZeXl0Hb9u3bMWzYMDg5OQGAtDHn/Pnz4ePjAwCYN28eOnTogMuXLyMgIECx70CWxbvniIhIKdUKTV9++aX0++3bt6UH9v6Zvb09XnvtNfNGVomkpCS4uLhIgQkA+vTpA7VajeTkZAwdOrTKc6SlpSE9PR1r1qyR2tq0aYOmTZti3bp1ePfdd1FWVoZ169YhKCgIvr6+Dz1XSUkJSkpKpNeFhYUAHqz1MmeGqvxYznKZT68vk37XlZYa1ZS1thzW2nJYa8thrS2nJmtt6jllLwTv2bMndu7caTSLU5Nyc3Ph6elp0GZjYwM3Nzfk5uaadI7yMNS9e3epzdnZGfv378cLL7yABQsWAABat26Nn376CTY2Dy9RbGwsYmJijNp37doFR0dHk8bzKImJiWafo6ErvPPH/yFcvZqNH374odJ+rLXlsNaWw1pbDmttOTVR6+LiYpP6yQ5NnTt3RmhoKH766ScEBgZK7enp6Xj33Xcf+sepMrNmzcKSJUse2SczM1PuUCV3795FQkIC5s6da9Q+YcIE9OjRA19++SXKysrw4YcfYuDAgUhNTUWjRo0qPd/s2bMRHR0tvS4sLISPjw8iIyOh1Wplj1On0yExMRF9+/aFra2t7PMQcD2/CJ9sWwcA8PRshgEDBhi8z1pbDmttOay15bDWllOTtS6/UlQV2aHps88+w7x58xAeHo5vvvkGnp6emDNnDrZt22b0h6kqM2bMwNixYx/Zx9/fH15eXsjLyzNoLy0tRX5+vkkzXlu3bkVxcTFGjx5t0J6QkIBLly4hKSlJesBrQkICXF1d8Z///AcjRoyo9Hz29vawt7c3are1tVXkH1Sp8zRkDg520u8CeGg9WWvLYa0th7W2HNbacmqi1qaez6x9mmJiYmBvb4++ffuirKwMERERSEpKQrdu3ap1Hg8PD3h4eFTZLywsDAUFBUhLS0NISAgAYO/evdDr9QgNDa3y+HXr1mHw4MFGn1VcXAy1Wg2V6o87rcpf8yGv9Zuaz54jIiKFyN4R/Pfff8e0adPw97//HW3btoWtrS3Gjh1b7cBUHUFBQejfvz8mTpyIlJQUHDlyBFOnTsWIESOkO+euXr2KwMBApKSkGBx7/vx5HDx4EK+++qrRefv27YubN29iypQpyMzMxOnTpzFu3DjY2NjgmWeeqbHvQzXP4IG9vHuOiIjMIDs0+fn54eDBg9iyZQvS0tKwbds2TJo0CcuWLVNyfEY2bdqEwMBAREREYMCAAQgPD0d8fLz0vk6nQ1ZWltGirvXr16NFixaIjIw0OmdgYCC+++47nDhxAmFhYejZsyeys7Oxc+dONG/evEa/D9Us7ghORERKkX15bv369QZrffr37499+/bh+eefx6VLlwxu6VeSm5sbEhISHvq+r68vhDCeUVi0aBEWLVr00OP69u2Lvn37KjJGqju4TxMRESlF9kxTZYuju3TpgqNHj2Lv3r1mDYpIKQY7gnN9GhERmaFaoenKlStV9vH19cXRo0cBPFhfRFSb1Hz2HBERKaRaoenJJ5/E5MmTH/lA3lu3bmHr1q0IDg7Gtm3bzB4gkTnUahXKb4rkmiYiIjJHtdY0ZWZmYuHChejbty8cHBwQEhICb29vODg44ObNmzhz5gxOnz6NLl26YOnSpdXer4moJmg0apSW6nn3HBERmaVaM02LFy/GwoULkZOTgzVr1qB169a4fv06zp07BwCIiopCWloakpKSGJiozii/RMeZJiIiMke1Zpo++ugjvPXWW/D09MR3332H//f//p8iz1gjqknli8G5USkREZmjWjNN3t7eOH78OADgiy++wJ07d2pkUERK0kgzTbw8R0RE8lUrNM2YMQODBg1Cz549AQD/+te/kJKSgrt379bI4IiUUL5XE2eaiIjIHNUKTa+//jp+/vln9O/fH0IIrFmzBt27d4dWq0VQUBBGjBiBxYsX48cff6yp8RJVW/mu4NyniYiIzFHtHcE7dOiADh064PPPP0dSUhIaN26MEydOID09Henp6fjPf/6DhQsX4vbt2zUxXqJqK59p4t1zRERkDtmPUSm/Yw4AQkNDERoaKr2u7DEmRLVFmmni3XNERGQG2Y9ReRSVSlV1JyILKV8Izh3BiYjIHLJnmgBgz5492LNnD/Ly8owW2a5fv96sgREpRcOZJiIiUoDs0BQTE4P58+eja9euaN68OWeXqM764+45zjQREZF8skNTXFwcPv/8c4waNUrJ8RApjnfPERGREmSvabp//z66d++u5FiIagTvniMiIiXIDk2vvvoqEhISlBwLUY3gs+eIiEgJsi/P3bt3D/Hx8di9ezc6dOgAW1tbg/dXrFhh9uCIlKDR8NlzRERkPtmh6cSJE+jUqRMA4NSpUwbvcVE41SXlWw4I8WAxePkaJyIiouqQHZr27dun5DiIakx5aAIezDap1ZpaHA0REdVXNbK5JVFdUnFmieuaiIhIrmrNNEVHR2PBggVo3LgxoqOjH9mXa5qorii/ew7gHXRERCRftULT8ePHodPppN8fhmuaqC4xmGniYnAiIpKpWqGp4jomrmmi+sJGU3FNE2eaiIhIHrOePXfv3j2cOHHC6NlzKpUKgwYNMntwRErgmiYiIlKC7NC0c+dOjBo1Cjdu3DB6T6VSoayszKyBESnF8O45zjQREZE8su+ee/311zFs2DDk5ORAr9cb/DAwUV1ScaaplDNNREQkk+zQ9PvvvyM6OhrNmjVTcjxEitNoDPdpIiIikkN2aPrLX/6C/fv3KzgUoppR8fJcWRkvzxERkTyy1zR9/PHHePnll3Ho0CG0b9/e6Nlzb7zxhtmDI1JC+bPnAM40ERGRfLJD05dffoldu3bBwcEB+/fvN9ibSaVSMTRRnaGuONPE0ERERDLJDk3vvfceYmJiMGvWLIM/SkR1DXcEJyIiJchOO/fv38fw4cMZmKjO4z5NRESkBNmJZ8yYMdi8ebOSYyGqEdyniYiIlCD78lxZWRmWLl2Kn376CR06dDBaCM4H9lJdoeFMExERKUB2aDp58iQ6d+4MADh16pTBe3xgL9UlGj57joiIFCA7NPGBvVRfcE0TEREpgau4yerx7jkiIlICQxNZPYOZJu7TREREMjE0kdXjs+eIiEgJDE1k9fjsOSIiUgJDE1m9ilsOcKaJiIjkYmgiq1dx1/pS3j1HREQyMTSR1eM+TUREpASGJrJ63KeJiIiUwNBEVo8zTUREpIR6F5ry8/MRFRUFrVYLFxcXTJgwAUVFRY88Jjc3F6NGjYKXlxcaN26MLl26YNu2bWafl+oHDfdpIiIiBdS70BQVFYXTp08jMTERO3bswMGDBzFp0qRHHjN69GhkZWXh22+/xcmTJ/Hiiy9i2LBhOH78uFnnpfrBYMsBzjQREZFM9So0ZWZmYufOnfjnP/+J0NBQhIeHY/Xq1fjqq6+QnZ390OOOHj2K119/Hd26dYO/vz/mzJkDFxcXpKWlmXVeqh+4pomIiJQg+4G9tSEpKQkuLi7o2rWr1NanTx+o1WokJydj6NChlR7XvXt3bN68GQMHDoSLiwu+/vpr3Lt3D7179zbrvCUlJSgpKZFeFxYWAgB0Oh10Op3s71l+rDnnoIr+mF3S6UoN6spaWw5rbTmsteWw1pZTk7U29Zz1KjTl5ubC09PToM3GxgZubm7Izc196HFff/01hg8fjqZNm8LGxgaOjo7Yvn07AgICzDpvbGwsYmJijNp37doFR0fH6ny1SiUmJpp9DgJ+uXRb+v3U6dNoJIxnD1lry2GtLYe1thzW2nJqotbFxcUm9asToWnWrFlYsmTJI/tkZmbKPv/cuXNRUFCA3bt3w93dHd988w2GDRuGQ4cOoX379rLPO3v2bERHR0uvCwsL4ePjg8jISGi1Wtnn1el0SExMRN++fWFrayv7PPSAU8p5fHfwewBAmzaBGDAgRHqPtbYc1tpyWGvLYa0tpyZrXX6lqCp1IjTNmDEDY8eOfWQff39/eHl5IS8vz6C9tLQU+fn58PLyqvS4Cxcu4OOPP8apU6fQrl07AEDHjh1x6NAhrFmzBnFxcbLOCwD29vawt7c3are1tVXkH1Sp8zR0dgY1VFVaU9baclhry2GtLYe1tpyaqLWp56sTocnDwwMeHh5V9gsLC0NBQQHS0tIQEvJgtmDv3r3Q6/UIDQ2t9JjyKbeKj9IAAI1GIz2HTM55qf7QaLjlABERma9e3T0XFBSE/v37Y+LEiUhJScGRI0cwdepUjBgxAt7e3gCAq1evIjAwECkpKQCAwMBABAQEYPLkyUhJScGFCxewfPlyJCYm4oUXXjD5vFR/VQzMvHuOiIjkqlehCQA2bdqEwMBAREREYMCAAQgPD0d8fLz0vk6nQ1ZWljTDZGtrix9++AEeHh4YNGgQOnTogI0bN2LDhg0YMGCAyeel+qviPk3cEZyIiOSqE5fnqsPNzQ0JCQkPfd/X1xdCGP5hbN26tdEO4NU9L9VfGu7TRERECqh3M01E1cVnzxERkRIYmsjqcUdwIiJSAkMTWb2KM0189hwREcnF0ERWz2CmiVsOEBGRTAxNZPW4pomIiJTA0ERWj3fPERGREhiayOoZ7tPE0ERERPIwNJHVM1gIXsbLc0REJA9DE1k9LgQnIiIlMDSR1TOcaWJoIiIieRiayOrx2XNERKQEhiayehoNL88REZH5GJrI6qk500RERApgaCKrV/HyXCnXNBERkUwMTWT1Kt49x5kmIiKSi6GJrB7vniMiIiUwNJHV03CmiYiIFMDQRFav4kJw3j1HRERyMTSR1VOrVVD932QTQxMREcnF0EQNQvm6Jj57joiI5GJoogah/BKdnjNNREQkE0MTNQjli8F59xwREcnF0EQNQvkGl7w8R0REcjE0UYNQvqaJl+eIiEguhiZqEMp3Befdc0REJBdDEzUI0t1z3NySiIhkYmiiBkHNheBERGQmhiZqEDTSlgOcaSIiInkYmqhB4JYDRERkLoYmahD+uHuOM01ERCQPQxM1CLx7joiIzMXQRA0C754jIiJzMTRRg6CWdgTnTBMREcnD0EQNgkbz4PIcdwQnIiK5GJqoQSjfckAILgYnIiJ5GJqoQSgPTQBnm4iISB6GJmoQyu+eA7iuiYiI5GFoogah/O45gHfQERGRPAxN1CAYzDTx8hwREcnA0EQNgo2m4pomzjQREVH1MTRRg8A1TUREZC6GJmoQDO+e40wTERFVH0MTNQgVZ5pKOdNEREQyMDRRg6DRcJ8mIiIyD0MTNQgVL8+VlfHyHBERVR9DEzUI5c+eAzjTRERE8tS70JSfn4+oqChotVq4uLhgwoQJKCoqeuQxubm5GDVqFLy8vNC4cWN06dIF27Ztk96/dOkSJkyYAD8/PzRq1AiPP/445s2bh/v379f01yELUVecaWJoIiIiGWxqewDVFRUVhZycHCQmJkKn02HcuHGYNGkSEhISHnrM6NGjUVBQgG+//Rbu7u5ISEjAsGHD8PPPP6Nz58745ZdfoNfr8cknnyAgIACnTp3CxIkTcefOHXz44YcW/HZUU7gjOBERmatehabMzEzs3LkTqamp6Nq1KwBg9erVGDBgAD788EN4e3tXetzRo0exdu1adOvWDQAwZ84c/OMf/0BaWho6d+6M/v37o3///lJ/f39/ZGVlYe3atQxNVoL7NBERkbnqVWhKSkqCi4uLFJgAoE+fPlCr1UhOTsbQoUMrPa579+7YvHkzBg4cCBcXF3z99de4d+8eevfu/dDPunXrFtzc3B45npKSEpSUlEivCwsLAQA6nQ46na4a38xQ+bHmnIMMqSr8fv++zqjGrHXNY60th7W2HNbacmqy1qaes16FptzcXHh6ehq02djYwM3NDbm5uQ897uuvv8bw4cPRtGlT2NjYwNHREdu3b0dAQECl/c+fP4/Vq1dXOcsUGxuLmJgYo/Zdu3bB0dHRhG/0aImJiWafgx64fPma9Pvhw0dw8Wwjg/dZa8thrS2HtbYc1tpyaqLWxcXFJvWrE6Fp1qxZWLJkySP7ZGZmyj7/3LlzUVBQgN27d8Pd3R3ffPMNhg0bhkOHDqF9+/YGfa9evYr+/fvj5ZdfxsSJEx953tmzZyM6Olp6XVhYCB8fH0RGRkKr1coer06nQ2JiIvr27QtbW1vZ56E/XLl5GGmZaQCA0NCn0CHoMQCstSWx1pbDWlsOa205NVnr8itFVakToWnGjBkYO3bsI/v4+/vDy8sLeXl5Bu2lpaXIz8+Hl5dXpcdduHABH3/8MU6dOoV27doBADp27IhDhw5hzZo1iIuLk/pmZ2fjmWeeQffu3REfH1/luO3t7WFvb2/Ubmtrq8g/qFLnIcDWRiP9rlKpjerKWlsOa205rLXlsNaWUxO1NvV8dSI0eXh4wMPDo8p+YWFhKCgoQFpaGkJCQgAAe/fuhV6vR2hoaKXHlE+5VbzlHAA0Go3Bfj1Xr17FM888g5CQEHz22WdG/al+491zRERkrnqVDIKCgtC/f39MnDgRKSkpOHLkCKZOnYoRI0ZId85dvXoVgYGBSElJAQAEBgYiICAAkydPRkpKCi5cuIDly5cjMTERL7zwgnRM79690bJlS3z44Ye4du0acnNzH7lOiuoXg7vnuE8TERHJUCdmmqpj06ZNmDp1KiIiIqBWq/HSSy9h1apV0vs6nQ5ZWVnSDJOtrS1++OEHzJo1C4MGDUJRURECAgKwYcMGDBgwAMCDRWXnz5/H+fPn0aJFC4PPE4KzEtaAz54jIiJz1bvQ5Obm9siNLH19fY2CTuvWrQ12AP+zsWPHVrmmiuo3PnuOiIjMVa8uzxHJpVHz2XNERGQehiZqECou7C/ljuBERCQDQxM1CIZrmnh5joiIqo+hiRoEPnuOiIjMxdBEDQJnmoiIyFwMTdQgaLhPExERmYmhiRoEgy0HONNEREQyMDRRg8A1TUREZC6GJmoQuKaJiIjMxdBEDQJnmoiIyFwMTdQg2PDZc0REZCaGJmoQ1FwITkREZmJoogZBo+GWA0REZB6GJmoQDGaauKaJiIhksKntARBZggp/zDRl5xZIwSn99P+QefE2vE//Dx3b+eBUVjZu3LyDpq6N0TGoBQAgI/N/UltwG+8q+8g9zpJ9auPzWeu6W+varlFdrWN1a92lfSvZ363iHb5UdzE0kdU78N+zWPZJovT6p4OZSDp2EQBQWHQPALDj0Dao1SqD7Qi0Tg4GfQCY1EfucZbsU5ufz1rXvVrXdo2sYYw7Dm2TfW6Ppk6YPv5Z9HrqCVDdphJCcFWsQgoLC9GkSRPcunULWq1W9nl0Oh1++OEHDBgwALa2tgqOsOE58N+zeG/Zt7U9DCKiKi2cOZjB6RFq8m+jqX+/OdNEVqusTI+P1u+t7WEQEZlkydpdaOxoj87tfAA0rMucpowx40+XQmvjkiZDE1mtjMz/4dqNotoeBhGRSQqL7mF6zJYGfZnTlON2HNpWa5c0ufKMrNaNm3dqewhERNVWWHTPIDAAxo9/ktunJs9tyTFeu1GE95Z9iwP/PQtLYmgiq9XUtXFtD4GIiGrQyvX7LLqNDEMTWa2OQS3g0dSptodBREQ1JO/GbWRk/s9in8fQRFZLo1Fj+vhna3sYRERUgyy5FIOhiaxar6eewMKZg41mnLRODtJiw3JqtUqRPjV57oY0Rmv5HrX9+Rxj3R8jmceSSzF49xxZvV5PPYHwJwMqvaX12MnL2LP/CCJ69+Au1TX8+ax13a11bdeortaxurWuzo7gx09fwfvLdxgteKbq8WzqLNXUEri5pYK4uWX9w1pbDmttOay15ZhTa26+az6lNgQ19e83L88RERHVAi4fkD9Gz6bOtbKDOi/PERER1ZJHLR9oSJc5TdsR/DeDS6HcEZyIiKiB0WjU6BLc0qj9z21K9anJc9fkGDu1a4Hsy87o1K5FrQQmgJfniIiIiEzC0ERERERkAoYmIiIiIhMwNBERERGZgKGJiIiIyAQMTUREREQmYGgiIiIiMgFDExEREZEJGJqIiIiITMAdwRVU/uzjwsJCs86j0+lQXFyMwsJCPmyzhrHWlsNaWw5rbTmsteXUZK3L/26X/x1/GIYmBd2+fRsA4OPjU8sjISIiouq6ffs2mjRp8tD3VaKqWEUm0+v1yM7OhrOzM1QqVdUHPERhYSF8fHzw22+/QavVKjhC+jPW2nJYa8thrS2Htbacmqy1EAK3b9+Gt7c31OqHr1ziTJOC1Go1WrRoodj5tFot/4/QQlhry2GtLYe1thzW2nJqqtaPmmEqx4XgRERERCZgaCIiIiIyAUNTHWRvb4958+bB3t6+todi9Vhry2GtLYe1thzW2nLqQq25EJyIiIjIBJxpIiIiIjIBQxMRERGRCRiaiIiIiEzA0ERERERkAoamOmbNmjXw9fWFg4MDQkNDkZKSUttDqvdiY2Px5JNPwtnZGZ6ennjhhReQlZVl0OfevXuYMmUKmjZtCicnJ7z00kv4/fffa2nE1mPx4sVQqVSYPn261MZaK+fq1at45ZVX0LRpUzRq1Ajt27fHzz//LL0vhMD777+P5s2bo1GjRujTpw/OnTtXiyOun8rKyjB37lz4+fmhUaNGePzxx7FgwQKD55Sx1vIcPHgQgwYNgre3N1QqFb755huD902pa35+PqKioqDVauHi4oIJEyagqKioRsbL0FSHbN68GdHR0Zg3bx6OHTuGjh07ol+/fsjLy6vtodVrBw4cwJQpU/Df//4XiYmJ0Ol0iIyMxJ07d6Q+b775Jr777jts2bIFBw4cQHZ2Nl588cVaHHX9l5qaik8++QQdOnQwaGetlXHz5k306NEDtra2+PHHH3HmzBksX74crq6uUp+lS5di1apViIuLQ3JyMho3box+/frh3r17tTjy+mfJkiVYu3YtPv74Y2RmZmLJkiVYunQpVq9eLfVhreW5c+cOOnbsiDVr1lT6vil1jYqKwunTp5GYmIgdO3bg4MGDmDRpUs0MWFCd0a1bNzFlyhTpdVlZmfD29haxsbG1OCrrk5eXJwCIAwcOCCGEKCgoELa2tmLLli1Sn8zMTAFAJCUl1dYw67Xbt2+L1q1bi8TERNGrVy8xbdo0IQRrraR33nlHhIeHP/R9vV4vvLy8xLJly6S2goICYW9vL7788ktLDNFqDBw4UIwfP96g7cUXXxRRUVFCCNZaKQDE9u3bpdem1PXMmTMCgEhNTZX6/Pjjj0KlUomrV68qPkbONNUR9+/fR1paGvr06SO1qdVq9OnTB0lJSbU4Mutz69YtAICbmxsAIC0tDTqdzqD2gYGBaNmyJWsv05QpUzBw4ECDmgKstZK+/fZbdO3aFS+//DI8PT3RuXNnfPrpp9L7Fy9eRG5urkGtmzRpgtDQUNa6mrp37449e/bg7NmzAICMjAwcPnwYzz33HADWuqaYUtekpCS4uLiga9euUp8+ffpArVYjOTlZ8THxgb11xPXr11FWVoZmzZoZtDdr1gy//PJLLY3K+uj1ekyfPh09evRAcHAwACA3Nxd2dnZwcXEx6NusWTPk5ubWwijrt6+++grHjh1Damqq0XustXJ+/fVXrF27FtHR0Xj33XeRmpqKN954A3Z2dhgzZoxUz8r+m8JaV8+sWbNQWFiIwMBAaDQalJWVYeHChYiKigIA1rqGmFLX3NxceHp6GrxvY2MDNze3Gqk9QxM1KFOmTMGpU6dw+PDh2h6KVfrtt98wbdo0JCYmwsHBobaHY9X0ej26du2KRYsWAQA6d+6MU6dOIS4uDmPGjKnl0VmXr7/+Gps2bUJCQgLatWuH9PR0TJ8+Hd7e3qx1A8PLc3WEu7s7NBqN0V1Ev//+O7y8vGppVNZl6tSp2LFjB/bt24cWLVpI7V5eXrh//z4KCgoM+rP21ZeWloa8vDx06dIFNjY2sLGxwYEDB7Bq1SrY2NigWbNmrLVCmjdvjrZt2xq0BQUF4cqVKwAg1ZP/TTHfzJkzMWvWLIwYMQLt27fHqFGj8OabbyI2NhYAa11TTKmrl5eX0c1SpaWlyM/Pr5HaMzTVEXZ2dggJCcGePXukNr1ejz179iAsLKwWR1b/CSEwdepUbN++HXv37oWfn5/B+yEhIbC1tTWofVZWFq5cucLaV1NERAROnjyJ9PR06adr166IioqSfmetldGjRw+jrTPOnj2LVq1aAQD8/Pzg5eVlUOvCwkIkJyez1tVUXFwMtdrwz6VGo4FerwfAWtcUU+oaFhaGgoICpKWlSX327t0LvV6P0NBQ5Qel+NJyku2rr74S9vb24vPPPxdnzpwRkyZNEi4uLiI3N7e2h1av/fWvfxVNmjQR+/fvFzk5OdJPcXGx1Oe1114TLVu2FHv37hU///yzCAsLE2FhYbU4autR8e45IVhrpaSkpAgbGxuxcOFCce7cObFp0ybh6Ogo/vWvf0l9Fi9eLFxcXMR//vMfceLECTFkyBDh5+cn7t69W4sjr3/GjBkjHnvsMbFjxw5x8eJF8e9//1u4u7uLt99+W+rDWstz+/Ztcfz4cXH8+HEBQKxYsUIcP35cXL58WQhhWl379+8vOnfuLJKTk8Xhw4dF69atxciRI2tkvAxNdczq1atFy5YthZ2dnejWrZv473//W9tDqvcAVPrz2WefSX3u3r0r/va3vwlXV1fh6Ogohg4dKnJycmpv0Fbkz6GJtVbOd999J4KDg4W9vb0IDAwU8fHxBu/r9Xoxd+5c0axZM2Fvby8iIiJEVlZWLY22/iosLBTTpk0TLVu2FA4ODsLf31+89957oqSkROrDWsuzb9++Sv/7PGbMGCGEaXW9ceOGGDlypHBychJarVaMGzdO3L59u0bGqxKiwpamRERERFQprmkiIiIiMgFDExEREZEJGJqIiIiITMDQRERERGQChiYiIiIiEzA0EREREZmAoYmIiIjIBAxNRERERCZgaCKieqt3796YPn16bQ9DIoTApEmT4ObmBpVKhfT09Br5nIrfu67VgMiaMTQRkWxjx46FSqXC4sWLDdq/+eYbqFSqWhpV7dm5cyc+//xz7NixAzk5OQgODq7tIRGRghiaiMgsDg4OWLJkCW7evFnbQ1HM/fv3ZR134cIFNG/eHN27d4eXlxdsbGwUHhkR1SaGJiIyS58+feDl5YXY2NiH9vH19cVHH31k0NapUyd88MEH0uvevXvj9ddfx/Tp0+Hq6opmzZrh008/xZ07dzBu3Dg4OzsjICAAP/74o8F5SktLMXXqVDRp0gTu7u6YO3cuKj5SU6/XIzY2Fn5+fmjUqBE6duyIrVu3Gpyjd+/emDp1KqZPnw53d3f069ev0u9RUlKCN954A56ennBwcEB4eDhSU1MBPJh1e/3113HlyhWoVCr4+vpWeg69Xo+lS5ciICAA9vb2aNmyJRYuXCi9v3PnToSHh8PFxQVNmzbF888/jwsXLjy0tn+2detWtG/fHo0aNULTpk3Rp08f3Llz56H9L1y4AJVKhR07diAiIgKOjo5o06YNkpOTTf5MooaCoYmIzKLRaLBo0SKsXr0a//vf/8w614YNG+Du7o6UlBS8/vrr+Otf/4qXX34Z3bt3x7FjxxAZGYlRo0ahuLjY4BgbGxukpKRg5cqVWLFiBf75z39K78fGxmLjxo2Ii4vD6dOn8eabb+KVV17BgQMHjD7bzs4OR44cQVxcXKXje/vtt7Ft2zZs2LABx44dQ0BAAPr164f8/HysXLkS8+fPR4sWLZCTkyOFqT+bPXs2Fi9ejLlz5+LMmTNISEhAs2bNpPfv3LmD6Oho/Pzzz9izZw/UajWGDh0KvV5fZf1ycnIwcuRIjB8/HpmZmdi/fz9efPFFPOq57BkZGVCpVFixYgXmzp2LjIwMtGzZErNmzary84gaHEFEJNOYMWPEkCFDhBBCPPXUU2L8+PFCCCG2b98uKv7npVWrVuIf//iHwbEdO3YU8+bNk1736tVLhIeHS69LS0tF48aNxahRo6S2nJwcAUAkJSVJxwQFBQm9Xi/1eeedd0RQUJAQQoh79+4JR0dHcfToUYPPnjBhghg5cqTBZ3fu3PmR37WoqEjY2tqKTZs2SW33798X3t7eYunSpUIIIf7xj3+IVq1aPfQchYWFwt7eXnz66aeP/KyKrl27JgCIkydPGox32rRpRr+npaUJAOLSpUsmn//9998Xrq6uIi8vT2pbtWqVaNeuncnnIGooONNERIpYsmQJNmzYgMzMTNnn6NChg/S7RqNB06ZN0b59e6mtfEYmLy9PanvqqacMFp2HhYXh3LlzKCsrw/nz51FcXIy+ffvCyclJ+tm4caPRJa+QkJBHju3ChQvQ6XTo0aOH1GZra4tu3bqZ/J0zMzNRUlKCiIiIh/Y5d+4cRo4cCX9/f2i1Wuky35UrV6o8f8eOHREREYH27dvj5ZdfxqefflrlWrOMjAwMGTIEHh4eUtvFixcREBBg0nciakgYmohIEU8//TT69euH2bNnG72nVquNLhHpdDqjfra2tgavVSqVQVt5ODLlUhUAFBUVAQC+//57pKenSz9nzpwxWtfUuHFjk85pjkaNGlXZZ9CgQcjPz8enn36K5ORkaW2RKYvTNRoNEhMT8eOPP6Jt27ZYvXo12rRpg4sXLz70mIyMDISFhRm0paeno1OnTlV+HlFDw9BERIpZvHgxvvvuOyQlJRm0e3h4ICcnR3pdWFj4yD/k1fHnBcv//e9/0bp1a2g0GrRt2xb29va4cuUKAgICDH58fHyq9TmPP/64tOapnE6nQ2pqKtq2bWvSOVq3bo1GjRphz549lb5/48YNZGVlYc6cOYiIiEBQUFC170pUqVTo0aMHYmJicPz4cdjZ2WH79u2V9r116xYuXbqEzp07G7QzNBFVjvfDEpFi2rdvj6ioKKxatcqg/dlnn8Xnn3+OQYMGwcXFBe+//z40Go0in3nlyhVER0dj8uTJOHbsGFavXo3ly5cDAJydnfHWW2/hzTffhF6vR3h4OG7duoUjR45Aq9VizJgxJn9O48aN8de//hUzZ86Em5sbWrZsiaVLl6K4uBgTJkww6RwODg5455138Pbbb8POzg49evTAtWvXcPr0aUyYMAGurq5o2rQp4uPj0bx5c1y5cqVaC7KTk5OxZ88eREZGwtPTE8nJybh27RqCgoIq7X/ixAnY2NgYXAK9fPkybt68ydBEVAmGJiJS1Pz587F582aDttmzZ+PixYt4/vnn0aRJEyxYsECxmabRo0fj7t276NatGzQaDaZNm4ZJkyZJ7y9YsAAeHh6IjY3Fr7/+ChcXF3Tp0gXvvvtutT9r8eLF0Ov1GDVqFG7fvo2uXbvip59+gqurq8nnmDt3LmxsbPD+++8jOzsbzZs3x2uvvQbgwWXMr776Cm+88QaCg4PRpk0brFq1Cr179zbp3FqtFgcPHsRHH32EwsJCtGrVCsuXL8dzzz1Xaf+MjAy0adMGDg4OUtvx48fh4uLy0C0TiBoylfjzQgMiIiIiMsI1TUREREQmYGgiIiIiMgFDExEREZEJGJqIiIiITMDQRERERGQChiYiIiIiEzA0EREREZmAoYmIiIjIBAxNRERERCZgaCIiIiIyAUMTERERkQkYmoiIiIhM8P8B+9Anl0v8pC4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. Begin the tuning process with Bayesian optimization using Gaussian Processes through gp_minimize from skopt\n",
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=100, # min n_calls=11\n",
    "                            x0=default_parameters)\n",
    "\n",
    "# save the search result for future use\n",
    "dump(search_result,result_path+'search_result.gz', compress=9)\n",
    "# if you need to reload the optimization history, use the following command\n",
    "#res_load = load(result_path+'search_result.gz')\n",
    "\n",
    "# check the optimization result\n",
    "plot_convergence(search_result)\n",
    "\n",
    "# check the optimized solution\n",
    "search_result.x\n",
    "\n",
    "# Evaluate the performance of the best model on the test set\n",
    "model_best = keras.models.load_model(path_best_model)\n",
    "train_score = model_best.evaluate(X_train, y_train, verbose=0)\n",
    "test_score = model_best.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train loss:', train_score[0])\n",
    "print('Train AUC:', train_score[1])\n",
    "print('Test loss:', test_score[0])\n",
    "print('Test AUC:', test_score[1])\n",
    "\n",
    "# check the optimization history by listing the combination of all x and \n",
    "# its corresponding funtion value\n",
    "sorted(zip(search_result.func_vals, search_result.x_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now save our best model\n",
    "result_path = 'C:/Your_Path/WHIchf_skopt_result'\n",
    "path_best_model = os.path.join(result_path+'best_model.keras')\n",
    "model_best = keras.models.load_model(path_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 1s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# We now want to visualize and see how our best model did\n",
    "from sklearn import metrics\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = model_best.predict(X_test)\n",
    "true = y_test.reshape(1541, 1)\n",
    "fpr, tpr, threshold = metrics.roc_curve(true, probs)\n",
    "roc_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.767028377376166"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our best AUC score\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can plot our AUC on the ROC curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8945c588",
   "metadata": {},
   "source": [
    "## Epilogue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b11d003",
   "metadata": {},
   "source": [
    "##### As we can see, the LSTM model is a specific type of neural network model which does well with long sequential data. In our case, however, we only had two time points (years 0 and 3), and thus the prediction of our model didn't do much better than a standard RNN. I suppose that we can also do a bit better with respect to the parameters and tuning we had, but once again, like the factorization machine model the limitation is in the computing resources I'm unfortunately limited to.\n",
    "\n",
    "##### The differences in our neural network approach compared to the FAFM approach is that instead of hand-picked variables we threw in just about every variable under the sun. This neural network model had over 919 variables, and could've been more had we not excluded all variables with >90% missing data!\n",
    "\n",
    "##### As we conclude, both types of modeling I have introduced so far relies on making predictions. The next series will explore how we can take the modeling approach and start trying to make causal inferences, which is what we really like to explore in epidemiology."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5f098e27c0b04a4d291bef7e0a45775f053a86a097de4a135541eeebd86e8ec2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
